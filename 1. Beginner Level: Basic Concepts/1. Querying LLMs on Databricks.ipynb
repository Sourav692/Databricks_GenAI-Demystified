{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790c3983-5574-4a16-bd98-27157378cf51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Getting Started with LLM Serving on Databricks: A Comprehensive Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized how we build intelligent applications, but deploying and querying them at scale can be challenging. Databricks simplifies this process through Foundation Model APIs, providing seamless access to state-of-the-art language models with minimal setup. In this guide, we'll explore how to quickly get started with LLM serving on Databricks, from initial setup to production deployment.\n",
    "\n",
    "Whether you're prototyping a generative AI application or building production-ready solutions, Databricks offers flexible options to meet your needs—from pay-per-token endpoints for experimentation to provisioned throughput for performance-critical workloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "012da273-0cca-458b-8dfc-0625d7adff6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Understanding Foundation Model APIs\n",
    "\n",
    "Foundation Model APIs on Databricks provide two primary deployment options:\n",
    "\n",
    "### 1. **Pay-Per-Token Endpoints**\n",
    "Perfect for getting started, prototyping, and variable workloads where you only pay for what you use. These endpoints are automatically available in your Databricks workspace's Serving UI, providing instant access to popular foundation models without any infrastructure setup.\n",
    "\n",
    "### 2. **Provisioned Throughput Endpoints**\n",
    "Recommended for production workloads that require:\n",
    "- Fine-tuned custom models\n",
    "- Performance guarantees and SLAs\n",
    "- Predictable latency\n",
    "- High-volume, consistent traffic patterns\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into LLM serving on Databricks, ensure you have:\n",
    "\n",
    "1. **A Databricks Workspace** in a [supported region](https://docs.databricks.com/aws/en/machine-learning/model-serving/model-serving-limits#regions) for Foundation Model APIs\n",
    "2. **Personal Access Token (PAT)** for authenticating API requests to Mosaic AI Model Serving endpoints\n",
    "\n",
    "### Security Best Practices\n",
    "\n",
    "⚠️ **Important Security Note**: For production environments, Databricks recommends:\n",
    "- Using **machine-to-machine OAuth tokens** for authentication\n",
    "- Leveraging **service principals** instead of individual user accounts for testing and development\n",
    "- Never hardcoding tokens in your application code\n",
    "\n",
    "## Hands-On: Querying Your First LLM\n",
    "\n",
    "Let's walk through a practical example of querying the **Meta Llama 3.1 405B Instruct** model using the OpenAI client. This example demonstrates how easily you can integrate Databricks Foundation Model APIs into your applications.\n",
    "\n",
    "### Step 1: Setup and Configuration\n",
    "\n",
    "The code below should be run in a Databricks notebook:\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Retrieve your Databricks personal access token\n",
    "DATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "# Initialize the OpenAI client with Databricks endpoint\n",
    "client = OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,  # Your personal access token\n",
    "    base_url='https://<workspace_id>.databricks.com/serving-endpoints',  # Your Databricks workspace URL\n",
    ")\n",
    "```\n",
    "\n",
    "**Key Configuration Points:**\n",
    "- Replace `<workspace_id>` with your actual Databricks workspace instance\n",
    "- Store your token securely using environment variables\n",
    "- The base URL points to your workspace's serving endpoints\n",
    "\n",
    "### Step 2: Making Your First Request\n",
    "\n",
    "```python\n",
    "# Create a chat completion request\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is a mixture of experts model?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"databricks-meta-llama-3-1-405b-instruct\",  # Model endpoint name\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "print(chat_completion.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### Step 3: Understanding the Response\n",
    "\n",
    "The API returns a structured JSON response:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"xxxxxxxxxxxxx\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": \"xxxxxxxxx\",\n",
    "  \"model\": \"databricks-meta-llama-3-1-405b-instruct\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A Mixture of Experts (MoE) model is a machine learning technique that combines the predictions of multiple expert models to improve overall performance. Each expert model specializes in a specific subset of the data, and the MoE model uses a gating network to determine which expert to use for a given input.\"\n",
    "      },\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 123,\n",
    "    \"completion_tokens\": 23,\n",
    "    \"total_tokens\": 146\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Response Elements:**\n",
    "- `choices[0].message.content`: The model's generated response\n",
    "- `usage`: Token consumption breakdown for billing and monitoring\n",
    "- `finish_reason`: Indicates why generation stopped (e.g., \"stop\" for natural completion)\n",
    "\n",
    "### Troubleshooting Common Issues\n",
    "\n",
    "If you encounter an `ImportError: cannot import name 'OpenAI' from 'openai'`, update your OpenAI package:\n",
    "\n",
    "```python\n",
    "!pip install -U openai\n",
    "dbutils.library.restartPython()\n",
    "```\n",
    "\n",
    "## Available Foundation Models\n",
    "\n",
    "Databricks provides access to a curated selection of state-of-the-art foundation models, including:\n",
    "\n",
    "- **Meta Llama 3.1** series (8B, 70B, 405B parameter variants)\n",
    "- **Mistral** models\n",
    "- **DBRX** - Databricks' own foundation model\n",
    "- And many more...\n",
    "\n",
    "For the complete list of supported models, visit the [Databricks Foundation Model APIs documentation](https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis/supported-models).\n",
    "\n",
    "## Beyond Basic Querying: Next Steps\n",
    "\n",
    "Once you're comfortable with basic LLM queries, explore these advanced capabilities:\n",
    "\n",
    "### 1. **AI Playground**\n",
    "Test and experiment with different models through an interactive chat interface before integrating them into your applications. The AI Playground provides a low-code environment for prompt engineering and model comparison.\n",
    "\n",
    "### 2. **External Models Integration**\n",
    "Access models hosted outside Databricks (like OpenAI, Anthropic, or Cohere) through a unified interface, enabling vendor flexibility and multi-model strategies.\n",
    "\n",
    "### 3. **Fine-Tuning and Custom Models**\n",
    "Deploy your own fine-tuned models using provisioned throughput endpoints for specialized use cases that require domain-specific knowledge or behavior.\n",
    "\n",
    "### 4. **Building AI Agents**\n",
    "Leverage Databricks' Agent Framework to build sophisticated AI agents with tool-calling capabilities, retrieval-augmented generation (RAG), and multi-step reasoning.\n",
    "\n",
    "### 5. **Monitoring and Observability**\n",
    "Implement comprehensive monitoring for model quality, endpoint health, latency metrics, and cost optimization to ensure production reliability.\n",
    "\n",
    "## Real-World Use Cases\n",
    "\n",
    "Foundation Model APIs on Databricks enable various enterprise applications:\n",
    "\n",
    "- **Customer Support Automation**: Deploy conversational AI agents that understand context and provide accurate responses\n",
    "- **Document Intelligence**: Extract insights from unstructured data, summarize reports, and answer questions about internal documents\n",
    "- **Code Generation**: Build developer productivity tools that generate, explain, and debug code\n",
    "- **Content Creation**: Automate marketing copy, product descriptions, and personalized communications\n",
    "- **Data Analysis**: Generate SQL queries, create visualizations, and explain analytical insights in natural language\n",
    "\n",
    "## Cost Optimization Tips\n",
    "\n",
    "When working with Foundation Model APIs:\n",
    "\n",
    "1. **Start with Pay-Per-Token**: Test and validate your use case before committing to provisioned throughput\n",
    "2. **Monitor Token Usage**: Track the `usage` field in responses to optimize prompt design and reduce costs\n",
    "3. **Right-Size Your Requests**: Use `max_tokens` parameter to control response length and prevent unnecessary token consumption\n",
    "4. **Cache Responses**: For repeated queries, implement caching strategies to reduce API calls\n",
    "5. **Upgrade to Provisioned Throughput**: Once you have predictable traffic patterns and performance requirements\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Databricks Foundation Model APIs democratize access to powerful LLMs, making it easier than ever to integrate advanced AI capabilities into your applications. With minimal setup, you can go from zero to querying state-of-the-art models in minutes.\n",
    "\n",
    "The combination of pay-per-token flexibility for experimentation and provisioned throughput for production workloads ensures you have the right tools for every stage of your AI journey. Whether you're building a proof-of-concept or deploying mission-critical applications, Databricks provides the infrastructure, governance, and tooling to succeed.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Databricks Foundation Model APIs Documentation](https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis/)\n",
    "- [AI Playground Tutorial](https://docs.databricks.com/aws/en/large-language-models/ai-playground)\n",
    "- [Building Generative AI Applications Guide](https://docs.databricks.com/aws/en/generative-ai/guide/introduction-generative-ai-apps)\n",
    "- [MLflow for Generative AI](https://docs.databricks.com/aws/en/mlflow3/genai/)\n",
    "- [Model Serving Best Practices](https://docs.databricks.com/aws/en/machine-learning/model-serving/)\n",
    "\n",
    "---\n",
    "\n",
    "*Ready to start building with LLMs on Databricks? Create your free workspace and begin experimenting with Foundation Model APIs today!*\n",
    "\n",
    "---\n",
    "\n",
    "**About the Author**: This guide is designed for data engineers, ML engineers, and solution architects looking to leverage LLMs in their data platforms. For questions or discussions about LLM serving on Databricks, feel free to reach out through the Databricks Community forums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8d1f51-4485-4fda-9260-c8231986e8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc33ee3e-a736-44b5-a7a4-38d42fd326e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATABRICKS_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d110fd-c429-4314-8591-e17374eb7c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve your Databricks personal access token\n",
    "# DATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "# Initialize the OpenAI client with Databricks endpoint\n",
    "client = OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,  # Your personal access token\n",
    "    base_url='https://fe-vm-agentic-ai.cloud.databricks.com/serving-endpoints',  # Your Databricks workspace URL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8ae77d-d2ba-4d36-8af0-63d4afe1ceb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a chat completion request\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is a mixture of experts model?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"databricks-meta-llama-3-1-405b-instruct\",  # Model endpoint name\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581cedc6-5591-4880-a119-1f046c714472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the ChatCompletion object to a dictionary\n",
    "response_dict = chat_completion.model_dump()\n",
    "\n",
    "# Pretty print as JSON\n",
    "print(json.dumps(response_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf83a965-684a-4772-8ffc-377e3d06d89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2218dc84-5098-4981-9726-14a48e43f1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Token consumption breakdown for billing and monitoring \")\n",
    "print(f\"completion_tokens count is {response_dict['usage']['completion_tokens']}\")\n",
    "print(f\"prompt_tokens count is {response_dict['usage']['prompt_tokens']}\")\n",
    "print(f\"total_tokens count is {response_dict['usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8733f7f7-0a98-4cb8-8e1e-36595c328b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Querying LLMs on Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
