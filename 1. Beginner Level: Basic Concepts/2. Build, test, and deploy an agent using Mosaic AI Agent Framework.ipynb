{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d74b7227-593d-4df0-b33b-cac3720f03e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Quickstart: Build, test, and deploy an agent using Mosaic AI Agent Framework\n",
    "This quickstart notebook demonstrates how to build, test, and deploy a generative AI agent ([AWS](https://docs.databricks.com/aws/en/generative-ai/guide/introduction-generative-ai-apps#what-are-gen-ai-apps) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/guide/introduction-generative-ai-apps#what-are-gen-ai-apps) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/guide/introduction-generative-ai-apps)) using Mosaic AI Agent Framework ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/build-genai-apps#-mosaic-ai-agent-framework) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/build-genai-apps#-mosaic-ai-agent-framework) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/build-genai-apps#-mosaic-ai-agent-framework)) on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "747bdfd3-f39c-4724-b95c-9ad949b3fcd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Building Your First AI Agent with Databricks Mosaic AI: A Complete Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this comprehensive walkthrough of building, testing, and deploying a production-ready AI agent using Databricks' Mosaic AI Agent Framework! Whether you're a data engineer, ML practitioner, or solution architect working with Databricks, this guide will take you through every step of creating an intelligent agent that can interact with users and execute code to solve problems.\n",
    "\n",
    "In this tutorial, we'll build an agent that:\n",
    "- Uses Large Language Models (LLMs) served on Databricks Foundation Model API\n",
    "- Can execute Python code dynamically to solve computational problems\n",
    "- Is fully deployable as a production endpoint\n",
    "- Integrates seamlessly with MLflow for tracking and versioning\n",
    "\n",
    "Let's dive into each cell of this quickstart notebook and understand exactly what's happening under the hood!\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Setting Up the Environment\n",
    "\n",
    "### Cell 1: Installing Required Dependencies\n",
    "\n",
    "```python\n",
    "%pip install -U -qqqq mlflow databricks-openai databricks-agents\n",
    "dbutils.library.restartPython()\n",
    "```\n",
    "\n",
    "**What's happening here?**\n",
    "\n",
    "This cell installs three critical Python packages:\n",
    "\n",
    "1. **`mlflow`**: The open-source platform for managing the ML lifecycle, including experimentation, reproducibility, and deployment. We'll use this to log, version, and deploy our agent.\n",
    "\n",
    "2. **`databricks-openai`**: A specialized SDK that provides an OpenAI-compatible interface for interacting with Databricks model serving endpoints. This allows us to use familiar OpenAI-style APIs while leveraging Databricks' infrastructure.\n",
    "\n",
    "3. **`databricks-agents`**: The core framework for building and deploying agents on Databricks, providing tools for agent orchestration and deployment.\n",
    "\n",
    "The `-U` flag ensures we get the latest versions, and `-qqqq` suppresses verbose installation output for a cleaner notebook experience.\n",
    "\n",
    "**Why restart Python?**\n",
    "\n",
    "The `dbutils.library.restartPython()` command is crucialâ€”it restarts the Python interpreter to ensure all newly installed packages are properly loaded into the notebook's runtime environment. This prevents import errors and version conflicts.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Selecting the Right LLM Endpoint\n",
    "\n",
    "### Cell 2: Dynamic LLM Endpoint Selection\n",
    "\n",
    "```python\n",
    "LLM_ENDPOINT_NAME = None\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "def is_endpoint_available(endpoint_name):\n",
    "    try:\n",
    "        client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "        client.chat.completions.create(\n",
    "            model=endpoint_name, \n",
    "            messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}]\n",
    "        )\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "client = WorkspaceClient()\n",
    "for candidate_endpoint_name in [\"databricks-claude-3-7-sonnet\", \"databricks-meta-llama-3-3-70b-instruct\"]:\n",
    "    if is_endpoint_available(candidate_endpoint_name):\n",
    "        LLM_ENDPOINT_NAME = candidate_endpoint_name\n",
    "        \n",
    "assert LLM_ENDPOINT_NAME is not None, \"Please specify LLM_ENDPOINT_NAME\"\n",
    "```\n",
    "\n",
    "**Understanding the `is_endpoint_available()` function:**\n",
    "\n",
    "This elegant function performs a \"health check\" on an LLM endpoint by:\n",
    "\n",
    "1. **Creating a WorkspaceClient**: This establishes a connection to your Databricks workspace with automatic authentication\n",
    "2. **Getting an OpenAI client**: Retrieves a client configured to communicate with Databricks serving endpoints\n",
    "3. **Sending a test query**: Makes a simple chat completion request with \"What is AI?\" to verify the endpoint responds\n",
    "4. **Returning True/False**: If the request succeeds, the endpoint is available; any exception means it's not\n",
    "\n",
    "**The smart fallback mechanism:**\n",
    "\n",
    "The code iterates through a list of candidate LLM endpoints:\n",
    "- **Claude 3.7 Sonnet**: Anthropic's powerful model known for following instructions precisely\n",
    "- **Meta Llama 3.3 70B Instruct**: Meta's open-source instruction-tuned model\n",
    "\n",
    "This approach ensures your notebook works across different Databricks workspace configurations. If your workspace doesn't have Claude available, it automatically falls back to Llama 3.3.\n",
    "\n",
    "**The assertion at the end** guarantees that we have a working endpoint before proceedingâ€”failing fast if no LLM is available.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Building the Agent Core Logic\n",
    "\n",
    "### Cell 3: Agent Implementation with Tool Calling\n",
    "\n",
    "This is the heart of our agent! Let's break it down function by function:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_openai import UCFunctionToolkit, DatabricksFunctionClient\n",
    "\n",
    "# Automatically log traces from LLM calls for ease of debugging\n",
    "mlflow.openai.autolog()\n",
    "```\n",
    "\n",
    "**MLflow Autologging:**\n",
    "\n",
    "`mlflow.openai.autolog()` is incredibly powerfulâ€”it automatically captures:\n",
    "- Every LLM request and response\n",
    "- Token usage and costs\n",
    "- Latency metrics\n",
    "- Tool calls made during execution\n",
    "\n",
    "This gives you complete observability into your agent's behavior without writing manual logging code.\n",
    "\n",
    "---\n",
    "\n",
    "#### Function 1: Setting Up the OpenAI Client\n",
    "\n",
    "```python\n",
    "# Get an OpenAI client configured to talk to Databricks model serving endpoints\n",
    "openai_client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "```\n",
    "\n",
    "This creates a client that speaks the OpenAI API protocol but routes requests to Databricks' infrastructureâ€”giving you the best of both worlds: familiar APIs and enterprise-grade serving.\n",
    "\n",
    "---\n",
    "\n",
    "#### Function 2: Loading Built-in Tools\n",
    "\n",
    "```python\n",
    "# Load Databricks built-in tools (a stateless Python code interpreter tool)\n",
    "client = DatabricksFunctionClient()\n",
    "builtin_tools = UCFunctionToolkit(\n",
    "    function_names=[\"system.ai.python_exec\"], \n",
    "    client=client\n",
    ").tools\n",
    "\n",
    "for tool in builtin_tools:\n",
    "    del tool[\"function\"][\"strict\"]\n",
    "```\n",
    "\n",
    "**What's happening here?**\n",
    "\n",
    "- **`DatabricksFunctionClient`**: Creates a client for accessing Unity Catalog functions\n",
    "- **`UCFunctionToolkit`**: Loads tools registered in Unity Catalogâ€”in this case, `system.ai.python_exec`, a secure Python code execution environment\n",
    "- **Removing \"strict\" parameter**: This line removes strict schema validation to ensure compatibility with different LLM providers\n",
    "\n",
    "**Why is this powerful?**\n",
    "\n",
    "The `system.ai.python_exec` tool allows your agent to write and execute Python code dynamically to solve problemsâ€”like a data scientist working through computations in a notebook!\n",
    "\n",
    "---\n",
    "\n",
    "#### Function 3: `call_tool()` - Executing Agent Tools\n",
    "\n",
    "```python\n",
    "def call_tool(tool_name, parameters):\n",
    "    if tool_name == \"system__ai__python_exec\":\n",
    "        return DatabricksFunctionClient().execute_function(\n",
    "            \"system.ai.python_exec\", \n",
    "            parameters=parameters\n",
    "        )\n",
    "    raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "```\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This function acts as a **router** that maps tool names to their execution logic.\n",
    "\n",
    "**Key details:**\n",
    "\n",
    "- **Tool name normalization**: Note the double underscores (`system__ai__python_exec`) replacing dotsâ€”this follows OpenAI's function calling naming conventions\n",
    "- **Parameter passing**: The function receives parameters as a dictionary and forwards them to the Unity Catalog function executor\n",
    "- **Error handling**: Raises a clear error if an unknown tool is requested\n",
    "- **Extensibility**: You can easily add more tools by adding additional `if` conditions\n",
    "\n",
    "---\n",
    "\n",
    "#### Function 4: `run_agent()` - The Orchestration Engine\n",
    "\n",
    "```python\n",
    "def run_agent(prompt):\n",
    "    \"\"\"\n",
    "    Send a user prompt to the LLM, and return a list of LLM response messages\n",
    "    The LLM is allowed to call the code interpreter tool if needed, to respond to the user\n",
    "    \"\"\"\n",
    "    result_msgs = []\n",
    "    \n",
    "    # Step 1: Send the prompt to the LLM with available tools\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=LLM_ENDPOINT_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=builtin_tools,\n",
    "    )\n",
    "    \n",
    "    msg = response.choices[0].message\n",
    "    result_msgs.append(msg.to_dict())\n",
    "\n",
    "    # Step 2: If the model executed a tool, call it\n",
    "    if msg.tool_calls:\n",
    "        call = msg.tool_calls[0]\n",
    "        tool_result = call_tool(\n",
    "            call.function.name, \n",
    "            json.loads(call.function.arguments)\n",
    "        )\n",
    "        result_msgs.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": tool_result.value,\n",
    "            \"name\": call.function.name,\n",
    "            \"tool_call_id\": call.id,\n",
    "        })\n",
    "        \n",
    "    return result_msgs\n",
    "```\n",
    "\n",
    "**Deep dive into this critical function:**\n",
    "\n",
    "**Step 1: LLM Invocation**\n",
    "- Creates a chat completion request with the user's prompt\n",
    "- Crucially includes `tools=builtin_tools`, informing the LLM it can call the Python executor\n",
    "- The LLM decides whether it needs to use a tool to answer the question\n",
    "\n",
    "**Step 2: Tool Execution (Conditional)**\n",
    "- Checks if `msg.tool_calls` existsâ€”this means the LLM decided it needs to execute code\n",
    "- Extracts the tool name and arguments from the first tool call\n",
    "- Calls our `call_tool()` function to execute the code\n",
    "- Appends the tool's result to the message history with proper formatting\n",
    "\n",
    "**Return value:**\n",
    "A list of messages representing the conversation flowâ€”this could be just the LLM's direct answer, or an LLM response + tool call + tool result sequence.\n",
    "\n",
    "**Important limitation:**\n",
    "This implementation handles only **one tool call** per prompt (note `tool_calls[0]`). For production agents, you'd want to loop through multiple tool calls and potentially allow multi-turn conversations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Testing the Agent\n",
    "\n",
    "```python\n",
    "answer = run_agent(\"What is the square root of 429?\")\n",
    "for message in answer:\n",
    "    print(f'{message[\"role\"]}: {message[\"content\"]}')\n",
    "```\n",
    "\n",
    "**What happens when we run this:**\n",
    "\n",
    "1. The LLM receives: \"What is the square root of 429?\"\n",
    "2. It recognizes this requires computation and generates a tool call to `python_exec`\n",
    "3. The tool executes Python code like: `import math; math.sqrt(429)`\n",
    "4. The result (~20.71) is returned\n",
    "5. We print both the assistant's tool call and the tool's result\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "assistant: [Tool call to execute Python code]\n",
    "tool: 20.71231... (the actual result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: MLflow Integration - Making it Production-Ready\n",
    "\n",
    "### Cell 4: Wrapping in MLflow's ChatAgent Interface\n",
    "\n",
    "```python\n",
    "import uuid\n",
    "import mlflow\n",
    "from typing import Any, Optional\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import ChatAgentMessage, ChatAgentResponse, ChatContext\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "class QuickstartAgent(ChatAgent):\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # 1. Extract the last user prompt from the input messages\n",
    "        prompt = messages[-1].content\n",
    "        \n",
    "        # 2. Call run_agent to get back a list of response messages\n",
    "        raw_msgs = run_agent(prompt)\n",
    "        \n",
    "        # 3. Map each response message into a ChatAgentMessage and return\n",
    "        out = []\n",
    "        for m in raw_msgs:\n",
    "            out.append(ChatAgentMessage(id=uuid.uuid4().hex, **m))\n",
    "            \n",
    "        return ChatAgentResponse(messages=out)\n",
    "```\n",
    "\n",
    "**Why wrap in MLflow's ChatAgent?**\n",
    "\n",
    "MLflow's `ChatAgent` is a standardized interface that provides:\n",
    "- **Consistent API**: All agents follow the same prediction pattern\n",
    "- **Built-in UI**: Automatic chat interfaces in Databricks AI Playground\n",
    "- **Deployment support**: Seamless deployment to model serving endpoints\n",
    "- **Tracking**: Integration with MLflow's experiment tracking\n",
    "\n",
    "**Understanding the `predict()` method:**\n",
    "\n",
    "**Parameters:**\n",
    "- **`messages`**: A list of `ChatAgentMessage` objects representing the conversation history\n",
    "- **`context`**: Optional additional context (documents, metadata, etc.)\n",
    "- **`custom_inputs`**: Custom parameters for specialized behavior\n",
    "\n",
    "**Implementation logic:**\n",
    "\n",
    "1. **Extract the latest prompt** (`messages[-1].content`): Gets the most recent user message from the conversation\n",
    "2. **Call our agent logic**: Invokes `run_agent()` with the prompt\n",
    "3. **Format responses**: Converts raw message dictionaries into `ChatAgentMessage` objects with unique IDs\n",
    "4. **Return standardized response**: Wraps messages in `ChatAgentResponse` for MLflow compatibility\n",
    "\n",
    "**Why generate UUIDs?**\n",
    "Each message needs a unique identifier for tracking, debugging, and UI rendering purposes.\n",
    "\n",
    "---\n",
    "\n",
    "#### Testing the MLflow Agent\n",
    "\n",
    "```python\n",
    "AGENT = QuickstartAgent()\n",
    "\n",
    "for response_message in AGENT.predict(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the square root of 429?\"}]}\n",
    ").messages:\n",
    "    print(f\"role: {response_message.role}, content: {response_message.content}\")\n",
    "```\n",
    "\n",
    "This tests that our MLflow-wrapped agent works identically to the raw implementation, but now follows MLflow's standard interface.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Logging the Agent for Deployment\n",
    "\n",
    "### Cell 5: Writing Agent Code to a File\n",
    "\n",
    "```python\n",
    "%%writefile quickstart_agent.py\n",
    "[... entire agent implementation ...]\n",
    "mlflow.models.set_model(AGENT)\n",
    "```\n",
    "\n",
    "**The magic of `%%writefile`:**\n",
    "\n",
    "This Jupyter/Databricks notebook magic command writes the entire cell content to a file named `quickstart_agent.py`. This is crucial because:\n",
    "\n",
    "1. **MLflow needs a Python file** to package and deploy\n",
    "2. **All dependencies must be in one place** for containerization\n",
    "3. **The code becomes versionable** as a standalone artifact\n",
    "\n",
    "**Key addition: `mlflow.models.set_model(AGENT)`**\n",
    "\n",
    "This line tells MLflow: \"When someone loads this model, instantiate the `AGENT` object and use it for predictions.\" It's the bridge between code and deployable model.\n",
    "\n",
    "**Complete agent structure in the file:**\n",
    "- All imports\n",
    "- Endpoint selection logic\n",
    "- Tool definitions\n",
    "- `call_tool()` function\n",
    "- `run_agent()` function\n",
    "- `QuickstartAgent` class\n",
    "- Model registration call\n",
    "\n",
    "---\n",
    "\n",
    "### Cell 6: Registering the Agent to Unity Catalog\n",
    "\n",
    "```python\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "from quickstart_agent import LLM_ENDPOINT_NAME\n",
    "```\n",
    "\n",
    "**Why restart Python again?**\n",
    "\n",
    "After writing `quickstart_agent.py`, we restart to ensure we can import it cleanly as a module.\n",
    "\n",
    "---\n",
    "\n",
    "#### Setting up Model Registry Location\n",
    "\n",
    "```python\n",
    "# Register the model to the workspace default catalog.\n",
    "catalog_name = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "schema_name = \"default\"\n",
    "registered_model_name = f\"{catalog_name}.{schema_name}.quickstart_agent\"\n",
    "```\n",
    "\n",
    "**Unity Catalog three-level namespace:**\n",
    "\n",
    "Databricks uses a hierarchical structure:\n",
    "- **Catalog**: Top-level container (like a database in traditional systems)\n",
    "- **Schema**: Logical grouping within a catalog (like a schema in PostgreSQL)\n",
    "- **Model**: The actual registered model artifact\n",
    "\n",
    "**Dynamic catalog detection:**\n",
    "\n",
    "`spark.sql(\"SELECT current_catalog()\")` retrieves your workspace's current catalog, making the code portable across environments.\n",
    "\n",
    "**Example result:** `\"main.default.quickstart_agent\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### Specifying Required Resources\n",
    "\n",
    "```python\n",
    "# Specify Databricks product resources that the agent needs access to\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\"),\n",
    "]\n",
    "```\n",
    "\n",
    "**Why declare resources?**\n",
    "\n",
    "This is one of Databricks' most powerful features for production deployment:\n",
    "\n",
    "1. **Automatic authentication**: Databricks configures service principals and tokens automatically\n",
    "2. **Dependency tracking**: The platform knows exactly what services your agent needs\n",
    "3. **Access control**: Ensures the deployed agent has proper permissions\n",
    "4. **Governance**: Auditing and compliance teams can see all resource dependencies\n",
    "\n",
    "**Resources in this agent:**\n",
    "- **LLM Endpoint**: The model serving endpoint for Claude or Llama\n",
    "- **Python Executor**: The Unity Catalog function for code execution\n",
    "\n",
    "---\n",
    "\n",
    "#### Logging and Registering the Model\n",
    "\n",
    "```python\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"agent\",\n",
    "        python_model=\"quickstart_agent.py\",\n",
    "        extra_pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\"\n",
    "        ],\n",
    "        resources=resources,\n",
    "        registered_model_name=registered_model_name,\n",
    "    )\n",
    "```\n",
    "\n",
    "**Breaking down each parameter:**\n",
    "\n",
    "**`mlflow.set_registry_uri(\"databricks-uc\")`**\n",
    "- Tells MLflow to use Unity Catalog (UC) as the model registry\n",
    "- Enables enterprise features like access control and lineage\n",
    "\n",
    "**`mlflow.start_run()`**\n",
    "- Creates an MLflow experiment run to track this model logging operation\n",
    "- All metrics, parameters, and artifacts are associated with this run\n",
    "\n",
    "**`mlflow.pyfunc.log_model()` parameters:**\n",
    "\n",
    "1. **`artifact_path=\"agent\"`**: Directory name within the MLflow run where artifacts are stored\n",
    "\n",
    "2. **`python_model=\"quickstart_agent.py\"`**: The Python file containing our agent (MLflow's \"models from code\" feature)\n",
    "\n",
    "3. **`extra_pip_requirements`**: Additional Python packages needed at serving time\n",
    "   - We include `databricks-connect` matching the current version for compatibility\n",
    "\n",
    "4. **`resources`**: The Databricks resources we defined earlier (endpoints and functions)\n",
    "\n",
    "5. **`registered_model_name`**: The three-level namespace where the model is registered in Unity Catalog\n",
    "\n",
    "**Return value:**\n",
    "\n",
    "`logged_agent_info` contains metadata about the logged model, including:\n",
    "- Model URI\n",
    "- Registered model version\n",
    "- Run ID\n",
    "- Artifact location\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Deploying to Production\n",
    "\n",
    "### Cell 7: Creating a Serving Endpoint\n",
    "\n",
    "```python\n",
    "from databricks import agents\n",
    "\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=registered_model_name,\n",
    "    model_version=logged_agent_info.registered_model_version,\n",
    "    deploy_feedback_model=False,\n",
    ")\n",
    "```\n",
    "\n",
    "**The final step: Deployment!**\n",
    "\n",
    "**`agents.deploy()` parameters:**\n",
    "\n",
    "1. **`model_name`**: The Unity Catalog model name we registered (e.g., \"main.default.quickstart_agent\")\n",
    "\n",
    "2. **`model_version`**: The specific version number returned from `log_model()` (ensures we deploy exactly what we just logged)\n",
    "\n",
    "3. **`deploy_feedback_model=False`**: Whether to deploy an additional endpoint for collecting user feedback\n",
    "   - Set to `False` for this quickstart to simplify deployment\n",
    "   - In production, you'd want this `True` to gather feedback for evaluation\n",
    "\n",
    "**What happens during deployment:**\n",
    "\n",
    "1. **Container creation**: Databricks packages your agent code, dependencies, and environment into a container\n",
    "2. **Resource provisioning**: Allocates compute resources for serving requests\n",
    "3. **Authentication setup**: Configures automatic credentials for accessing the LLM endpoint and tools\n",
    "4. **Health checks**: Verifies the endpoint can serve predictions\n",
    "5. **Endpoint activation**: Makes the agent available via REST API\n",
    "\n",
    "**Post-deployment:**\n",
    "\n",
    "Once deployed, you can:\n",
    "- **Test in AI Playground**: Interactive chat interface for manual testing\n",
    "- **Share with stakeholders**: Generate review links for feedback collection\n",
    "- **Query via API**: Make programmatic requests to the endpoint\n",
    "- **Monitor performance**: Track latency, throughput, and errors in real-time\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Summary: How It All Fits Together\n",
    "\n",
    "Here's the complete flow of your deployed agent:\n",
    "\n",
    "```\n",
    "User Input\n",
    "    â†“\n",
    "[Deployed Agent Endpoint]\n",
    "    â†“\n",
    "QuickstartAgent.predict() â† MLflow interface\n",
    "    â†“\n",
    "run_agent() â† Orchestration logic\n",
    "    â†“\n",
    "OpenAI Client â†’ [LLM Endpoint] (Claude/Llama)\n",
    "    â†“\n",
    "LLM decides: Tool needed?\n",
    "    â”œâ”€ No â†’ Return answer directly\n",
    "    â””â”€ Yes â†’ Generate tool call\n",
    "        â†“\n",
    "call_tool() â† Route to correct tool\n",
    "    â†“\n",
    "DatabricksFunctionClient â†’ [system.ai.python_exec]\n",
    "    â†“\n",
    "Execute Python code securely\n",
    "    â†“\n",
    "Return result\n",
    "    â†“\n",
    "Format as ChatAgentResponse\n",
    "    â†“\n",
    "Return to user\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways and Best Practices\n",
    "\n",
    "### 1. Modularity is Key\n",
    "The code cleanly separates concerns:\n",
    "- Tool definitions\n",
    "- Agent orchestration\n",
    "- MLflow integration\n",
    "- Deployment configuration\n",
    "\n",
    "### 2. MLflow Autologging\n",
    "Always enable `mlflow.openai.autolog()` for production agentsâ€”the observability is invaluable for debugging and optimization.\n",
    "\n",
    "### 3. Resource Declaration\n",
    "Explicitly declaring Databricks resources ensures:\n",
    "- Automatic authentication\n",
    "- Clear dependency tracking\n",
    "- Proper governance\n",
    "\n",
    "### 4. Dynamic Endpoint Selection\n",
    "The fallback mechanism for LLM endpoints makes your code robust across different workspace configurations.\n",
    "\n",
    "### 5. Standardized Interfaces\n",
    "Using MLflow's `ChatAgent` interface ensures compatibility with Databricks' agent ecosystem and UIs.\n",
    "\n",
    "---\n",
    "\n",
    "## Production Enhancements to Consider\n",
    "\n",
    "While this quickstart is excellent for learning, here are enhancements for production:\n",
    "\n",
    "### 1. Multi-turn Conversations\n",
    "Currently, the agent processes single prompts. Add conversation memory:\n",
    "```python\n",
    "def run_agent(messages_history):  # Accept full history\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=LLM_ENDPOINT_NAME,\n",
    "        messages=messages_history,  # Use full context\n",
    "        tools=builtin_tools,\n",
    "    )\n",
    "```\n",
    "\n",
    "### 2. Multiple Tool Calls\n",
    "Handle scenarios where the LLM makes multiple tool calls:\n",
    "```python\n",
    "if msg.tool_calls:\n",
    "    for call in msg.tool_calls:  # Loop through all calls\n",
    "        tool_result = call_tool(call.function.name, ...)\n",
    "        # Append each result\n",
    "```\n",
    "\n",
    "### 3. Error Handling\n",
    "Add robust error handling for tool execution:\n",
    "```python\n",
    "try:\n",
    "    tool_result = call_tool(...)\n",
    "except Exception as e:\n",
    "    # Return error message to LLM for recovery\n",
    "    tool_result = f\"Error: {str(e)}\"\n",
    "```\n",
    "\n",
    "### 4. Custom Tools\n",
    "Extend beyond Python execution:\n",
    "```python\n",
    "builtin_tools = UCFunctionToolkit(\n",
    "    function_names=[\n",
    "        \"system.ai.python_exec\",\n",
    "        \"catalog.schema.custom_search_tool\",\n",
    "        \"catalog.schema.data_retrieval_tool\",\n",
    "    ],\n",
    "    client=client\n",
    ").tools\n",
    "```\n",
    "\n",
    "### 5. Prompt Engineering\n",
    "Add system prompts for better behavior:\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful data analysis assistant...\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "```\n",
    "\n",
    "### 6. Streaming Responses\n",
    "For better user experience, implement streaming:\n",
    "```python\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=LLM_ENDPOINT_NAME,\n",
    "    messages=messages,\n",
    "    tools=builtin_tools,\n",
    "    stream=True,  # Enable streaming\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Congratulations! You now understand every aspect of building a production-ready AI agent on Databricks. From installing dependencies to deploying a fully-functional endpoint, you've learned:\n",
    "\n",
    "âœ… How to set up the Databricks Agent Framework  \n",
    "âœ… Dynamic LLM endpoint selection and fallback logic  \n",
    "âœ… Building agents with tool-calling capabilities  \n",
    "âœ… MLflow integration for tracking and deployment  \n",
    "âœ… Unity Catalog registration for governance  \n",
    "âœ… Production deployment with automatic authentication  \n",
    "\n",
    "This quickstart provides a solid foundation for building more sophisticated agents that can:\n",
    "- Query databases\n",
    "- Retrieve documents\n",
    "- Call custom APIs\n",
    "- Perform complex data analysis\n",
    "- Interact with external systems\n",
    "\n",
    "The Databricks Mosaic AI Agent Framework handles the infrastructure complexity, letting you focus on building intelligent, valuable agents for your organization.\n",
    "\n",
    "**Next steps:**\n",
    "- Experiment with different LLM models\n",
    "- Add custom tools from your Unity Catalog\n",
    "- Implement evaluation pipelines\n",
    "- Collect user feedback for continuous improvement\n",
    "- Scale to handle production traffic\n",
    "\n",
    "Happy building! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Databricks Agent Framework Documentation](https://docs.databricks.com/en/generative-ai/agent-framework/)\n",
    "- [MLflow ChatAgent API](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatAgent)\n",
    "- [Unity Catalog Functions](https://docs.databricks.com/en/sql/language-manual/sql-ref-functions-builtin-ai.html)\n",
    "- [Databricks Foundation Model APIs](https://docs.databricks.com/en/machine-learning/foundation-model-apis/)\n",
    "\n",
    "---\n",
    "\n",
    "*This blog post provides a comprehensive breakdown of the Databricks Agent Framework quickstart notebook, explaining each cell, function, and architectural decision in detail. Perfect for data professionals looking to understand and extend AI agent capabilities on Databricks!*\n",
    "\n",
    "---\n",
    "\n",
    "**Author Notes:**\n",
    "- Tutorial source: [Databricks Agent Quickstart](https://docs.databricks.com/aws/en/notebooks/source/generative-ai/agent-quickstart.html)\n",
    "- Target audience: Data engineers, ML engineers, solution architects\n",
    "- Difficulty level: Intermediate\n",
    "- Estimated reading time: 25-30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb46b22-8e01-4380-a8aa-a5c4573ed7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define and test an agent\n",
    "This section defines and tests a simple agent with the following attributes:\n",
    "\n",
    "- The agent uses an LLM served on Databricks Foundation Model API ([AWS](https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/foundation-model-apis/) | [GCP](https://docs.databricks.com/gcp/en/machine-learning/foundation-model-apis))\n",
    "- The agent has access to a single tool, the built-in Python code interpreter tool on Databricks Unity Catalog. It can use this tool to run LLM-generated code in order to respond to user questions. ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/code-interpreter-tools#built-in-python-executor-tool) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/code-interpreter-tools) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/code-interpreter-tools))\n",
    "\n",
    "We will use `databricks_openai` SDK ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent#requirements) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent#requirements) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/author-agent#requirements)) to query the LLM endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b7d312-e9f6-4daa-8f1d-bca73b7238d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow databricks-openai databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2500c116-f700-42c8-857d-046df6897aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The snippet below tries to pick the first LLM API available in your Databricks workspace\n",
    "# from a set of candidates. You can override and simplify it\n",
    "# to just specify LLM_ENDPOINT_NAME.\n",
    "LLM_ENDPOINT_NAME = None\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "def is_endpoint_available(endpoint_name):\n",
    "  try:\n",
    "    client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "    client.chat.completions.create(model=endpoint_name, messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}])\n",
    "    return True\n",
    "  except Exception:\n",
    "    return False\n",
    "  \n",
    "client = WorkspaceClient()\n",
    "for candidate_endpoint_name in [\"databricks-claude-3-7-sonnet\", \"databricks-meta-llama-3-3-70b-instruct\"]:\n",
    "    if is_endpoint_available(candidate_endpoint_name):\n",
    "      LLM_ENDPOINT_NAME = candidate_endpoint_name\n",
    "assert LLM_ENDPOINT_NAME is not None, \"Please specify LLM_ENDPOINT_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad85702-3943-42b0-a665-6a636ed69a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_openai import UCFunctionToolkit, DatabricksFunctionClient\n",
    "\n",
    "# Automatically log traces from LLM calls for ease of debugging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Get an OpenAI client configured to talk to Databricks model serving endpoints\n",
    "# We'll use this to query an LLM in our agent\n",
    "openai_client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# Load Databricks built-in tools (a stateless Python code interpreter tool)\n",
    "client = DatabricksFunctionClient()\n",
    "builtin_tools = UCFunctionToolkit(\n",
    "    function_names=[\"system.ai.python_exec\"], client=client\n",
    ").tools\n",
    "for tool in builtin_tools:\n",
    "    del tool[\"function\"][\"strict\"]\n",
    "\n",
    "\n",
    "def call_tool(tool_name, parameters):\n",
    "    if tool_name == \"system__ai__python_exec\":\n",
    "        return DatabricksFunctionClient().execute_function(\n",
    "            \"system.ai.python_exec\", parameters=parameters\n",
    "        )\n",
    "    raise ValueError(f\"Unknown tool: {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46833fd-5ab7-44b9-9566-728541b2eba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "        model=LLM_ENDPOINT_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the square root of 429?\"}],\n",
    "        tools=builtin_tools,\n",
    "    )\n",
    "\n",
    "# Convert the ChatCompletion object to a dictionary\n",
    "response_dict = response.model_dump()\n",
    "\n",
    "# Pretty print as JSON\n",
    "print(f\"Response is {json.dumps(response_dict, indent=2)}\")\n",
    "\n",
    "msg = response.choices[0].message\n",
    "if msg.tool_calls:\n",
    "    call = msg.tool_calls[0]\n",
    "    tool_result = call_tool(call.function.name, json.loads(call.function.arguments))\n",
    "    print(f\"Tool result: {tool_result.value}\")\n",
    "    print(f\"Tool call ID: {call.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0661a199-d4ca-4eae-903f-c1abc2579d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d62e07f-023d-4484-8a4e-bedfad65e27b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_agent(prompt):\n",
    "    \"\"\"\n",
    "    Send a user prompt to the LLM, and return a list of LLM response messages\n",
    "    The LLM is allowed to call the code interpreter tool if needed, to respond to the user\n",
    "    \"\"\"\n",
    "    result_msgs = []\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=LLM_ENDPOINT_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=builtin_tools,\n",
    "    )\n",
    "    msg = response.choices[0].message\n",
    "    result_msgs.append(msg.to_dict())\n",
    "\n",
    "    # If the model executed a tool, call it\n",
    "    if msg.tool_calls:\n",
    "        call = msg.tool_calls[0]\n",
    "        tool_result = call_tool(call.function.name, json.loads(call.function.arguments))\n",
    "        result_msgs.append(\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": tool_result.value,\n",
    "                \"name\": call.function.name,\n",
    "                \"tool_call_id\": call.id,\n",
    "            }\n",
    "        )\n",
    "    return result_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f402ded-a2b2-42e3-87b7-b21c671331b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "answer = run_agent(\"What is the square root of 429?\")\n",
    "for message in answer:\n",
    "    print(f'{message[\"role\"]}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5e8fa2-5318-401f-928c-b2e9a02b9936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c94061c2-f519-4213-8e4a-4321d72a3fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare agent code for logging\n",
    "\n",
    "Wrap your agent definition in MLflowâ€™s [ChatAgent interface](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatAgent) to prepare your code for logging.\n",
    "\n",
    "By using MLflowâ€™s standard agent authoring interface, you get built-in UIs for chatting with your agent and sharing it with others after deployment. ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent#-use-chatagent-to-author-agents) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/author-agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaaeee19-bb4c-4c0e-bf1a-8ef84432625f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import mlflow\n",
    "from typing import Any, Optional\n",
    "\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import ChatAgentMessage, ChatAgentResponse, ChatContext\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "class QuickstartAgent(ChatAgent):\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # 1. Extract the last user prompt from the input messages\n",
    "        prompt = messages[-1].content\n",
    "\n",
    "        # 2. Call run_agent to get back a list of response messages\n",
    "        raw_msgs = run_agent(prompt)\n",
    "\n",
    "        # 3. Map each response message into a ChatAgentMessage and return\n",
    "        # the response\n",
    "        out = []\n",
    "        for m in raw_msgs:\n",
    "            out.append(ChatAgentMessage(id=uuid.uuid4().hex, **m))\n",
    "\n",
    "        return ChatAgentResponse(messages=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ee1e3e-3590-4c15-b684-971037014cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AGENT = QuickstartAgent()\n",
    "for response_message in AGENT.predict(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the square root of 429?\"}]}\n",
    ").messages:\n",
    "    print(f\"role: {response_message.role}, content: {response_message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94ae452f-37f1-4983-9b70-caa5185616a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent\n",
    "\n",
    "Log the agent and register it to Unity Catalog as a model ([AWS](https://docs.databricks.com/aws/en/machine-learning/manage-model-lifecycle/) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/manage-model-lifecycle/) | [GCP](https://docs.databricks.com/gcp/en/machine-learning/manage-model-lifecycle/)). This step packages the agent code and its dependencies into a single artifact to deploy it to a serving endpoint.\n",
    "\n",
    "The following code cells do the following:\n",
    "\n",
    "1. Copy the agent code from above and combine it into a single cell.\n",
    "1. Add the `%%writefile` cell magic command at the top of the cell to save the agent code to a file called `quickstart_agent.py`.\n",
    "1. Add a [mlflow.models.set_model()](https://mlflow.org/docs/latest/model#models-from-code) call to the bottom of the cell. This tells MLflow which Python agent object to use for making predictions when your agent is deployed.\n",
    "1. Log the agent code in the `quickstart_agent.py` file using MLflow APIs ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/log-agent) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/log-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/log-agent))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbdca20f-fa2f-4626-9d79-7264a0212cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile quickstart_agent.py\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_openai import UCFunctionToolkit, DatabricksFunctionClient\n",
    "from typing import Any, Optional\n",
    "\n",
    "import mlflow\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import ChatAgentMessage, ChatAgentResponse, ChatContext\n",
    "\n",
    "# Get an OpenAI client configured to talk to Databricks model serving endpoints\n",
    "# We'll use this to query an LLM in our agent\n",
    "openai_client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# The snippet below tries to pick the first LLM API available in your Databricks workspace\n",
    "# from a set of candidates. You can override and simplify it\n",
    "# to just specify LLM_ENDPOINT_NAME.\n",
    "LLM_ENDPOINT_NAME = None\n",
    "\n",
    "def is_endpoint_available(endpoint_name):\n",
    "  try:\n",
    "    client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "    client.chat.completions.create(model=endpoint_name, messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}])\n",
    "    return True\n",
    "  except Exception:\n",
    "    return False\n",
    "  \n",
    "for candidate_endpoint_name in [\"databricks-claude-3-7-sonnet\", \"databricks-meta-llama-3-3-70b-instruct\"]:\n",
    "    if is_endpoint_available(candidate_endpoint_name):\n",
    "      LLM_ENDPOINT_NAME = candidate_endpoint_name\n",
    "assert LLM_ENDPOINT_NAME is not None, \"Please specify LLM_ENDPOINT_NAME\"\n",
    "\n",
    "# Enable automatic tracing of LLM calls\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Load Databricks built-in tools (a stateless Python code interpreter tool)\n",
    "client = DatabricksFunctionClient()\n",
    "builtin_tools = UCFunctionToolkit(function_names=[\"system.ai.python_exec\"], client=client).tools\n",
    "for tool in builtin_tools:\n",
    "    del tool[\"function\"][\"strict\"]\n",
    "\n",
    "def call_tool(tool_name, parameters):\n",
    "    if tool_name == \"system__ai__python_exec\":\n",
    "        return DatabricksFunctionClient().execute_function(\"system.ai.python_exec\", parameters=parameters)\n",
    "    raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "def run_agent(prompt):\n",
    "    \"\"\"\n",
    "    Send a user prompt to the LLM, and return a list of LLM response messages\n",
    "    The LLM is allowed to call the code interpreter tool if needed, to respond to the user\n",
    "    \"\"\"\n",
    "    result_msgs = []\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=LLM_ENDPOINT_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools=builtin_tools,\n",
    "    )\n",
    "    msg = response.choices[0].message\n",
    "    result_msgs.append(msg.to_dict())\n",
    "\n",
    "    # If the model executed a tool, call it\n",
    "    if msg.tool_calls:\n",
    "        call = msg.tool_calls[0]\n",
    "        tool_result = call_tool(call.function.name, json.loads(call.function.arguments))\n",
    "        result_msgs.append({\"role\": \"tool\", \"content\": tool_result.value, \"name\": call.function.name, \"tool_call_id\": call.id})\n",
    "    return result_msgs\n",
    "\n",
    "class QuickstartAgent(ChatAgent):\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        prompt = messages[-1].content\n",
    "        raw_msgs = run_agent(prompt)\n",
    "        out = []\n",
    "        for m in raw_msgs:\n",
    "            out.append(ChatAgentMessage(\n",
    "                id=uuid.uuid4().hex,\n",
    "                **m\n",
    "            ))\n",
    "\n",
    "        return ChatAgentResponse(messages=out)\n",
    "\n",
    "AGENT = QuickstartAgent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0decec2c-13f8-4bf1-85e0-f8b33eac9753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5185465-6345-46fe-b49d-da4e643cecd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "from quickstart_agent import LLM_ENDPOINT_NAME\n",
    "\n",
    "# Register the model to the workspace default catalog.\n",
    "# Specify a catalog (e.g. \"main\") and schema name (e.g. \"custom_schema\") if needed,\n",
    "# in order to register the agent to a different location\n",
    "catalog_name = \"agentic_ai\"\n",
    "schema_name = \"databricks\"\n",
    "registered_model_name = f\"{catalog_name}.{schema_name}.quickstart_agent\"\n",
    "\n",
    "# Specify Databricks product resources that the agent needs access to (our builtin python\n",
    "# code interpreter tool and LLM serving endpoint), so that Databricks can automatically\n",
    "# configure authentication for the agent to access these resources when it's deployed.\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\"),\n",
    "]\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"agent\",\n",
    "        python_model=\"quickstart_agent.py\",\n",
    "        extra_pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\"\n",
    "        ],\n",
    "        resources=resources,\n",
    "        registered_model_name=registered_model_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d60087e6-077f-484c-9e14-f699ddafa5eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent\n",
    "\n",
    "Run the cell below to deploy the agent ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/deploy-agent) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/deploy-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/deploy-agent)). Once the agent endpoint starts, you can chat with it via AI Playground ([AWS](https://docs.databricks.com/aws/en/large-language-models/ai-playground) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/large-language-models/ai-playground) | [GCP](https://docs.databricks.com/gcp/en/large-language-models/ai-playground)), or share it with stakeholders ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/review-app) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/review-app) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-evaluation/review-app)) for initial feedback, before sharing it more broadly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e52e933-400c-4ce0-8ddd-bd6f1e1a3925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=registered_model_name,\n",
    "    model_version=logged_agent_info.registered_model_version,\n",
    "    deploy_feedback_model=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bfa94b0-1cbc-4534-b251-ec7403f95d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Build, test, and deploy an agent using Mosaic AI Agent Framework",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
