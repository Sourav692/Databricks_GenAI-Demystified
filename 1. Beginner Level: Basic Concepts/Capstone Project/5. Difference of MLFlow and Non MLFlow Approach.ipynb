{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "209a4306-00e8-4604-b39e-d661bbc0e96d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# From Black Box to Observable: Transforming Non-Compatible Agents to MLflow-Ready Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Building an AI agent is one thingâ€”making it production-ready is another challenge entirely. The difference between a prototype agent and a production-grade system often comes down to one word: **observability**.\n",
    "\n",
    "In this comprehensive guide, we'll dissect the exact changes needed to transform a basic LangGraph agent into an MLflow-compatible, production-ready system. We'll compare the two implementations side-by-side, explain why each change matters, and show you the real-world impact of these modifications.\n",
    "\n",
    "By the end of this article, you'll understand not just *what* to change, but *why* these changes are critical for deploying reliable AI agents at scale.\n",
    "\n",
    "## The Two Implementations: Overview\n",
    "\n",
    "Let's start by understanding what we're comparing:\n",
    "\n",
    "### Non-Compatible Agent\n",
    "A straightforward LangGraph implementation that works perfectly in development but lacks the infrastructure needed for production deployment.\n",
    "\n",
    "### MLflow-Compatible Agent\n",
    "The same agent functionality, enhanced with MLflow's observability, configuration management, and deployment capabilities.\n",
    "\n",
    "**The Promise**: Same behavior, better infrastructure.\n",
    "\n",
    "## Side-by-Side Comparison\n",
    "\n",
    "Let's examine each component and understand the differences.\n",
    "\n",
    "---\n",
    "\n",
    "## Change 1: State Definition\n",
    "\n",
    "### Non-Compatible Version\n",
    "\n",
    "```python\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Defines a simple state structure\n",
    "- Uses TypedDict for type hints\n",
    "- Manages message history with add_messages reducer\n",
    "\n",
    "**What's missing:**\n",
    "- No MLflow metadata fields\n",
    "- No tracing context\n",
    "- No standardized format for deployment\n",
    "\n",
    "### MLflow-Compatible Version\n",
    "\n",
    "```python\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "```\n",
    "\n",
    "**What changed:**\n",
    "- Uses `ChatAgentState` from MLflow\n",
    "- No need to define custom state class\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "`ChatAgentState` is a pre-built state class that includes:\n",
    "\n",
    "```python\n",
    "# What ChatAgentState contains (conceptual):\n",
    "class ChatAgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # Same as before\n",
    "    # Plus MLflow-specific fields:\n",
    "    trace_id: Optional[str]           # For linking spans\n",
    "    run_id: Optional[str]             # For MLflow runs\n",
    "    metadata: Optional[Dict]          # For custom tracking\n",
    "    # And other internal fields for observability\n",
    "```\n",
    "\n",
    "**Real-World Impact:**\n",
    "\n",
    "```python\n",
    "# Non-Compatible: Just messages\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"Hello\")]\n",
    "}\n",
    "\n",
    "# MLflow-Compatible: Messages + Context\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"Hello\")],\n",
    "    \"trace_id\": \"tr_abc123\",          # Automatic tracing\n",
    "    \"run_id\": \"run_xyz789\",           # Experiment tracking\n",
    "    \"metadata\": {                      # Custom metadata\n",
    "        \"user_id\": \"user_456\",\n",
    "        \"session_id\": \"session_789\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "âœ… Automatic trace linking across spans  \n",
    "âœ… Integration with MLflow experiments  \n",
    "âœ… Standardized format for deployment  \n",
    "âœ… Built-in metadata support  \n",
    "\n",
    "---\n",
    "\n",
    "## Change 2: Node Function Signature\n",
    "\n",
    "### Non-Compatible Version\n",
    "\n",
    "```python\n",
    "def tool_calling_llm(state: State) -> State:\n",
    "    \"\"\"Execute tools based on tool calls in the last message\"\"\"\n",
    "    current_state = state[\"messages\"]\n",
    "    return {\"messages\": [llm_with_tools.invoke(current_state)]}\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Takes state as single parameter\n",
    "- Extracts messages\n",
    "- Invokes LLM\n",
    "- Returns updated state\n",
    "\n",
    "**What's missing:**\n",
    "- No configuration parameter\n",
    "- Can't pass runtime settings\n",
    "- No tracing context propagation\n",
    "\n",
    "### MLflow-Compatible Version\n",
    "\n",
    "```python\n",
    "def tool_calling_llm(state: ChatAgentState, config: RunnableConfig):\n",
    "    \"\"\"Execute tools based on tool calls in the last message\"\"\"\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "**What changed:**\n",
    "- Added `config: RunnableConfig` parameter\n",
    "- Passes config to model invocation\n",
    "- Uses model_runnable chain\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "The `config` parameter is the key to everything:\n",
    "\n",
    "```python\n",
    "# Without config: Static behavior\n",
    "def tool_calling_llm(state: State):\n",
    "    # Temperature is hardcoded\n",
    "    # Max tokens is fixed\n",
    "    # No runtime customization\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# With config: Dynamic behavior\n",
    "def tool_calling_llm(state: ChatAgentState, config: RunnableConfig):\n",
    "    # Temperature comes from config\n",
    "    # Max tokens comes from config\n",
    "    # Runtime customization enabled\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "**Real-World Impact:**\n",
    "\n",
    "```python\n",
    "# Scenario: Same agent, different use cases\n",
    "\n",
    "# Analytics request: Low temperature for precision\n",
    "analytics_config = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.1, \"max_tokens\": 2000}\n",
    ")\n",
    "analytics_response = agent.invoke(state, config=analytics_config)\n",
    "\n",
    "# Creative request: High temperature for creativity\n",
    "creative_config = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.8, \"max_tokens\": 1500}\n",
    ")\n",
    "creative_response = agent.invoke(state, config=creative_config)\n",
    "\n",
    "# Same agent, different behaviors!\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "âœ… Runtime configuration changes  \n",
    "âœ… A/B testing without redeployment  \n",
    "âœ… Use-case specific optimization  \n",
    "âœ… Tracing context propagation  \n",
    "\n",
    "---\n",
    "\n",
    "## Change 3: Runnable Chain Construction\n",
    "\n",
    "### Non-Compatible Version\n",
    "\n",
    "```python\n",
    "def tool_calling_llm(state: State) -> State:\n",
    "    current_state = state[\"messages\"]\n",
    "    return {\"messages\": [llm_with_tools.invoke(current_state)]}\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Directly invokes LLM\n",
    "- Manually extracts messages\n",
    "- Simple but not serializable\n",
    "\n",
    "### MLflow-Compatible Version\n",
    "\n",
    "```python\n",
    "# Create preprocessing chain\n",
    "preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "model_runnable = preprocessor | model\n",
    "\n",
    "def tool_calling_llm(state: ChatAgentState, config: RunnableConfig):\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "**What changed:**\n",
    "- Created explicit preprocessor step\n",
    "- Chained preprocessor with model using pipe (`|`)\n",
    "- Everything is now a Runnable\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "The chain pattern creates a serializable pipeline:\n",
    "\n",
    "```python\n",
    "# Non-Compatible: Not serializable\n",
    "def process(state):\n",
    "    messages = state[\"messages\"]  # Manual extraction\n",
    "    return llm.invoke(messages)\n",
    "\n",
    "# MLflow tries to serialize:\n",
    "# âŒ Can't save \"manual extraction\" logic\n",
    "# âŒ Can't track preprocessing step\n",
    "# âŒ Can't deploy to Model Serving\n",
    "\n",
    "# MLflow-Compatible: Fully serializable\n",
    "preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "model_runnable = preprocessor | model\n",
    "\n",
    "# MLflow can serialize:\n",
    "# âœ… Can save entire pipeline\n",
    "# âœ… Can track each step\n",
    "# âœ… Can deploy to Model Serving\n",
    "```\n",
    "\n",
    "**Execution Flow Comparison:**\n",
    "\n",
    "```python\n",
    "# Non-Compatible: Single-step\n",
    "state â†’ llm.invoke(state[\"messages\"]) â†’ response\n",
    "\n",
    "# MLflow-Compatible: Multi-step pipeline\n",
    "state â†’ preprocessor â†’ messages â†’ model â†’ response\n",
    "         â†‘              â†‘         â†‘        â†‘\n",
    "      Tracked     Logged    Traced  Monitored\n",
    "```\n",
    "\n",
    "**Real-World Impact:**\n",
    "\n",
    "```python\n",
    "# When you deploy to Databricks Model Serving:\n",
    "\n",
    "# Non-Compatible:\n",
    "# âŒ Deployment fails: \"Cannot serialize function\"\n",
    "# âŒ No preprocessing visibility\n",
    "# âŒ Can't inject config at deployment time\n",
    "\n",
    "# MLflow-Compatible:\n",
    "# âœ… Deployment succeeds\n",
    "# âœ… Full pipeline visibility\n",
    "# âœ… Runtime config injection works\n",
    "# âœ… Each step is traced and monitored\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "âœ… Complete serializability  \n",
    "âœ… Step-by-step tracing  \n",
    "âœ… Configuration injection  \n",
    "âœ… Deployment compatibility  \n",
    "\n",
    "---\n",
    "\n",
    "## Change 4: Tool Node Implementation\n",
    "\n",
    "### Non-Compatible Version\n",
    "\n",
    "```python\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Uses standard LangGraph ToolNode\n",
    "- Executes tools\n",
    "- Returns results\n",
    "\n",
    "**What's missing:**\n",
    "- No automatic span creation\n",
    "- No token tracking for tool calls\n",
    "- No MLflow metadata\n",
    "\n",
    "### MLflow-Compatible Version\n",
    "\n",
    "```python\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentToolNode\n",
    "\n",
    "builder.add_node(\"tools\", ChatAgentToolNode(tools=tools))\n",
    "```\n",
    "\n",
    "**What changed:**\n",
    "- Uses `ChatAgentToolNode` instead of `ToolNode`\n",
    "- Everything else stays the same\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "`ChatAgentToolNode` is a wrapper around `ToolNode` that adds observability:\n",
    "\n",
    "```python\n",
    "# Standard ToolNode execution (conceptual):\n",
    "class ToolNode:\n",
    "    def invoke(self, state):\n",
    "        tool_calls = extract_tool_calls(state)\n",
    "        for tool_call in tool_calls:\n",
    "            result = execute_tool(tool_call)  # Executed but not traced\n",
    "        return results\n",
    "\n",
    "# ChatAgentToolNode execution (conceptual):\n",
    "class ChatAgentToolNode:\n",
    "    def invoke(self, state):\n",
    "        tool_calls = extract_tool_calls(state)\n",
    "        for tool_call in tool_calls:\n",
    "            with mlflow.start_span(name=tool_call.name, span_type=\"TOOL\"):\n",
    "                span.set_inputs(tool_call.args)\n",
    "                result = execute_tool(tool_call)  # Executed AND traced\n",
    "                span.set_outputs(result)\n",
    "                span.set_attributes({\"execution_time\": duration})\n",
    "        return results\n",
    "```\n",
    "\n",
    "**Real-World Impact:**\n",
    "\n",
    "```python\n",
    "# Non-Compatible ToolNode:\n",
    "# Tool executes, but you don't know:\n",
    "# âŒ Which tool was called\n",
    "# âŒ What parameters were passed\n",
    "# âŒ How long it took\n",
    "# âŒ What results were returned\n",
    "# âŒ If it succeeded or failed\n",
    "\n",
    "# MLflow-Compatible ChatAgentToolNode:\n",
    "# Full observability:\n",
    "# âœ… Tool name logged\n",
    "# âœ… Input parameters captured\n",
    "# âœ… Execution time tracked\n",
    "# âœ… Output results stored\n",
    "# âœ… Success/failure status recorded\n",
    "\n",
    "# Example trace:\n",
    "{\n",
    "    \"span_id\": \"sp_004\",\n",
    "    \"name\": \"search_web\",\n",
    "    \"span_type\": \"TOOL\",\n",
    "    \"inputs\": {\"query\": \"OpenAI latest news\"},\n",
    "    \"outputs\": {\"results\": \"GPT-4 Turbo launched...\"},\n",
    "    \"attributes\": {\n",
    "        \"execution_time_ms\": 3350,\n",
    "        \"tool_status\": \"success\",\n",
    "        \"result_length\": 487\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "âœ… Automatic tool execution tracing  \n",
    "âœ… Input/output capture  \n",
    "âœ… Performance monitoring  \n",
    "âœ… Error tracking  \n",
    "\n",
    "---\n",
    "\n",
    "## Change 5: Node Wrapping\n",
    "\n",
    "### Non-Compatible Version\n",
    "\n",
    "```python\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Adds function directly to graph\n",
    "- Function executes normally\n",
    "\n",
    "**What's missing:**\n",
    "- No automatic span creation\n",
    "- Limited observability\n",
    "\n",
    "### MLflow-Compatible Version\n",
    "\n",
    "```python\n",
    "builder.add_node(\"tool_calling_llm\", RunnableLambda(tool_calling_llm))\n",
    "```\n",
    "\n",
    "**What changed:**\n",
    "- Wrapped function with `RunnableLambda`\n",
    "- Function becomes a Runnable\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "`RunnableLambda` transforms regular functions into observable, serializable Runnables:\n",
    "\n",
    "```python\n",
    "# Non-Compatible: Plain function\n",
    "def tool_calling_llm(state):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Properties:\n",
    "# - Not automatically traced\n",
    "# - Not easily serializable\n",
    "# - Can't inject config at runtime\n",
    "# - No standardized interface\n",
    "\n",
    "# MLflow-Compatible: Runnable-wrapped function\n",
    "tool_calling_llm_runnable = RunnableLambda(tool_calling_llm)\n",
    "\n",
    "# Properties:\n",
    "# - Automatically traced\n",
    "# - Fully serializable\n",
    "# - Config injection works\n",
    "# - Standardized Runnable interface\n",
    "```\n",
    "\n",
    "**Real-World Impact:**\n",
    "\n",
    "```python\n",
    "# When MLflow traces your agent:\n",
    "\n",
    "# Non-Compatible:\n",
    "# Graph execution: âœ… Traced\n",
    "# Node functions: âŒ Not traced (black boxes)\n",
    "# Result: Incomplete observability\n",
    "\n",
    "# MLflow-Compatible:\n",
    "# Graph execution: âœ… Traced\n",
    "# Node functions: âœ… Traced (full visibility)\n",
    "# Result: Complete observability\n",
    "\n",
    "# Example trace hierarchy:\n",
    "ðŸ“Š Trace\n",
    "â”œâ”€â”€ StateGraph.invoke (CHAIN)\n",
    "â”‚   â”œâ”€â”€ tool_calling_llm (LLM) â† Visible because of RunnableLambda\n",
    "â”‚   â”œâ”€â”€ search_web (TOOL)      â† Visible because of ChatAgentToolNode\n",
    "â”‚   â””â”€â”€ tool_calling_llm (LLM) â† Visible because of RunnableLambda\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "âœ… Automatic span creation  \n",
    "âœ… Node-level tracing  \n",
    "âœ… Serialization support  \n",
    "âœ… Config injection capability  \n",
    "\n",
    "---\n",
    "\n",
    "## Change 6: Agent Class Structure\n",
    "\n",
    "### Non-Compatible Version\n",
    "\n",
    "```python\n",
    "# Direct agent creation and usage\n",
    "catalog = \"agentic_ai\"\n",
    "schema = \"databricks\"\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT)\n",
    "uc_tool_names = [f\"{catalog}.{schema}.search_web\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools = [*uc_toolkit.tools]\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools)\n",
    "\n",
    "# Direct invocation\n",
    "for event in agent.stream({\"messages\": user_input}, stream_mode='values'):\n",
    "    event['messages'][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Creates agent directly\n",
    "- No wrapper class\n",
    "- Simple but limited\n",
    "\n",
    "**What's missing:**\n",
    "- No MLflow ChatAgent interface\n",
    "- Can't deploy to Model Serving\n",
    "- No standardized predict method\n",
    "- No configuration management\n",
    "\n",
    "### MLflow-Compatible Version\n",
    "\n",
    "```python\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.models import ModelConfig\n",
    "from mlflow.types.agent import ChatAgentMessage, ChatAgentResponse, ChatContext\n",
    "\n",
    "class DocsAgent(ChatAgent):\n",
    "    def __init__(self, config, tools):\n",
    "        \"\"\"Initialize agent with configuration\"\"\"\n",
    "        self.config = ModelConfig(development_config=config)\n",
    "        self.tools = tools\n",
    "        self.agent = self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        \"\"\"Build the agent graph with configured LLM\"\"\"\n",
    "        llm = ChatDatabricks(\n",
    "            endpoint=self.config.get(\"endpoint_name\"),\n",
    "            temperature=self.config.get(\"temperature\"),\n",
    "            max_tokens=self.config.get(\"max_tokens\"),\n",
    "        )\n",
    "        agent = create_tool_calling_agent(llm, tools=self.tools)\n",
    "        return agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \"\"\"Main prediction method - standardized interface\"\"\"\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        output = self.agent.invoke(request)\n",
    "        return ChatAgentResponse(**output)\n",
    "\n",
    "# Configuration-based initialization\n",
    "baseline_config = {\n",
    "    \"endpoint_name\": LLM_ENDPOINT,\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "\n",
    "AGENT = DocsAgent(baseline_config, tools)\n",
    "\n",
    "# Standardized invocation\n",
    "result = AGENT.predict([{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is the latest news on OpenAI?\"\n",
    "}])\n",
    "```\n",
    "\n",
    "**What changed:**\n",
    "- Created `ChatAgent` subclass\n",
    "- Separated configuration from code\n",
    "- Added standardized `predict()` method\n",
    "- Implemented `ModelConfig` management\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "The `ChatAgent` base class provides the deployment interface:\n",
    "\n",
    "```python\n",
    "# Non-Compatible: No standard interface\n",
    "# Different ways to use:\n",
    "agent.invoke(...)           # LangGraph way\n",
    "agent.stream(...)           # Streaming way\n",
    "agent.astream(...)          # Async way\n",
    "# Problem: No consistency for deployment\n",
    "\n",
    "# MLflow-Compatible: Standardized interface\n",
    "# Always use:\n",
    "result = agent.predict(messages, context)\n",
    "# MLflow knows how to:\n",
    "# - Log this agent\n",
    "# - Deploy this agent\n",
    "# - Monitor this agent\n",
    "```\n",
    "\n",
    "**Real-World Impact:**\n",
    "\n",
    "```python\n",
    "# Deployment to Databricks Model Serving:\n",
    "\n",
    "# Non-Compatible:\n",
    "# âŒ Can't deploy: No standard interface\n",
    "# âŒ Can't log: No ChatAgent inheritance\n",
    "# âŒ Can't monitor: No predict method\n",
    "\n",
    "mlflow.pyfunc.log_model(\"agent\", python_model=agent)\n",
    "# Error: Agent must inherit from ChatAgent\n",
    "\n",
    "# MLflow-Compatible:\n",
    "# âœ… Can deploy: Standard ChatAgent interface\n",
    "# âœ… Can log: Proper inheritance\n",
    "# âœ… Can monitor: Standard predict method\n",
    "\n",
    "mlflow.pyfunc.log_model(\n",
    "    \"agent\",\n",
    "    python_model=AGENT,\n",
    "    model_config=baseline_config\n",
    ")\n",
    "# Success! Ready for deployment\n",
    "\n",
    "# Once deployed, you can call it:\n",
    "response = requests.post(\n",
    "    \"https://model-serving-endpoint\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**Configuration Management Benefits:**\n",
    "\n",
    "```python\n",
    "# Non-Compatible: Hardcoded configuration\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    temperature=0.01,  # Hardcoded\n",
    "    max_tokens=1000    # Hardcoded\n",
    ")\n",
    "# To change: Must modify code and redeploy\n",
    "\n",
    "# MLflow-Compatible: External configuration\n",
    "baseline_config = {\n",
    "    \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "AGENT = DocsAgent(baseline_config, tools)\n",
    "\n",
    "# To change: Just update config, no code changes\n",
    "production_config = {\n",
    "    \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"temperature\": 0.1,   # Different setting\n",
    "    \"max_tokens\": 2000    # Different setting\n",
    "}\n",
    "# Can override at deployment time without code changes!\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "âœ… Deployment compatibility  \n",
    "âœ… Standardized interface  \n",
    "âœ… Configuration management  \n",
    "âœ… MLflow integration  \n",
    "\n",
    "---\n",
    "\n",
    "## Change 7: Message Format Handling\n",
    "\n",
    "### Non-Compatible Version\n",
    "\n",
    "```python\n",
    "# Messages are used directly\n",
    "user_input = \"Explain AI in 2 bullets\"\n",
    "for event in agent.stream({\"messages\": user_input}, stream_mode='values'):\n",
    "    event['messages'][-1].pretty_print()\n",
    "\n",
    "# Output is printed directly\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Simple string input\n",
    "- Direct output consumption\n",
    "- No format conversion\n",
    "\n",
    "**What's missing:**\n",
    "- No standardized message format\n",
    "- Not compatible with MLflow's ChatAgentMessage\n",
    "- Can't capture metadata\n",
    "\n",
    "### MLflow-Compatible Version\n",
    "\n",
    "```python\n",
    "result = AGENT.predict([{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is the latest news on OpenAI product releases?\"\n",
    "}])\n",
    "\n",
    "# result is ChatAgentResponse\n",
    "print(result.messages[-1].content)\n",
    "```\n",
    "\n",
    "**What changed:**\n",
    "- Messages follow standardized format\n",
    "- Input: `ChatAgentMessage` format\n",
    "- Output: `ChatAgentResponse` format\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "Standardized formats enable monitoring and compatibility:\n",
    "\n",
    "```python\n",
    "# Non-Compatible: Various formats\n",
    "\"Hello\"                          # String\n",
    "[\"Hello\"]                        # List\n",
    "{\"messages\": \"Hello\"}            # Dict\n",
    "HumanMessage(content=\"Hello\")    # LangChain object\n",
    "# Problem: Inconsistent, hard to monitor\n",
    "\n",
    "# MLflow-Compatible: Standardized format\n",
    "{\n",
    "    \"role\": \"user\",              # Always has role\n",
    "    \"content\": \"Hello\",          # Always has content\n",
    "    \"metadata\": {...}            # Optional metadata\n",
    "}\n",
    "# Benefit: Consistent, easy to monitor\n",
    "```\n",
    "\n",
    "**Real-World Impact:**\n",
    "\n",
    "```python\n",
    "# When deployed to Model Serving:\n",
    "\n",
    "# Non-Compatible:\n",
    "# Request can be anything:\n",
    "{\"input\": \"hello\"}\n",
    "{\"query\": \"hello\"}\n",
    "{\"text\": \"hello\"}\n",
    "# Problem: No consistency, hard to integrate\n",
    "\n",
    "# MLflow-Compatible:\n",
    "# Request is always:\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"hello\"}\n",
    "    ]\n",
    "}\n",
    "# Benefit: \n",
    "# - Frontend knows exact format\n",
    "# - Monitoring tools understand structure\n",
    "# - Easy integration with other systems\n",
    "```\n",
    "\n",
    "**Message Conversion Benefits:**\n",
    "\n",
    "```python\n",
    "class DocsAgent(ChatAgent):\n",
    "    def predict(self, messages: list[ChatAgentMessage], ...) -> ChatAgentResponse:\n",
    "        # Convert to internal format\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        \n",
    "        # Process\n",
    "        output = self.agent.invoke(request)\n",
    "        \n",
    "        # Convert to standard output\n",
    "        return ChatAgentResponse(**output)\n",
    "\n",
    "# Benefits:\n",
    "# âœ… Input validation\n",
    "# âœ… Format normalization\n",
    "# âœ… Metadata preservation\n",
    "# âœ… Monitoring compatibility\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "âœ… Standardized message format  \n",
    "âœ… Metadata support  \n",
    "âœ… Monitoring integration  \n",
    "âœ… API consistency  \n",
    "\n",
    "---\n",
    "\n",
    "## Complete Comparison Table\n",
    "\n",
    "| Aspect | Non-Compatible | MLflow-Compatible | Impact |\n",
    "|--------|----------------|-------------------|--------|\n",
    "| **State** | Custom `TypedDict` | `ChatAgentState` | Automatic tracing context |\n",
    "| **Node Signature** | `(state)` | `(state, config)` | Runtime configuration |\n",
    "| **Chain** | Direct invocation | Runnable pipeline | Serializability |\n",
    "| **Tool Node** | `ToolNode` | `ChatAgentToolNode` | Tool execution tracing |\n",
    "| **Node Wrapper** | Direct function | `RunnableLambda` | Node-level tracing |\n",
    "| **Agent Class** | No wrapper | `ChatAgent` subclass | Deployment interface |\n",
    "| **Messages** | Various formats | `ChatAgentMessage` | Standardization |\n",
    "| **Deployment** | âŒ Not possible | âœ… Ready | Production-ready |\n",
    "| **Tracing** | âŒ Manual only | âœ… Automatic | Full observability |\n",
    "| **Configuration** | âŒ Hardcoded | âœ… External | Runtime flexibility |\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Scenarios\n",
    "\n",
    "### Scenario 1: Debugging in Production\n",
    "\n",
    "**Non-Compatible Agent:**\n",
    "```python\n",
    "# User reports: \"Agent is slow\"\n",
    "# You have: Application logs showing request/response\n",
    "# You DON'T have:\n",
    "# - Which step was slow?\n",
    "# - Was it LLM or tool?\n",
    "# - How many tokens used?\n",
    "# - What were the intermediate outputs?\n",
    "\n",
    "# Your options:\n",
    "# 1. Add manual logging (requires code change + redeploy)\n",
    "# 2. Guess based on timing (unreliable)\n",
    "# 3. Try to reproduce locally (may not reproduce)\n",
    "```\n",
    "\n",
    "**MLflow-Compatible Agent:**\n",
    "```python\n",
    "# User reports: \"Agent is slow\"\n",
    "# You have: Complete trace with all spans\n",
    "\n",
    "trace = mlflow.get_trace(trace_id)\n",
    "\n",
    "# Immediately see:\n",
    "for span in trace.data.spans:\n",
    "    duration = span.end_time_unix_ms - span.start_time_unix_ms\n",
    "    print(f\"{span.name}: {duration}ms\")\n",
    "\n",
    "# Output:\n",
    "# tool_calling_llm: 2650ms\n",
    "# search_web: 4500ms â† Bottleneck found!\n",
    "# tool_calling_llm: 2100ms\n",
    "\n",
    "# Solution: Optimize search_web tool\n",
    "# No code changes needed to diagnose!\n",
    "```\n",
    "\n",
    "### Scenario 2: Cost Optimization\n",
    "\n",
    "**Non-Compatible Agent:**\n",
    "```python\n",
    "# CFO asks: \"How much are we spending on AI?\"\n",
    "# You have: API bills\n",
    "# You DON'T have:\n",
    "# - Token usage per request\n",
    "# - Token usage per user\n",
    "# - Token usage per use case\n",
    "# - Which requests are expensive?\n",
    "\n",
    "# Your options:\n",
    "# 1. Estimate based on averages (inaccurate)\n",
    "# 2. Add manual token tracking (code change)\n",
    "# 3. Wait for next month's bill (too late)\n",
    "```\n",
    "\n",
    "**MLflow-Compatible Agent:**\n",
    "```python\n",
    "# CFO asks: \"How much are we spending on AI?\"\n",
    "# You have: Complete token tracking\n",
    "\n",
    "traces = mlflow.search_traces(\n",
    "    experiment_names=[\"Production_Agent\"],\n",
    "    filter_string=\"timestamp > '2024-01-01'\"\n",
    ")\n",
    "\n",
    "total_tokens = sum(\n",
    "    t.info.token_usage['total_tokens'] \n",
    "    for t in traces \n",
    "    if t.info.token_usage\n",
    ")\n",
    "\n",
    "cost = total_tokens * 0.0001  # $0.10 per 1K tokens\n",
    "print(f\"Monthly cost: ${cost:.2f}\")\n",
    "\n",
    "# Breakdown by use case:\n",
    "for use_case in [\"analytics\", \"support\", \"creative\"]:\n",
    "    use_case_traces = mlflow.search_traces(\n",
    "        filter_string=f\"tags.use_case = '{use_case}'\"\n",
    "    )\n",
    "    use_case_tokens = sum(\n",
    "        t.info.token_usage['total_tokens']\n",
    "        for t in use_case_traces\n",
    "    )\n",
    "    print(f\"{use_case}: {use_case_tokens} tokens\")\n",
    "```\n",
    "\n",
    "### Scenario 3: A/B Testing\n",
    "\n",
    "**Non-Compatible Agent:**\n",
    "```python\n",
    "# Product Manager: \"Let's test temperature 0.1 vs 0.5\"\n",
    "# Your process:\n",
    "# 1. Create two separate agent instances\n",
    "# 2. Deploy both\n",
    "# 3. Manually route traffic\n",
    "# 4. Manually collect metrics\n",
    "# 5. Compare results manually\n",
    "\n",
    "agent_a = create_agent(temperature=0.1)  # Deploy\n",
    "agent_b = create_agent(temperature=0.5)  # Deploy again\n",
    "# Complex setup, manual tracking\n",
    "```\n",
    "\n",
    "**MLflow-Compatible Agent:**\n",
    "```python\n",
    "# Product Manager: \"Let's test temperature 0.1 vs 0.5\"\n",
    "# Your process:\n",
    "# 1. Use same agent\n",
    "# 2. Pass different config\n",
    "# 3. MLflow tracks everything\n",
    "\n",
    "agent = DocsAgent(config, tools)  # Deploy once\n",
    "\n",
    "# Variant A\n",
    "config_a = RunnableConfig(configurable={\"temperature\": 0.1})\n",
    "mlflow.set_tags({\"variant\": \"A\"})\n",
    "result_a = agent.invoke(state, config=config_a)\n",
    "\n",
    "# Variant B\n",
    "config_b = RunnableConfig(configurable={\"temperature\": 0.5})\n",
    "mlflow.set_tags({\"variant\": \"B\"})\n",
    "result_b = agent.invoke(state, config=config_b)\n",
    "\n",
    "# Compare automatically:\n",
    "traces_a = mlflow.search_traces(filter_string=\"tags.variant = 'A'\")\n",
    "traces_b = mlflow.search_traces(filter_string=\"tags.variant = 'B'\")\n",
    "\n",
    "# Metrics calculated automatically\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Migration Checklist\n",
    "\n",
    "If you have a non-compatible agent and want to make it MLflow-compatible, follow this checklist:\n",
    "\n",
    "### Step 1: Update State\n",
    "```python\n",
    "# Before\n",
    "from typing_extensions import TypedDict\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# After\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "# Use ChatAgentState directly\n",
    "```\n",
    "\n",
    "### Step 2: Update Node Functions\n",
    "```python\n",
    "# Before\n",
    "def tool_calling_llm(state: State) -> State:\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# After\n",
    "def tool_calling_llm(state: ChatAgentState, config: RunnableConfig):\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "### Step 3: Create Runnable Chain\n",
    "```python\n",
    "# Before\n",
    "# Direct invocation in function\n",
    "\n",
    "# After\n",
    "preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "model_runnable = preprocessor | model\n",
    "```\n",
    "\n",
    "### Step 4: Update Tool Node\n",
    "```python\n",
    "# Before\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "\n",
    "# After\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentToolNode\n",
    "builder.add_node(\"tools\", ChatAgentToolNode(tools=tools))\n",
    "```\n",
    "\n",
    "### Step 5: Wrap Node Functions\n",
    "```python\n",
    "# Before\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "\n",
    "# After\n",
    "builder.add_node(\"tool_calling_llm\", RunnableLambda(tool_calling_llm))\n",
    "```\n",
    "\n",
    "### Step 6: Create Agent Class\n",
    "```python\n",
    "# Before\n",
    "agent = create_tool_calling_agent(llm, tools)\n",
    "\n",
    "# After\n",
    "class DocsAgent(ChatAgent):\n",
    "    def __init__(self, config, tools):\n",
    "        self.config = ModelConfig(development_config=config)\n",
    "        self.tools = tools\n",
    "        self.agent = self._build_agent_from_config()\n",
    "    \n",
    "    def _build_agent_from_config(self):\n",
    "        llm = ChatDatabricks(\n",
    "            endpoint=self.config.get(\"endpoint_name\"),\n",
    "            temperature=self.config.get(\"temperature\"),\n",
    "            max_tokens=self.config.get(\"max_tokens\"),\n",
    "        )\n",
    "        return create_tool_calling_agent(llm, tools=self.tools)\n",
    "    \n",
    "    def predict(self, messages, context=None, custom_inputs=None):\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        output = self.agent.invoke(request)\n",
    "        return ChatAgentResponse(**output)\n",
    "\n",
    "agent = DocsAgent(config, tools)\n",
    "```\n",
    "\n",
    "### Step 7: Standardize Messages\n",
    "```python\n",
    "# Before\n",
    "result = agent.stream({\"messages\": \"Hello\"})\n",
    "\n",
    "# After\n",
    "result = agent.predict([{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hello\"\n",
    "}])\n",
    "```\n",
    "\n",
    "### Step 8: Enable Tracing\n",
    "```python\n",
    "# Add at the beginning\n",
    "import mlflow\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_experiment(\"My_Agent\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The transformation from a non-compatible to MLflow-compatible agent isn't about changing functionalityâ€”it's about adding the infrastructure needed for production deployment:\n",
    "\n",
    "### What Stays the Same\n",
    "âœ… Agent behavior and logic  \n",
    "âœ… Tool calling capabilities  \n",
    "âœ… Graph structure and flow  \n",
    "âœ… LLM integration  \n",
    "\n",
    "### What Gets Better\n",
    "âœ… **Observability**: Complete trace visibility  \n",
    "âœ… **Configuration**: Runtime customization  \n",
    "âœ… **Deployment**: Model Serving ready  \n",
    "âœ… **Monitoring**: Automatic metrics tracking  \n",
    "âœ… **Debugging**: Pinpoint issues instantly  \n",
    "âœ… **Cost Tracking**: Token-level visibility  \n",
    "âœ… **Standardization**: Consistent interfaces  \n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "Non-compatible agents work great for prototypes and demos. But when you need to:\n",
    "- Deploy to production\n",
    "- Monitor performance\n",
    "- Debug issues quickly\n",
    "- Track costs accurately\n",
    "- A/B test configurations\n",
    "- Scale with confidence\n",
    "\n",
    "...then MLflow-compatible agents are not just betterâ€”they're **essential**.\n",
    "\n",
    "The changes might seem small, but the impact is transformational. You're not just building an agent anymoreâ€”you're building a **production-grade AI system** with the observability and control needed to succeed at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "**Minimum Changes for MLflow Compatibility:**\n",
    "\n",
    "1. Use `ChatAgentState` instead of custom state\n",
    "2. Add `config: RunnableConfig` to node functions\n",
    "3. Create Runnable chain with preprocessor\n",
    "4. Use `ChatAgentToolNode` instead of `ToolNode`\n",
    "5. Wrap functions with `RunnableLambda`\n",
    "6. Create `ChatAgent` subclass with `predict()` method\n",
    "7. Standardize message formats\n",
    "8. Enable `mlflow.langchain.autolog()`\n",
    "\n",
    "**Result:** Production-ready agent with full observability!\n",
    "\n",
    "Happy building! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5559ce8f-d89c-4b50-bcc6-773174cd1e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''''''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5. Difference of MLFlow and Non MLFlow Approach",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
