{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515b618a-c313-401d-9402-93804602157a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "50fa7f8a-8764-4bb9-9968-48b681a0e4f1"
   },
   "source": [
    "# Build Simple Tool-Use AI Agents in LangGraph\n",
    "\n",
    "Here we will extend the capability of the previously built Augmented LLM with feedback from the tool execution back to the LLM node to process it and generate human-like answers to user queries.\n",
    "\n",
    "### Tool-based Agentic AI System\n",
    "\n",
    "- Dynamic Decision-Making: LLM determines whether to directly respond or invoke a tool based on the query context.\n",
    "- Seamless Tool Integration: External tools are integrated to handle specific tasks, such as real-time web queries or computations.\n",
    "- Workflow Flexibility: Conditional routing ensures efficient task delegation:\n",
    "  - Tool Required: Routes to tool execution.\n",
    "  - No Tool Required: Ends the workflow with an LLM response.\n",
    "- Feedback Loop: Incorporates a feedback loop to improve responses by combining LLM insights and tool outputs to further improve responses or call more tools if needed\n",
    "\n",
    "![](https://i.imgur.com/DHxiOLl.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04fe71a5-256e-4bcc-b31d-c5e3a5ebe04a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ff151ef1-fa30-482a-94da-8f49964afbc3"
   },
   "outputs": [],
   "source": [
    "# !pip install -qqqq langchain==0.3.14\n",
    "# !pip install -qqqq langchain-openai==0.3.0\n",
    "# !pip install -qqqq langchain-community==0.3.14\n",
    "# !pip install -qqqq langgraph==0.2.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847bdcd8-2c51-4b18-a049-b6323b067fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -U -qqqq mlflow databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks]\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672e0cf8-fe44-4f03-bb09-3fa3fcec3d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cv3JzCEx_PAd",
    "outputId": "e9fc3c4f-ce4e-47cd-ba27-6b890da5af92"
   },
   "outputs": [],
   "source": [
    "# from getpass import getpass\n",
    "# from typing import Annotated\n",
    "# from typing_extensions import TypedDict\n",
    "# from langgraph.graph.message import add_messages\n",
    "# from databricks_langchain.uc_ai import (\n",
    "#     DatabricksFunctionClient,\n",
    "#     UCFunctionToolkit,\n",
    "#     set_uc_function_client,\n",
    "# )\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.tools import tool\n",
    "# from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# from langgraph.graph import StateGraph, START, END\n",
    "# from typing import Annotated, Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bba0cd77-c565-44c0-a993-e54cfefab40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c570abd-d1b8-4704-be51-c5a077802a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow langchain langgraph==0.3.4 databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d8a469b-29b8-4a8a-b6c3-19b778bce399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d84ab82e-3af7-448f-bee8-35978fa83ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Annotated, Literal,Any, Optional, Sequence, Union\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.prebuilt.tool_node import ToolNode,tools_condition\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from mlflow.models import ModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b05057-bc1e-469a-a939-2fa8e558b52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Augment the LLM with tools (Testing purpose)\n",
    "\n",
    "Here we define our custom search tool and then bind it to the LLM to augment the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98941093-7b70-44d8-9005-bd84add74858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uc_client = DatabricksFunctionClient()\n",
    "set_uc_function_client(uc_client)\n",
    "\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT)\n",
    "\n",
    "catalog = \"agentic_ai\"\n",
    "schema = \"databricks\"\n",
    "\n",
    "uc_tool_names = [f\"{catalog}.{schema}.search_web\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools=[*uc_toolkit.tools]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4443d1e1-5072-4a12-beee-5870923ea772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm_with_tools.invoke('what is the latest news on nvidia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "670eee98-fb40-41b8-ae3d-44cc58f6dd20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Agent Not Compatible with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749c3310-4ec3-4e59-b28c-9445421adf87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5999f8d0-989f-4638-8ade-5c257cbadfe8"
   },
   "source": [
    "### State\n",
    "\n",
    "First, define the [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) of the graph.\n",
    "\n",
    "The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
    "\n",
    "Let's use the `TypedDict` class from python's `typing` module as our schema, which provides type hints for the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160067da-a560-4543-8619-d9e85c7f7711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f4953c-91b8-4984-9c58-da2312247a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Tool Calling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b287e2f-e0f2-42fd-a09e-4dbabb8a3fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: list\n",
    ") -> CompiledGraph:\n",
    "    llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "    # Augmented LLM with Tools Node function\n",
    "    def tool_calling_llm(state: State) -> State:\n",
    "        \"\"\"Execute tools based on tool calls in the last message\"\"\"\n",
    "        current_state = state[\"messages\"]\n",
    "        return {\"messages\": [llm_with_tools.invoke(current_state)]}\n",
    "    \n",
    "    # Build the graph\n",
    "    builder = StateGraph(State)\n",
    "    builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "    builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "    builder.add_edge(START, \"tool_calling_llm\")\n",
    "\n",
    "    # Conditional Edge\n",
    "    builder.add_conditional_edges(\n",
    "        \"tool_calling_llm\",\n",
    "        # If the latest message (result) from LLM is a tool call -> tools_condition routes to tools\n",
    "        # If the latest message (result) from LLM is a not a tool call -> tools_condition routes to END\n",
    "        tools_condition,\n",
    "        [\"tools\", END]\n",
    "    )\n",
    "    builder.add_edge(\"tools\", \"tool_calling_llm\") # this is the key feedback loop\n",
    "    builder.add_edge(\"tools\", END)\n",
    "    agent = builder.compile()\n",
    "\n",
    "    return agent\n",
    "\n",
    "catalog = \"agentic_ai\"\n",
    "schema = \"databricks\"\n",
    "\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT)\n",
    "\n",
    "uc_client = DatabricksFunctionClient()\n",
    "set_uc_function_client(uc_client)\n",
    "\n",
    "\n",
    "\n",
    "uc_tool_names = [f\"{catalog}.{schema}.search_web\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools=[*uc_toolkit.tools]\n",
    "agent = create_tool_calling_agent(llm, tools)\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e4984a-cb5e-482d-9763-26ffdc3ac398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Call the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e50234-b152-4710-9dfa-aa355e3d1e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_input = \"Explain AI in 2 bullets\"\n",
    "for event in agent.stream({\"messages\": user_input},\n",
    "                          stream_mode='values'):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d00f52-10d3-45a9-a59f-abfe41728328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_input = \"What is the latest news on OpenAI product releases. Provide the result in bullet points\"\n",
    "for event in agent.stream({\"messages\": user_input},\n",
    "                          stream_mode='values'):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59cfc10a-5040-4218-9fe6-1fba942fcd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "event['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45480ace-ea50-4bb7-b099-15c4532ca039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(event['messages'][-1].content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3441b763-be4b-4fa6-8493-0b80b2de57b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Agent  Compatible with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf95c09-fcb0-4779-9b21-4345d4bac973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Annotated, Literal,Any, Optional, Sequence, Union\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.prebuilt.tool_node import ToolNode,tools_condition\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: list\n",
    ") -> CompiledGraph:\n",
    "    \n",
    "    llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "    preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    # Augmented LLM with Tools Node function\n",
    "    def tool_calling_llm(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig):\n",
    "\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    # Build the graph\n",
    "    builder = StateGraph(ChatAgentState)\n",
    "    builder.add_node(\"tool_calling_llm\", RunnableLambda(tool_calling_llm))\n",
    "    builder.add_node(\"tools\", ChatAgentToolNode(tools=tools))\n",
    "    builder.add_edge(START, \"tool_calling_llm\")\n",
    "\n",
    "    # Conditional Edge\n",
    "    builder.add_conditional_edges(\n",
    "        \"tool_calling_llm\",\n",
    "        # If the latest message (result) from LLM is a tool call -> tools_condition routes to tools\n",
    "        # If the latest message (result) from LLM is a not a tool call -> tools_condition routes to END\n",
    "        tools_condition,\n",
    "        [\"tools\", END]\n",
    "    )\n",
    "    builder.add_edge(\"tools\", \"tool_calling_llm\") # this is the key feedback loop\n",
    "    builder.add_edge(\"tools\", END)\n",
    "    agent = builder.compile()\n",
    "\n",
    "    return agent\n",
    "\n",
    "class DocsAgent(ChatAgent):\n",
    "    def __init__(self, config, tools):\n",
    "        # Load config\n",
    "        # When this agent is deployed to Model Serving, the configuration loaded here is replaced with the config passed to mlflow.pyfunc.log_model(model_config=...)\n",
    "        self.config = ModelConfig(development_config=config)\n",
    "        self.tools = tools\n",
    "        self.agent = self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        llm = ChatDatabricks(\n",
    "            endpoint=self.config.get(\"endpoint_name\"),\n",
    "            temperature=self.config.get(\"temperature\"),\n",
    "            max_tokens=self.config.get(\"max_tokens\"),\n",
    "        )\n",
    "        agent = create_tool_calling_agent(\n",
    "            llm,\n",
    "            tools=self.tools\n",
    "        )\n",
    "        return agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # ChatAgent has a built-in helper method to help convert framework-specific messages, like langchain BaseMessage to a python dictionary\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        output = self.agent.invoke(request)\n",
    "        # Here 'output' is already a ChatAgentResponse, but to make the ChatAgent signature explicit for this demonstration we are returning a new instance\n",
    "        return ChatAgentResponse(**output)\n",
    "    \n",
    "\n",
    "catalog = \"agentic_ai\"\n",
    "schema = \"databricks\"\n",
    "\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "baseline_config = {\n",
    "    \"endpoint_name\": LLM_ENDPOINT,\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT)\n",
    "\n",
    "uc_client = DatabricksFunctionClient()\n",
    "set_uc_function_client(uc_client)\n",
    "\n",
    "\n",
    "\n",
    "uc_tool_names = [f\"{catalog}.{schema}.search_web\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools=[*uc_toolkit.tools]\n",
    "\n",
    "AGENT = DocsAgent(baseline_config, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01e487c-1bf6-4ae6-a652-69198f5cc6da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"What is the latest news on OpenAI product releases? Provide the result in bullet points\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7901fe76-af19-49f7-b03c-5fe552d394e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result.messages[-1].content))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8870256162052502,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "9. Capstone Project: Build_Simple_Tool_Use_AI_Agents",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
