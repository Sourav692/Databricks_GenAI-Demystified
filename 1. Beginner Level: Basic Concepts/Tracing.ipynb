{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f628f41f-6740-49ee-9652-a818953035c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# MLflow Tracing in the Databricks Agent: Complete Deep Dive\n",
    "\n",
    "Let me explain how MLflow tracing is integrated and used throughout this Databricks agent code, as it's crucial for observability and debugging in production AI systems.\n",
    "\n",
    "## MLflow Trace Integration Points\n",
    "\n",
    "Looking at the code, MLflow tracing is used in several key places:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5) -> list[dict[str, Any]]:\n",
    "    \"\"\"gets relevant documents for the query\"\"\"\n",
    "    # ... function implementation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The `@mlflow.trace` Decorator Explained\n",
    "\n",
    "### Basic Syntax Breakdown\n",
    "\n",
    "```python\n",
    "@mlflow.trace(\n",
    "    name=\"LittleIndex\",                           # Custom span name\n",
    "    span_type=mlflow.entities.SpanType.RETRIEVER  # Semantic span type\n",
    ")\n",
    "def find_relevant_documents(...):\n",
    "```\n",
    "\n",
    "### What Each Parameter Does\n",
    "\n",
    "**`name=\"LittleIndex\"`**\n",
    "- **Purpose**: Custom identifier for this operation in trace logs\n",
    "- **Visibility**: Shows up in MLflow UI as \"LittleIndex\" instead of function name\n",
    "- **Naming Convention**: Descriptive name that indicates what this component does\n",
    "- **Alternative**: If omitted, would default to function name `find_relevant_documents`\n",
    "\n",
    "**`span_type=mlflow.entities.SpanType.RETRIEVER`**\n",
    "- **Purpose**: Categorizes this operation semantically for better observability\n",
    "- **Built-in Types**: MLflow provides predefined span types:\n",
    "  ```python\n",
    "  mlflow.entities.SpanType.RETRIEVER    # For document/data retrieval\n",
    "  mlflow.entities.SpanType.LLM          # For language model calls\n",
    "  mlflow.entities.SpanType.CHAIN        # For workflow chains\n",
    "  mlflow.entities.SpanType.TOOL         # For tool executions\n",
    "  mlflow.entities.SpanType.AGENT        # For agent operations\n",
    "  ```\n",
    "- **Benefits**: Enables filtering and analysis by operation type in MLflow UI\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What Gets Traced Automatically\n",
    "\n",
    "### Function Execution Metrics\n",
    "\n",
    "```python\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5) -> list[dict[str, Any]]:\n",
    "    # MLflow automatically captures:\n",
    "    # - Start timestamp\n",
    "    # - End timestamp  \n",
    "    # - Duration\n",
    "    # - Input parameters: query=\"create delta table\", top_n=5\n",
    "    # - Return value: [{\"page_content\": \"...\", \"metadata\": {...}}, ...]\n",
    "    # - Any exceptions raised\n",
    "    \n",
    "    query_tfidf = doc_vectorizer.transform([query])\n",
    "    similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
    "    # ... rest of implementation\n",
    "    \n",
    "    return result  # This return value is captured in the trace\n",
    "```\n",
    "\n",
    "### Automatic Trace Data Collection\n",
    "\n",
    "**Input Parameters:**\n",
    "```json\n",
    "{\n",
    "  \"inputs\": {\n",
    "    \"query\": \"How do I create a Delta table?\",\n",
    "    \"top_n\": 5\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Output Data:**\n",
    "```json\n",
    "{\n",
    "  \"outputs\": [\n",
    "    {\n",
    "      \"page_content\": \"Delta Lake is an open-source storage framework...\",\n",
    "      \"metadata\": {\n",
    "        \"doc_uri\": \"https://docs.databricks.com/delta/...\",\n",
    "        \"score\": 0.85\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Performance Metrics:**\n",
    "```json\n",
    "{\n",
    "  \"start_time\": \"2024-01-15T10:30:45.123Z\",\n",
    "  \"end_time\": \"2024-01-15T10:30:45.445Z\", \n",
    "  \"duration_ms\": 322,\n",
    "  \"status\": \"OK\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Nested Tracing in the Agent Workflow\n",
    "\n",
    "### Complete Trace Hierarchy\n",
    "\n",
    "When a user asks a question, the trace hierarchy looks like this:\n",
    "\n",
    "```\n",
    "Agent Execution (Root Span)\n",
    "‚îú‚îÄ‚îÄ call_model (LLM Span)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Preprocessor (Processing)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ChatDatabricks LLM Call (LLM Span)\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ Input: [system_prompt, user_message]\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ Output: Response with tool_calls\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ Token Usage: input=150, output=75\n",
    "‚îú‚îÄ‚îÄ Tool Execution (Tool Span)  \n",
    "‚îÇ   ‚îî‚îÄ‚îÄ LittleIndex (RETRIEVER Span) ‚Üê Our custom trace\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ Input: query=\"Delta table\", top_n=5\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ TF-IDF Computation\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ Similarity Calculation  \n",
    "‚îÇ       ‚îî‚îÄ‚îÄ Output: 5 relevant documents\n",
    "‚îî‚îÄ‚îÄ call_model (Second LLM Span)\n",
    "    ‚îú‚îÄ‚îÄ Input: [conversation + tool_results]\n",
    "    ‚îî‚îÄ‚îÄ Output: Final synthesized answer\n",
    "```\n",
    "\n",
    "### Visual Representation in MLflow UI\n",
    "\n",
    "```\n",
    "üîÑ Agent Conversation                                    [2.3s]\n",
    "  ‚îú‚îÄ‚îÄ ü§ñ LLM Call (Initial)                            [0.8s]\n",
    "  ‚îú‚îÄ‚îÄ üîß Tool Execution                                 [0.3s] \n",
    "  ‚îÇ   ‚îî‚îÄ‚îÄ üìö LittleIndex (RETRIEVER)                   [0.3s]\n",
    "  ‚îÇ       ‚îú‚îÄ‚îÄ Query: \"Delta table creation\"\n",
    "  ‚îÇ       ‚îú‚îÄ‚îÄ Retrieved: 5 documents  \n",
    "  ‚îÇ       ‚îî‚îÄ‚îÄ Best Match Score: 0.89\n",
    "  ‚îú‚îÄ‚îÄ ü§ñ LLM Call (Synthesis)                          [1.2s]\n",
    "  ‚îî‚îÄ‚îÄ ‚úÖ Final Response                                 [Complete]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Integration with Other MLflow Features\n",
    "\n",
    "### Model Registry Integration\n",
    "\n",
    "```python\n",
    "# At the end of the notebook:\n",
    "AGENT = DocsAgent(baseline_config, tools)\n",
    "mlflow.models.set_model(AGENT)  # Links traces to the registered model\n",
    "```\n",
    "\n",
    "**What this enables:**\n",
    "- **Model Versioning**: Each model version has associated trace data\n",
    "- **Performance Tracking**: Compare trace performance across model versions\n",
    "- **Deployment Monitoring**: Production traces linked to specific model versions\n",
    "\n",
    "### Experiment Tracking\n",
    "\n",
    "```python\n",
    "# Implicit experiment tracking happens when traces are captured\n",
    "with mlflow.start_run(experiment_id=\"databricks_agent_experiment\"):\n",
    "    # All traces during this run are associated with the experiment\n",
    "    agent_response = AGENT.predict(messages)\n",
    "    \n",
    "    # Additional custom metrics can be logged\n",
    "    mlflow.log_metric(\"retrieval_count\", 5)\n",
    "    mlflow.log_metric(\"response_quality_score\", 0.92)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Custom Trace Enhancement\n",
    "\n",
    "### Adding Manual Trace Points\n",
    "\n",
    "You can enhance the existing tracing with custom spans:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5) -> list[dict[str, Any]]:\n",
    "    # Start a nested span for TF-IDF computation\n",
    "    with mlflow.start_span(name=\"TFIDF_Computation\") as span:\n",
    "        span.set_inputs({\"query\": query, \"vocabulary_size\": len(doc_vectorizer.vocabulary_)})\n",
    "        \n",
    "        query_tfidf = doc_vectorizer.transform([query])\n",
    "        span.set_outputs({\"query_vector_shape\": query_tfidf.shape})\n",
    "    \n",
    "    # Another span for similarity calculation\n",
    "    with mlflow.start_span(name=\"Similarity_Calculation\") as span:\n",
    "        span.set_inputs({\"documents_count\": tfidf_matrix.shape[0]})\n",
    "        \n",
    "        similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
    "        ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        span.set_outputs({\n",
    "            \"max_similarity\": float(similarities.max()),\n",
    "            \"min_similarity\": float(similarities.min()),\n",
    "            \"avg_similarity\": float(similarities.mean())\n",
    "        })\n",
    "    \n",
    "    # Document formatting span\n",
    "    with mlflow.start_span(name=\"Document_Formatting\") as span:\n",
    "        result = []\n",
    "        for idx, score in ranked_docs[:top_n]:\n",
    "            row = documents.iloc[idx]\n",
    "            content = row[\"content\"]\n",
    "            doc_entry = {\n",
    "                \"page_content\": content,\n",
    "                \"metadata\": {\n",
    "                    \"doc_uri\": row[\"doc_uri\"],\n",
    "                    \"score\": score,\n",
    "                },\n",
    "            }\n",
    "            result.append(doc_entry)\n",
    "            \n",
    "        span.set_outputs({\"formatted_documents\": len(result)})\n",
    "    \n",
    "    return result\n",
    "```\n",
    "\n",
    "### Enhanced Trace Hierarchy\n",
    "\n",
    "```\n",
    "üìö LittleIndex (RETRIEVER)                              [322ms]\n",
    "‚îú‚îÄ‚îÄ üßÆ TFIDF_Computation                               [45ms]\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Input: query=\"Delta table\", vocabulary_size=15420\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Output: query_vector_shape=(1, 15420)\n",
    "‚îú‚îÄ‚îÄ üîç Similarity_Calculation                          [267ms]  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Input: documents_count=5824\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Output: max=0.89, min=0.02, avg=0.15\n",
    "‚îî‚îÄ‚îÄ üìù Document_Formatting                             [10ms]\n",
    "    ‚îî‚îÄ‚îÄ Output: formatted_documents=5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Production Monitoring with Traces\n",
    "\n",
    "### Real-Time Performance Monitoring\n",
    "\n",
    "```python\n",
    "class AgentPerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.trace_data = []\n",
    "        \n",
    "    def analyze_traces(self):\n",
    "        # Query MLflow for recent traces\n",
    "        traces = mlflow.search_traces(\n",
    "            experiment_ids=[\"databricks_agent_experiment\"],\n",
    "            filter_string=\"span_type = 'RETRIEVER'\",\n",
    "            max_results=100\n",
    "        )\n",
    "        \n",
    "        for trace in traces:\n",
    "            # Extract performance metrics\n",
    "            duration = trace.info.execution_time_ms\n",
    "            query = trace.data.inputs.get(\"query\", \"\")\n",
    "            top_score = max([doc[\"metadata\"][\"score\"] \n",
    "                           for doc in trace.data.outputs], default=0)\n",
    "            \n",
    "            self.trace_data.append({\n",
    "                \"timestamp\": trace.info.start_time_ms,\n",
    "                \"duration_ms\": duration,\n",
    "                \"query_length\": len(query),\n",
    "                \"retrieval_quality\": top_score\n",
    "            })\n",
    "    \n",
    "    def detect_performance_issues(self):\n",
    "        df = pd.DataFrame(self.trace_data)\n",
    "        \n",
    "        # Alert on slow retrievals\n",
    "        slow_queries = df[df[\"duration_ms\"] > 500]\n",
    "        if len(slow_queries) > 0:\n",
    "            alert(f\"Found {len(slow_queries)} slow retrieval operations\")\n",
    "            \n",
    "        # Alert on low quality retrievals  \n",
    "        low_quality = df[df[\"retrieval_quality\"] < 0.3]\n",
    "        if len(low_quality) > 0:\n",
    "            alert(f\"Found {len(low_quality)} low quality retrievals\")\n",
    "```\n",
    "\n",
    "### Automated Quality Monitoring\n",
    "\n",
    "```python\n",
    "def monitor_retrieval_quality():\n",
    "    recent_traces = mlflow.search_traces(\n",
    "        filter_string=\"span_type = 'RETRIEVER' AND status = 'OK'\",\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=50\n",
    "    )\n",
    "    \n",
    "    quality_scores = []\n",
    "    for trace in recent_traces:\n",
    "        # Extract quality metrics from trace data\n",
    "        outputs = trace.data.outputs\n",
    "        if outputs and len(outputs) > 0:\n",
    "            max_score = max([doc[\"metadata\"][\"score\"] for doc in outputs])\n",
    "            quality_scores.append(max_score)\n",
    "    \n",
    "    avg_quality = sum(quality_scores) / len(quality_scores)\n",
    "    \n",
    "    # Log quality metric\n",
    "    mlflow.log_metric(\"avg_retrieval_quality\", avg_quality)\n",
    "    \n",
    "    # Alert if quality drops\n",
    "    if avg_quality < 0.5:  # Threshold\n",
    "        send_alert(f\"Retrieval quality dropped to {avg_quality:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Debugging with Traces\n",
    "\n",
    "### Identifying Retrieval Issues\n",
    "\n",
    "```python\n",
    "def debug_poor_retrieval(query: str):\n",
    "    # Find traces for similar queries\n",
    "    traces = mlflow.search_traces(\n",
    "        filter_string=f\"inputs.query LIKE '%{query}%'\",\n",
    "        order_by=[\"start_time DESC\"]\n",
    "    )\n",
    "    \n",
    "    for trace in traces:\n",
    "        print(f\"Query: {trace.data.inputs['query']}\")\n",
    "        print(f\"Duration: {trace.info.execution_time_ms}ms\")\n",
    "        print(f\"Status: {trace.info.status}\")\n",
    "        \n",
    "        if trace.data.outputs:\n",
    "            scores = [doc[\"metadata\"][\"score\"] for doc in trace.data.outputs]\n",
    "            print(f\"Similarity Scores: {scores}\")\n",
    "            print(f\"Best Match: {max(scores):.3f}\")\n",
    "            \n",
    "            # Analyze retrieved content\n",
    "            for i, doc in enumerate(trace.data.outputs[:3]):\n",
    "                print(f\"Doc {i+1} (score={doc['metadata']['score']:.3f}):\")\n",
    "                print(f\"  Content: {doc['page_content'][:100]}...\")\n",
    "                print(f\"  Source: {doc['metadata']['doc_uri']}\")\n",
    "        print(\"-\" * 50)\n",
    "```\n",
    "\n",
    "### Performance Analysis\n",
    "\n",
    "```python\n",
    "def analyze_retrieval_performance():\n",
    "    # Get all retrieval traces from last 24 hours\n",
    "    yesterday = datetime.now() - timedelta(days=1)\n",
    "    \n",
    "    traces = mlflow.search_traces(\n",
    "        filter_string=f\"span_type = 'RETRIEVER' AND start_time >= '{yesterday.isoformat()}'\",\n",
    "        order_by=[\"execution_time_ms DESC\"]\n",
    "    )\n",
    "    \n",
    "    # Performance statistics\n",
    "    durations = [trace.info.execution_time_ms for trace in traces]\n",
    "    \n",
    "    print(f\"Retrieval Performance Analysis:\")\n",
    "    print(f\"Total Retrievals: {len(durations)}\")\n",
    "    print(f\"Average Duration: {np.mean(durations):.1f}ms\")\n",
    "    print(f\"95th Percentile: {np.percentile(durations, 95):.1f}ms\")\n",
    "    print(f\"Slowest Query: {max(durations):.1f}ms\")\n",
    "    \n",
    "    # Find slowest queries\n",
    "    slowest_traces = sorted(traces, key=lambda t: t.info.execution_time_ms, reverse=True)[:5]\n",
    "    \n",
    "    print(\"\\nSlowest Queries:\")\n",
    "    for trace in slowest_traces:\n",
    "        query = trace.data.inputs.get(\"query\", \"Unknown\")\n",
    "        duration = trace.info.execution_time_ms\n",
    "        print(f\"  {duration}ms: {query[:50]}...\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. MLflow UI Navigation\n",
    "\n",
    "### Accessing Traces in Databricks\n",
    "\n",
    "**In Databricks Workspace:**\n",
    "1. Navigate to **Experiments** in the left sidebar\n",
    "2. Find your experiment (created automatically or explicitly)\n",
    "3. Click on a specific run\n",
    "4. Go to the **Traces** tab\n",
    "\n",
    "**Trace View Features:**\n",
    "- **Timeline View**: See execution flow and timing\n",
    "- **Tree View**: Hierarchical span structure  \n",
    "- **Inputs/Outputs**: Detailed parameter and return value inspection\n",
    "- **Performance Metrics**: Duration, token usage, success rates\n",
    "- **Error Analysis**: Exception details and stack traces\n",
    "\n",
    "### Filtering and Searching\n",
    "\n",
    "```python\n",
    "# Search for specific patterns\n",
    "slow_retrievals = mlflow.search_traces(\n",
    "    filter_string=\"span_type = 'RETRIEVER' AND execution_time_ms > 1000\"\n",
    ")\n",
    "\n",
    "failed_retrievals = mlflow.search_traces(\n",
    "    filter_string=\"span_type = 'RETRIEVER' AND status = 'ERROR'\"\n",
    ")\n",
    "\n",
    "high_quality_retrievals = mlflow.search_traces(\n",
    "    filter_string=\"span_type = 'RETRIEVER' AND outputs LIKE '%score\\\":0.9%'\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Integration with Agent Deployment\n",
    "\n",
    "### Model Serving Integration\n",
    "\n",
    "```python\n",
    "# When deployed to Model Serving, traces are automatically captured\n",
    "class DocsAgent(ChatAgent):\n",
    "    def predict(self, messages, context=None, custom_inputs=None):\n",
    "        # This entire method execution gets traced automatically\n",
    "        # Including the call to self.agent.invoke() which triggers\n",
    "        # all the nested spans we've configured\n",
    "        \n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        output = self.agent.invoke(request)\n",
    "        return ChatAgentResponse(**output)\n",
    "```\n",
    "\n",
    "**Production Trace Benefits:**\n",
    "- **Request Tracing**: Every API call to your deployed agent creates traces\n",
    "- **Performance SLAs**: Monitor if responses meet latency requirements  \n",
    "- **Quality Monitoring**: Track retrieval quality in production\n",
    "- **Error Detection**: Immediate notification of failures\n",
    "- **Usage Analytics**: Understand how users interact with your agent\n",
    "\n",
    "### Continuous Improvement Loop\n",
    "\n",
    "```python\n",
    "def improve_agent_based_on_traces():\n",
    "    # Analyze production traces\n",
    "    low_quality_traces = mlflow.search_traces(\n",
    "        filter_string=\"span_type = 'RETRIEVER' AND outputs LIKE '%score\\\":0.[0-3]%'\"\n",
    "    )\n",
    "    \n",
    "    # Extract queries that had poor retrieval\n",
    "    poor_queries = [trace.data.inputs[\"query\"] for trace in low_quality_traces]\n",
    "    \n",
    "    # Analyze common patterns\n",
    "    query_analysis = analyze_query_patterns(poor_queries)\n",
    "    \n",
    "    # Improve documentation corpus or retrieval algorithm\n",
    "    if query_analysis[\"missing_topics\"]:\n",
    "        expand_documentation(query_analysis[\"missing_topics\"])\n",
    "        \n",
    "    if query_analysis[\"synonym_issues\"]:\n",
    "        update_tfidf_preprocessing(query_analysis[\"synonyms\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: MLflow Tracing Value\n",
    "\n",
    "The MLflow tracing in this Databricks agent provides:\n",
    "\n",
    "### 1. **Development Benefits**\n",
    "- **Debugging**: See exactly what happens during retrieval\n",
    "- **Performance Optimization**: Identify bottlenecks in the pipeline\n",
    "- **Quality Analysis**: Understand when and why retrieval fails\n",
    "\n",
    "### 2. **Production Benefits**  \n",
    "- **Monitoring**: Real-time visibility into agent performance\n",
    "- **Alerting**: Automatic detection of issues\n",
    "- **Analytics**: Usage patterns and performance trends\n",
    "\n",
    "### 3. **Business Benefits**\n",
    "- **SLA Compliance**: Ensure response times meet requirements\n",
    "- **Quality Assurance**: Maintain consistent user experience\n",
    "- **Continuous Improvement**: Data-driven agent optimization\n",
    "\n",
    "The `@mlflow.trace` decorator on the `find_relevant_documents` function is a small addition that provides enormous value for operating AI agents at scale. It transforms a black-box function into a fully observable, debuggable, and optimizable component of your AI system.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a4de53f-41ee-4c02-89bf-9ba67b2f65f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We only explicitly decorated `find_relevant_documents` with `@mlflow.trace`, but MLflow actually traces much more. Let me explain how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dd7f037-70d3-4cda-b38b-407e73b60617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Excellent Question! MLflow Tracing Propagation Explained\n",
    "\n",
    "You're absolutely right to ask this! We only explicitly decorated `find_relevant_documents` with `@mlflow.trace`, but MLflow actually traces much more. Let me explain how this works.\n",
    "\n",
    "## 1. Automatic Tracing vs Manual Tracing\n",
    "\n",
    "### What We Manually Traced\n",
    "```python\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5):\n",
    "    # Only THIS function is manually traced\n",
    "```\n",
    "\n",
    "### What Gets Automatically Traced\n",
    "\n",
    "MLflow has **built-in automatic tracing** for many components, especially when they're part of LangChain workflows:\n",
    "\n",
    "```python\n",
    "# These get traced automatically (no decorator needed):\n",
    "- ChatDatabricks LLM calls\n",
    "- LangChain Runnable executions  \n",
    "- Tool invocations\n",
    "- Agent workflow steps\n",
    "- StateGraph node executions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. LangChain Integration Auto-Tracing\n",
    "\n",
    "### Automatic LLM Tracing\n",
    "\n",
    "```python\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=self.config.get(\"endpoint_name\"),\n",
    "    temperature=self.config.get(\"temperature\"),\n",
    "    max_tokens=self.config.get(\"max_tokens\"),\n",
    ")\n",
    "# ‚Üë This automatically gets traced when called!\n",
    "```\n",
    "\n",
    "**When `call_model` executes:**\n",
    "```python\n",
    "def call_model(state: ChatAgentState, config: RunnableConfig):\n",
    "    response = model_runnable.invoke(state, config)  # ‚Üê Auto-traced!\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "MLflow automatically captures:\n",
    "- **Span Type**: `mlflow.entities.SpanType.LLM`  \n",
    "- **Inputs**: The messages sent to the LLM\n",
    "- **Outputs**: The LLM's response (including tool calls)\n",
    "- **Metadata**: Token usage, model endpoint, parameters\n",
    "\n",
    "### Automatic Tool Execution Tracing\n",
    "\n",
    "```python\n",
    "@tool  # ‚Üê This @tool decorator enables auto-tracing\n",
    "def find_relevant_documents(query: str, top_n: int = 5):\n",
    "    # Even without @mlflow.trace, this would be traced\n",
    "    # because of the @tool decorator\n",
    "```\n",
    "\n",
    "The `@tool` decorator from LangChain automatically integrates with MLflow tracing:\n",
    "- **Span Type**: `mlflow.entities.SpanType.TOOL`\n",
    "- **Tool Name**: Function name or explicit name\n",
    "- **Inputs/Outputs**: Parameters and return values\n",
    "\n",
    "---\n",
    "\n",
    "## 3. StateGraph Workflow Auto-Tracing\n",
    "\n",
    "### Agent Node Tracing\n",
    "\n",
    "```python\n",
    "workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "```\n",
    "\n",
    "When this node executes:\n",
    "```python\n",
    "# MLflow automatically creates a span for:\n",
    "# - Node name: \"agent\" \n",
    "# - Node type: LangGraph node execution\n",
    "# - Nested spans for everything inside call_model\n",
    "```\n",
    "\n",
    "### Tools Node Tracing  \n",
    "\n",
    "```python\n",
    "workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "```\n",
    "\n",
    "`ChatAgentToolNode` has built-in MLflow integration:\n",
    "```python\n",
    "# Automatically traces:\n",
    "# - Tool selection logic\n",
    "# - Individual tool executions  \n",
    "# - Tool result formatting\n",
    "# - Error handling\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Trace Hierarchy Creation\n",
    "\n",
    "Here's how the complete trace hierarchy gets built:\n",
    "\n",
    "### Level 1: Agent Invocation (Auto-traced)\n",
    "```python\n",
    "# When you call:\n",
    "agent.invoke({\"messages\": [...]}, config)\n",
    "\n",
    "# MLflow automatically creates root span:\n",
    "# Name: \"Agent Execution\" or similar\n",
    "# Type: AGENT or CHAIN\n",
    "```\n",
    "\n",
    "### Level 2: Workflow Nodes (Auto-traced)\n",
    "```python\n",
    "# Each StateGraph node gets its own span:\n",
    "\n",
    "# Agent Node Span (Auto)\n",
    "workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "# ‚Üì Creates span with name=\"agent\"\n",
    "\n",
    "# Tools Node Span (Auto)  \n",
    "workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "# ‚Üì Creates span with name=\"tools\"\n",
    "```\n",
    "\n",
    "### Level 3: Model Calls (Auto-traced)\n",
    "```python\n",
    "# Inside call_model:\n",
    "def call_model(state, config):\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    # ‚Üë ChatDatabricks automatically traced\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "### Level 4: Tool Executions\n",
    "```python\n",
    "# Our manual trace (Enhanced):\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=RETRIEVER)\n",
    "def find_relevant_documents(...):\n",
    "    # Custom span with our chosen name and type\n",
    "\n",
    "# Without @mlflow.trace, would still be traced as:\n",
    "# Name: \"find_relevant_documents\" (function name)\n",
    "# Type: TOOL (from @tool decorator)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why We Added Custom Tracing\n",
    "\n",
    "### Without Custom Tracing\n",
    "```python\n",
    "@tool\n",
    "def find_relevant_documents(query: str, top_n: int = 5):\n",
    "    # Would appear in traces as:\n",
    "    # Name: \"find_relevant_documents\" \n",
    "    # Type: TOOL\n",
    "    # Generic tool execution span\n",
    "```\n",
    "\n",
    "### With Custom Tracing  \n",
    "```python\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5):\n",
    "    # Appears in traces as:\n",
    "    # Name: \"LittleIndex\" (more descriptive)\n",
    "    # Type: RETRIEVER (semantic meaning)\n",
    "    # Enhanced observability\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Demonstration: What Actually Gets Traced\n",
    "\n",
    "Let me show you the complete trace that gets generated:\n",
    "\n",
    "```python\n",
    "# User query triggers this trace hierarchy:\n",
    "\n",
    "üîÑ Agent.invoke() [AUTO-TRACED]                          [2.3s]\n",
    "‚îú‚îÄ‚îÄ üèóÔ∏è  StateGraph Execution [AUTO-TRACED]               [2.3s] \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ü§ñ Node: \"agent\" [AUTO-TRACED]                  [0.8s]\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìû call_model [AUTO-TRACED]                 [0.8s]\n",
    "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üîß RunnableLambda(preprocessor) [AUTO]  [0.01s]  \n",
    "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ü¶ô ChatDatabricks LLM [AUTO-TRACED]     [0.79s]\n",
    "‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ Input: [system_msg, user_msg]\n",
    "‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ Output: {tool_calls: [...]}\n",
    "‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ Tokens: in=150, out=45\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ üõ†Ô∏è  Node: \"tools\" [AUTO-TRACED]                  [0.3s]\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üîß ChatAgentToolNode [AUTO-TRACED]          [0.3s]\n",
    "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìö LittleIndex [MANUAL TRACE]           [0.3s] ‚Üê Our custom trace\n",
    "‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ Input: query=\"Delta table\", top_n=5\n",
    "‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ TF-IDF processing...\n",
    "‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ Output: 5 documents with scores\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ü§ñ Node: \"agent\" [AUTO-TRACED]                  [1.2s]\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ üìû call_model [AUTO-TRACED]                 [1.2s]  \n",
    "‚îÇ           ‚îî‚îÄ‚îÄ ü¶ô ChatDatabricks LLM [AUTO-TRACED]     [1.2s]\n",
    "‚îÇ               ‚îú‚îÄ‚îÄ Input: [conversation + tool_results]\n",
    "‚îÇ               ‚îî‚îÄ‚îÄ Output: Final synthesized answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. MLflow's Built-in Integrations\n",
    "\n",
    "### LangChain Components with Auto-Tracing\n",
    "\n",
    "```python\n",
    "# These have built-in MLflow tracing:\n",
    "from langchain_core.runnables import RunnableLambda     # ‚úÖ Auto-traced\n",
    "from langchain_core.language_models import BaseChatModel # ‚úÖ Auto-traced  \n",
    "from langchain_core.tools import BaseTool               # ‚úÖ Auto-traced\n",
    "from langgraph.graph import StateGraph                  # ‚úÖ Auto-traced\n",
    "from databricks_langchain import ChatDatabricks         # ‚úÖ Auto-traced\n",
    "```\n",
    "\n",
    "### How to Verify Auto-Tracing\n",
    "\n",
    "```python\n",
    "# You can check if a component supports auto-tracing:\n",
    "import mlflow\n",
    "\n",
    "# Enable tracing (usually on by default in Databricks)\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Now all LangChain components get traced automatically\n",
    "llm = ChatDatabricks(endpoint=\"llama-3-70b\")\n",
    "response = llm.invoke(\"Hello\")  # ‚Üê This creates a trace span\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Configuration-Level Tracing Control\n",
    "\n",
    "### Enabling/Disabling Auto-Tracing\n",
    "\n",
    "```python\n",
    "# Enable automatic tracing for all LangChain components\n",
    "mlflow.langchain.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=True, \n",
    "    log_models=True,\n",
    "    disable=False  # Set to True to disable auto-tracing\n",
    ")\n",
    "```\n",
    "\n",
    "### Trace Configuration in RunnableConfig\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    # Tracing configuration\n",
    "    callbacks=[MLflowCallbackHandler()],  # Explicit MLflow callback\n",
    "    metadata={\n",
    "        \"trace_enabled\": True,\n",
    "        \"trace_level\": \"detailed\"  # Custom metadata for tracing\n",
    "    }\n",
    ")\n",
    "\n",
    "# This config enables enhanced tracing for all components\n",
    "agent.invoke(messages, config=config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Custom vs Automatic Tracing Comparison\n",
    "\n",
    "### What You Get with Just Auto-Tracing\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def find_relevant_documents(query: str, top_n: int = 5):\n",
    "    # Auto-trace provides:\n",
    "    # ‚úÖ Function name as span name  \n",
    "    # ‚úÖ Input parameters\n",
    "    # ‚úÖ Return values\n",
    "    # ‚úÖ Execution time\n",
    "    # ‚úÖ Success/failure status\n",
    "    # ‚ùå Custom span name\n",
    "    # ‚ùå Semantic span type (RETRIEVER)\n",
    "    # ‚ùå Enhanced metadata\n",
    "```\n",
    "\n",
    "### What You Get with Custom Tracing\n",
    "\n",
    "```python\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5):\n",
    "    # Custom trace provides everything above PLUS:\n",
    "    # ‚úÖ Descriptive span name (\"LittleIndex\")\n",
    "    # ‚úÖ Semantic categorization (RETRIEVER)\n",
    "    # ‚úÖ Better filtering in MLflow UI\n",
    "    # ‚úÖ Professional trace readability\n",
    "    # ‚úÖ Type-specific analytics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Practical Example: Trace Without Custom Decorator\n",
    "\n",
    "Let me show you what would happen if we removed our custom trace:\n",
    "\n",
    "### Original Code (With Custom Trace)\n",
    "```python\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5):\n",
    "    # Implementation...\n",
    "```\n",
    "\n",
    "### Without Custom Trace\n",
    "```python\n",
    "@tool  # Only @tool decorator - still gets traced!\n",
    "def find_relevant_documents(query: str, top_n: int = 5):  \n",
    "    # Implementation...\n",
    "```\n",
    "\n",
    "### Resulting Trace Difference\n",
    "\n",
    "**With Custom Trace:**\n",
    "```\n",
    "üìö LittleIndex (RETRIEVER)                              [322ms]\n",
    "‚îú‚îÄ‚îÄ Input: {query: \"Delta table\", top_n: 5}\n",
    "‚îî‚îÄ‚îÄ Output: [{page_content: \"...\", metadata: {...}}, ...]\n",
    "```\n",
    "\n",
    "**Without Custom Trace (Auto-trace only):**\n",
    "```\n",
    "üîß find_relevant_documents (TOOL)                       [322ms]  \n",
    "‚îú‚îÄ‚îÄ Input: {query: \"Delta table\", top_n: 5}\n",
    "‚îî‚îÄ‚îÄ Output: [{page_content: \"...\", metadata: {...}}, ...]\n",
    "```\n",
    "\n",
    "**Key Differences:**\n",
    "- **Name**: \"LittleIndex\" vs \"find_relevant_documents\"\n",
    "- **Type**: \"RETRIEVER\" vs \"TOOL\"  \n",
    "- **Semantics**: Clear purpose vs generic tool\n",
    "\n",
    "---\n",
    "\n",
    "## 11. How to Verify All Tracing Works\n",
    "\n",
    "### Check MLflow Traces Programmatically\n",
    "\n",
    "```python\n",
    "def verify_tracing_coverage():\n",
    "    # Make a test call to the agent\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"What is Databricks?\"}]\n",
    "    \n",
    "    # Start explicit run to capture traces\n",
    "    with mlflow.start_run():\n",
    "        response = AGENT.predict(test_messages)\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "    \n",
    "    # Query for traces from this run\n",
    "    traces = mlflow.search_traces(filter_string=f\"request_id = '{run_id}'\")\n",
    "    \n",
    "    print(f\"Found {len(traces)} traces:\")\n",
    "    for trace in traces:\n",
    "        print(f\"- {trace.info.request_id}: {trace.data.span_type} - {trace.data.name}\")\n",
    "        print(f\"  Duration: {trace.info.execution_time_ms}ms\")\n",
    "        print(f\"  Status: {trace.info.status}\")\n",
    "        \n",
    "        if trace.data.inputs:\n",
    "            print(f\"  Inputs: {list(trace.data.inputs.keys())}\")\n",
    "        if trace.data.outputs:\n",
    "            print(f\"  Outputs available: {trace.data.outputs is not None}\")\n",
    "        print()\n",
    "\n",
    "# Run verification\n",
    "verify_tracing_coverage()\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "```\n",
    "Found 6 traces:\n",
    "- abc123: AGENT - Agent Execution\n",
    "  Duration: 2300ms\n",
    "  Status: OK\n",
    "  Inputs: ['messages']\n",
    "  Outputs available: True\n",
    "\n",
    "- def456: LLM - ChatDatabricks  \n",
    "  Duration: 800ms\n",
    "  Status: OK\n",
    "  Inputs: ['messages']\n",
    "  Outputs available: True\n",
    "\n",
    "- ghi789: RETRIEVER - LittleIndex\n",
    "  Duration: 322ms  \n",
    "  Status: OK\n",
    "  Inputs: ['query', 'top_n']\n",
    "  Outputs available: True\n",
    "\n",
    "- jkl012: LLM - ChatDatabricks\n",
    "  Duration: 1200ms\n",
    "  Status: OK  \n",
    "  Inputs: ['messages']\n",
    "  Outputs available: True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: The Complete Tracing Picture\n",
    "\n",
    "You're absolutely correct that we only manually added one trace decorator. Here's why we get comprehensive tracing:\n",
    "\n",
    "### 1. **MLflow Auto-Integration** \n",
    "- LangChain components have built-in MLflow support\n",
    "- `ChatDatabricks`, `StateGraph`, tool executions all auto-trace\n",
    "\n",
    "### 2. **Framework-Level Tracing**\n",
    "- LangGraph automatically traces workflow execution\n",
    "- Each node, edge, and decision point gets tracked\n",
    "\n",
    "### 3. **Tool Ecosystem Integration**\n",
    "- `@tool` decorator enables automatic tool tracing\n",
    "- `ChatAgentToolNode` has built-in observability\n",
    "\n",
    "### 4. **Our Custom Enhancement**\n",
    "- `@mlflow.trace` on `find_relevant_documents` provides:\n",
    "  - Better naming (\"LittleIndex\" vs function name)\n",
    "  - Semantic categorization (RETRIEVER vs generic TOOL)\n",
    "  - Enhanced filtering and analytics capabilities\n",
    "\n",
    "The beauty of this architecture is that you get **comprehensive observability by default**, and you can **enhance specific components** with custom tracing where it adds value. Our single `@mlflow.trace` decorator makes the retrieval component more professional and easier to monitor, while the framework handles tracing everything else automatically.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30648ed5-8e3c-4487-b098-60072932d263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Tracing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
