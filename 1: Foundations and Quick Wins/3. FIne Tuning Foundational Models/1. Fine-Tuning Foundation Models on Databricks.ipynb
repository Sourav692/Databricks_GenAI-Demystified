{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f573e7b7-dced-4208-8b68-6cfd6884f744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fine-Tuning Foundation Models on Databricks: A Complete Guide\n",
    "\n",
    "Fine-tuning large language models has become an essential skill for data scientists and ML engineers looking to customize AI models for specific use cases. Databricks' Foundation Model Fine-tuning capability (now part of Mosaic AI Model Training) makes this process more accessible through a streamlined API and integrated workflow. Let me walk you through how to get started.\n",
    "\n",
    "## What You'll Need\n",
    "\n",
    "Before diving in, ensure you have:\n",
    "- A Databricks workspace in the **us-east-1** or **us-west-2** AWS region\n",
    "- **Databricks Runtime 12.2 LTS ML** or higher\n",
    "- Training data formatted correctly (more on this below)\n",
    "- Access to a Databricks notebook environment\n",
    "\n",
    "Note that this feature is currently in Public Preview in the specified regions.\n",
    "\n",
    "## The Fine-Tuning Workflow\n",
    "\n",
    "### 1. Prepare Your Training Data\n",
    "\n",
    "The quality of your fine-tuned model depends heavily on your training data. Databricks requires data in a specific format, typically JSONL (JSON Lines) files stored in Unity Catalog Volumes. Each line should represent a training example structured appropriately for your use case.\n",
    "\n",
    "### 2. Set Up Your Environment\n",
    "\n",
    "Start by installing the required SDK and importing the necessary libraries:\n",
    "\n",
    "```python\n",
    "%pip install databricks_genai\n",
    "dbutils.library.restartPython()\n",
    "from databricks.model_training import foundation_model as fm\n",
    "```\n",
    "\n",
    "### 3. Launch Your Training Run\n",
    "\n",
    "Creating a training run is straightforward with the `create()` function. Here's a basic example:\n",
    "\n",
    "```python\n",
    "run = fm.create(\n",
    "    model='meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
    "    train_data_path='dbfs:/Volumes/main/my-directory/ift/train.jsonl',\n",
    "    register_to='main.my-directory',\n",
    "    training_duration='1ep'\n",
    ")\n",
    "```\n",
    "\n",
    "This code specifies:\n",
    "- **model**: Which foundation model to fine-tune (in this case, Llama 3.1 8B)\n",
    "- **train_data_path**: Location of your training dataset\n",
    "- **register_to**: The Unity Catalog location for saving checkpoints\n",
    "- **training_duration**: How long to train (here, 1 epoch)\n",
    "\n",
    "### 4. Monitor Progress\n",
    "\n",
    "Training times vary based on dataset size, model complexity, and GPU availability. You can track your run's status programmatically:\n",
    "\n",
    "```python\n",
    "run.get_events()\n",
    "```\n",
    "\n",
    "For production workloads, Databricks recommends using reserved compute for faster training times.\n",
    "\n",
    "## Understanding Your Results\n",
    "\n",
    "### Viewing Metrics\n",
    "\n",
    "Once training completes, navigate to the **Experiments** section in your Databricks workspace to review detailed metrics:\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Loss**: The primary metric showing training progress (lower is better)\n",
    "- **Evaluation Loss**: Helps identify overfitting, though it's not always a reliable indicator for instruction-tuning tasks\n",
    "\n",
    "**Evaluation Metrics** (if evaluation data provided):\n",
    "- **LanguageCrossEntropy**: Measures prediction accuracy on language modeling tasks\n",
    "- **LanguagePerplexity**: How well the model predicts the next token (lower scores indicate better performance)\n",
    "- **TokenAccuracy**: Token-level prediction accuracy (higher is better)\n",
    "\n",
    "### Watch for Overfitting\n",
    "\n",
    "While high accuracy seems desirable, values approaching 100% may indicate overfitting. For instruction-tuning tasks, the model might continue improving even when evaluation loss suggests overfitting, so consider multiple metrics together.\n",
    "\n",
    "## Evaluation Before Deployment\n",
    "\n",
    "Before putting your model into production, leverage **Mosaic AI Agent Evaluation** to compare multiple fine-tuned versions. This helps ensure you're deploying the best-performing model for your specific use case.\n",
    "\n",
    "## Deploying Your Fine-Tuned Model\n",
    "\n",
    "The training process automatically registers your model in Unity Catalog. To make it available for inference:\n",
    "\n",
    "1. Navigate to your model in **Unity Catalog**\n",
    "2. Click **\"Serve this model\"**\n",
    "3. Click **\"Create serving endpoint\"**\n",
    "4. Provide a name for your endpoint\n",
    "5. Click **\"Create\"**\n",
    "\n",
    "Your model is now ready to serve predictions through Mosaic AI Model Serving!\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "Databricks' Foundation Model Fine-tuning streamlines the process of customizing large language models:\n",
    "\n",
    "- **Simple API** for launching training runs\n",
    "- **Automatic model registration** in Unity Catalog\n",
    "- **Integrated metrics tracking** through MLflow\n",
    "- **Seamless deployment** through Model Serving\n",
    "\n",
    "This end-to-end workflow removes much of the complexity traditionally associated with fine-tuning, allowing you to focus on data quality and model performance rather than infrastructure management.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To deepen your understanding, check out the Databricks demo notebook on \"Instruction fine-tuning: Named Entity Recognition,\" which provides a complete example including data preparation, training configuration, and deployment steps.\n",
    "\n",
    "Whether you're adapting models for domain-specific knowledge, instruction-following, or specialized tasks, Foundation Model Fine-tuning on Databricks provides a robust platform for bringing custom AI capabilities to production.\n",
    "\n",
    "---\n",
    "\n",
    "*For the most up-to-date information and detailed API documentation, visit the [Databricks documentation](https://docs.databricks.com/aws/en/large-language-models/foundation-model-training/fine-tune-run-tutorial).*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Fine-Tuning Foundation Models on Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
