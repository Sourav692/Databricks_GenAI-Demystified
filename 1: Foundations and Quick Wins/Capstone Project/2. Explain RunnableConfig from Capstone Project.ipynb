{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c5bf32-4b90-4cb6-b50b-ffe5cdf9caa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Understanding RunnableConfig in MLflow: A Complete Guide to Dynamic Agent Configuration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "When building production-ready AI agents with LangChain and MLflow, one of the most powerful yet often misunderstood concepts is `RunnableConfig`. If you've ever wondered how to make your agent behave differently for various use cases without rewriting code, or how MLflow tracks and manages agent execution, `RunnableConfig` is the key.\n",
    "\n",
    "In this article, we'll demystify `RunnableConfig` by exploring what it is, why it matters, and how to use it to create flexible, multi-purpose AI agents. We'll use a practical tool-calling agent example to demonstrate these concepts.\n",
    "\n",
    "## What is RunnableConfig?\n",
    "\n",
    "`RunnableConfig` is a configuration object in LangChain that carries runtime parameters through your agent's execution chain. Think of it as a \"metadata backpack\" that travels with your request, containing instructions on *how* to process the data, not *what* data to process.\n",
    "\n",
    "### The Core Purpose\n",
    "\n",
    "While your messages contain the *content* (what the user asked), `RunnableConfig` contains the *context* (how to process that request):\n",
    "\n",
    "- **Model parameters**: Temperature, max tokens, etc.\n",
    "- **Execution metadata**: User session, use case type\n",
    "- **Runtime overrides**: Dynamic configuration changes\n",
    "- **Tracing context**: For MLflow observability\n",
    "\n",
    "## RunnableConfig in Action: A Simple Example\n",
    "\n",
    "Let's start with a basic example to see `RunnableConfig` in action:\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# Initialize your LLM\n",
    "llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\")\n",
    "\n",
    "# Create a configuration\n",
    "config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 500\n",
    "    },\n",
    "    metadata={\n",
    "        \"use_case\": \"analytics\",\n",
    "        \"user_id\": \"user_123\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use the config when invoking\n",
    "response = llm.invoke(\n",
    "    \"Explain machine learning in simple terms\",\n",
    "    config=config\n",
    ")\n",
    "```\n",
    "\n",
    "In this example, `config` travels with your request, telling the LLM to use low temperature (0.1) for precise responses and limit output to 500 tokens.\n",
    "\n",
    "## Why MLflow Agents Need RunnableConfig\n",
    "\n",
    "When building MLflow-compatible agents, `RunnableConfig` becomes essential. Let's understand why by comparing two implementations.\n",
    "\n",
    "### Non-Compatible Version (Without RunnableConfig)\n",
    "\n",
    "```python\n",
    "def tool_calling_llm(state: State) -> State:\n",
    "    \"\"\"Simple function without config support\"\"\"\n",
    "    current_state = state[\"messages\"]\n",
    "    return {\"messages\": [llm_with_tools.invoke(current_state)]}\n",
    "```\n",
    "\n",
    "**Limitations:**\n",
    "- No runtime configuration\n",
    "- No MLflow tracing context\n",
    "- Can't dynamically adjust behavior\n",
    "- Not serializable for deployment\n",
    "\n",
    "### MLflow-Compatible Version (With RunnableConfig)\n",
    "\n",
    "```python\n",
    "def tool_calling_llm(state: ChatAgentState, config: RunnableConfig):\n",
    "    \"\"\"MLflow-compatible function with config support\"\"\"\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Supports runtime configuration\n",
    "- Carries MLflow tracing context\n",
    "- Enables dynamic behavior changes\n",
    "- Fully serializable for deployment\n",
    "\n",
    "## How RunnableConfig Flows Through Your Agent\n",
    "\n",
    "Understanding the flow of `RunnableConfig` is crucial. Let's trace it through a complete agent execution.\n",
    "\n",
    "### The Agent Architecture\n",
    "\n",
    "```python\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def create_tool_calling_agent(model, tools):\n",
    "    # Bind tools to the model\n",
    "    llm_with_tools = model.bind_tools(tools=tools)\n",
    "    \n",
    "    # Create a preprocessing chain\n",
    "    preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "    \n",
    "    # Define the LLM node with config support\n",
    "    def tool_calling_llm(state: ChatAgentState, config: RunnableConfig):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    # Build the graph\n",
    "    builder = StateGraph(ChatAgentState)\n",
    "    builder.add_node(\"tool_calling_llm\", RunnableLambda(tool_calling_llm))\n",
    "    builder.add_node(\"tools\", ChatAgentToolNode(tools=tools))\n",
    "    builder.add_edge(START, \"tool_calling_llm\")\n",
    "    builder.add_conditional_edges(\"tool_calling_llm\", tools_condition, [\"tools\", END])\n",
    "    builder.add_edge(\"tools\", \"tool_calling_llm\")\n",
    "    \n",
    "    return builder.compile()\n",
    "```\n",
    "\n",
    "### The Config Flow Journey\n",
    "\n",
    "When you invoke the agent, here's how `RunnableConfig` travels:\n",
    "\n",
    "```\n",
    "User Request + Config\n",
    "        â†“\n",
    "    agent.invoke(state, config)\n",
    "        â†“\n",
    "    tool_calling_llm(state, config)\n",
    "        â†“\n",
    "    model_runnable.invoke(state, config)\n",
    "        â†“\n",
    "    preprocessor.invoke(state, config) â†’ extracts messages\n",
    "        â†“\n",
    "    model.invoke(messages, config) â†’ uses config parameters\n",
    "        â†“\n",
    "    Response\n",
    "```\n",
    "\n",
    "The beauty is that `config` flows through every step automatically, ensuring consistent behavior and tracing context.\n",
    "\n",
    "## Building a Multi-Purpose Agent with RunnableConfig\n",
    "\n",
    "Now let's create a practical agent that can serve different use cases by leveraging `RunnableConfig`.\n",
    "\n",
    "### Step 1: Define Use Case Configurations\n",
    "\n",
    "```python\n",
    "# Analytics: Precise, factual, detailed\n",
    "analytics_config = {\n",
    "    \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 2000,\n",
    "    \"system_prompt\": \"You are a data analyst. Provide precise, fact-based analysis.\",\n",
    "    \"use_case\": \"analytics\"\n",
    "}\n",
    "\n",
    "# Storytelling: Creative, engaging, moderate length\n",
    "storytelling_config = {\n",
    "    \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_tokens\": 1500,\n",
    "    \"system_prompt\": \"You are a creative storyteller. Craft engaging narratives.\",\n",
    "    \"use_case\": \"storytelling\"\n",
    "}\n",
    "\n",
    "# Technical Support: Helpful, concise, solution-focused\n",
    "support_config = {\n",
    "    \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"temperature\": 0.3,\n",
    "    \"max_tokens\": 800,\n",
    "    \"system_prompt\": \"You are a helpful support agent. Be empathetic and solution-focused.\",\n",
    "    \"use_case\": \"support\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 2: Create the Dynamic Agent\n",
    "\n",
    "```python\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.models import ModelConfig\n",
    "from mlflow.types.agent import ChatAgentMessage, ChatAgentResponse\n",
    "\n",
    "class DynamicDocsAgent(ChatAgent):\n",
    "    def __init__(self, config, tools):\n",
    "        self.base_config = config\n",
    "        self.config = ModelConfig(development_config=config)\n",
    "        self.tools = tools\n",
    "        self.agent = self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        llm = ChatDatabricks(\n",
    "            endpoint=self.config.get(\"endpoint_name\"),\n",
    "            temperature=self.config.get(\"temperature\"),\n",
    "            max_tokens=self.config.get(\"max_tokens\"),\n",
    "        )\n",
    "        agent = create_tool_calling_agent(llm, tools=self.tools)\n",
    "        return agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \n",
    "        # Convert messages\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        \n",
    "        # Create RunnableConfig from agent configuration\n",
    "        runtime_config = RunnableConfig(\n",
    "            configurable={\n",
    "                \"temperature\": self.config.get(\"temperature\"),\n",
    "                \"max_tokens\": self.config.get(\"max_tokens\"),\n",
    "            },\n",
    "            metadata={\n",
    "                \"use_case\": self.config.get(\"use_case\"),\n",
    "                \"system_prompt\": self.config.get(\"system_prompt\"),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Invoke agent with config\n",
    "        output = self.agent.invoke(request, config=runtime_config)\n",
    "        \n",
    "        return ChatAgentResponse(**output)\n",
    "```\n",
    "\n",
    "### Step 3: Use the Same Agent for Different Purposes\n",
    "\n",
    "```python\n",
    "# Initialize tools once\n",
    "catalog = \"agentic_ai\"\n",
    "schema = \"databricks\"\n",
    "uc_tool_names = [f\"{catalog}.{schema}.search_web\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools = [*uc_toolkit.tools]\n",
    "\n",
    "# Create different agent instances for different use cases\n",
    "analytics_agent = DynamicDocsAgent(analytics_config, tools)\n",
    "storytelling_agent = DynamicDocsAgent(storytelling_config, tools)\n",
    "support_agent = DynamicDocsAgent(support_config, tools)\n",
    "\n",
    "# Same query, different behaviors\n",
    "query = \"Explain artificial intelligence\"\n",
    "\n",
    "# Analytics response - precise and detailed\n",
    "analytics_response = analytics_agent.predict([\n",
    "    {\"role\": \"user\", \"content\": f\"{query} with technical accuracy\"}\n",
    "])\n",
    "print(\"Analytics Response:\")\n",
    "print(analytics_response.messages[-1].content)\n",
    "\n",
    "# Storytelling response - creative and engaging\n",
    "storytelling_response = storytelling_agent.predict([\n",
    "    {\"role\": \"user\", \"content\": f\"Tell me a story about {query}\"}\n",
    "])\n",
    "print(\"\\nStorytelling Response:\")\n",
    "print(storytelling_response.messages[-1].content)\n",
    "\n",
    "# Support response - helpful and concise\n",
    "support_response = support_agent.predict([\n",
    "    {\"role\": \"user\", \"content\": f\"I need help understanding {query}\"}\n",
    "])\n",
    "print(\"\\nSupport Response:\")\n",
    "print(support_response.messages[-1].content)\n",
    "```\n",
    "\n",
    "## Dynamic Configuration Updates\n",
    "\n",
    "One of the most powerful features is the ability to update configurations dynamically without recreating the agent. This is particularly useful when you want to switch between use cases or fine-tune behavior on the fly.\n",
    "\n",
    "### Complete Configuration Update\n",
    "\n",
    "You can completely replace the agent's configuration to switch between use cases:\n",
    "\n",
    "```python\n",
    "class ConfigurableDocsAgent(ChatAgent):\n",
    "    def __init__(self, base_config, tools):\n",
    "        self.base_config = base_config\n",
    "        self.tools = tools\n",
    "        self.agent = None\n",
    "        self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        \"\"\"Build agent with current configuration\"\"\"\n",
    "        llm = ChatDatabricks(\n",
    "            endpoint=self.base_config.get(\"endpoint_name\"),\n",
    "            temperature=self.base_config.get(\"temperature\", 0.7),\n",
    "            max_tokens=self.base_config.get(\"max_tokens\", 1000),\n",
    "        )\n",
    "        self.agent = create_tool_calling_agent(llm, tools=self.tools)\n",
    "\n",
    "    def update_config(self, new_config):\n",
    "        \"\"\"\n",
    "        Completely update configuration and rebuild agent\n",
    "        Use this to switch between different use cases\n",
    "        \"\"\"\n",
    "        self.base_config = new_config\n",
    "        self._build_agent_from_config()\n",
    "        print(f\"âœ… Agent updated to use case: {new_config.get('use_case', 'general')}\")\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        \n",
    "        runtime_config = RunnableConfig(\n",
    "            configurable={\n",
    "                \"temperature\": self.base_config.get(\"temperature\"),\n",
    "                \"max_tokens\": self.base_config.get(\"max_tokens\"),\n",
    "            },\n",
    "            metadata={\n",
    "                \"use_case\": self.base_config.get(\"use_case\", \"general\"),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        output = self.agent.invoke(request, config=runtime_config)\n",
    "        return ChatAgentResponse(**output)\n",
    "```\n",
    "\n",
    "### Using Complete Configuration Update\n",
    "\n",
    "```python\n",
    "# Initialize tools\n",
    "tools = [*uc_toolkit.tools]\n",
    "\n",
    "# Start with analytics configuration\n",
    "agent = ConfigurableDocsAgent(analytics_config, tools)\n",
    "\n",
    "# Use for analytics\n",
    "response = agent.predict([\n",
    "    {\"role\": \"user\", \"content\": \"Analyze the AI market trends\"}\n",
    "])\n",
    "print(\"Analytics mode:\", response.messages[-1].content[:100])\n",
    "\n",
    "# Switch to storytelling mode\n",
    "agent.update_config(storytelling_config)\n",
    "\n",
    "# Now the same agent behaves differently\n",
    "response = agent.predict([\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about AI market trends\"}\n",
    "])\n",
    "print(\"Storytelling mode:\", response.messages[-1].content[:100])\n",
    "\n",
    "# Switch to support mode\n",
    "agent.update_config(support_config)\n",
    "\n",
    "# Again, different behavior\n",
    "response = agent.predict([\n",
    "    {\"role\": \"user\", \"content\": \"I need help understanding AI market trends\"}\n",
    "])\n",
    "print(\"Support mode:\", response.messages[-1].content[:100])\n",
    "```\n",
    "\n",
    "### Partial Configuration Update\n",
    "\n",
    "Sometimes you don't want to replace the entire configurationâ€”you just want to tweak a few parameters. Partial updates allow you to modify specific settings while keeping others intact:\n",
    "\n",
    "```python\n",
    "class ConfigurableDocsAgent(ChatAgent):\n",
    "    # ... (previous code remains the same)\n",
    "    \n",
    "    def partial_update_config(self, config_updates):\n",
    "        \"\"\"\n",
    "        Partially update configuration without replacing everything\n",
    "        Use this to fine-tune specific parameters\n",
    "        \"\"\"\n",
    "        # Merge updates into existing config\n",
    "        self.base_config.update(config_updates)\n",
    "        self._build_agent_from_config()\n",
    "        \n",
    "        updated_keys = \", \".join(config_updates.keys())\n",
    "        print(f\"âœ… Updated configuration: {updated_keys}\")\n",
    "```\n",
    "\n",
    "### Using Partial Configuration Update\n",
    "\n",
    "```python\n",
    "# Start with analytics configuration\n",
    "agent = ConfigurableDocsAgent(analytics_config, tools)\n",
    "print(f\"Initial config - Temperature: {agent.base_config['temperature']}, \"\n",
    "      f\"Max tokens: {agent.base_config['max_tokens']}\")\n",
    "\n",
    "# Test with initial config\n",
    "response = agent.predict([\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n",
    "])\n",
    "print(\"Initial response length:\", len(response.messages[-1].content))\n",
    "\n",
    "# Partial update: Just increase creativity slightly\n",
    "agent.partial_update_config({\n",
    "    \"temperature\": 0.3\n",
    "})\n",
    "print(f\"After partial update - Temperature: {agent.base_config['temperature']}, \"\n",
    "      f\"Max tokens: {agent.base_config['max_tokens']}\")  # max_tokens unchanged\n",
    "\n",
    "response = agent.predict([\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n",
    "])\n",
    "print(\"Updated response length:\", len(response.messages[-1].content))\n",
    "\n",
    "# Another partial update: Reduce response length\n",
    "agent.partial_update_config({\n",
    "    \"max_tokens\": 500\n",
    "})\n",
    "print(f\"After second update - Temperature: {agent.base_config['temperature']}, \"\n",
    "      f\"Max tokens: {agent.base_config['max_tokens']}\")\n",
    "\n",
    "# Partial update multiple parameters at once\n",
    "agent.partial_update_config({\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 1200,\n",
    "    \"system_prompt\": \"You are a balanced assistant providing both analytical and creative insights.\"\n",
    "})\n",
    "print(f\"After multi-param update - Temperature: {agent.base_config['temperature']}, \"\n",
    "      f\"Max tokens: {agent.base_config['max_tokens']}\")\n",
    "```\n",
    "\n",
    "### Configuration Update Comparison\n",
    "\n",
    "Here's when to use each approach:\n",
    "\n",
    "```python\n",
    "# Scenario 1: Switching between completely different use cases\n",
    "# Use: update_config()\n",
    "agent = ConfigurableDocsAgent(analytics_config, tools)\n",
    "agent.update_config(storytelling_config)  # Complete switch\n",
    "\n",
    "# Scenario 2: Fine-tuning for a specific request\n",
    "# Use: partial_update_config()\n",
    "agent = ConfigurableDocsAgent(analytics_config, tools)\n",
    "agent.partial_update_config({\"temperature\": 0.2})  # Just tweak temperature\n",
    "\n",
    "# Scenario 3: A/B testing different parameters\n",
    "# Use: partial_update_config()\n",
    "agent = ConfigurableDocsAgent(analytics_config, tools)\n",
    "\n",
    "# Test version A\n",
    "agent.partial_update_config({\"temperature\": 0.1, \"max_tokens\": 1000})\n",
    "response_a = agent.predict([{\"role\": \"user\", \"content\": \"Test query\"}])\n",
    "\n",
    "# Test version B\n",
    "agent.partial_update_config({\"temperature\": 0.3, \"max_tokens\": 1500})\n",
    "response_b = agent.predict([{\"role\": \"user\", \"content\": \"Test query\"}])\n",
    "\n",
    "# Compare results\n",
    "print(f\"Version A length: {len(response_a.messages[-1].content)}\")\n",
    "print(f\"Version B length: {len(response_b.messages[-1].content)}\")\n",
    "```\n",
    "\n",
    "### Real-World Example: Adaptive Agent\n",
    "\n",
    "Here's a practical example of an agent that adapts its configuration based on the query complexity:\n",
    "\n",
    "```python\n",
    "class AdaptiveAgent(ConfigurableDocsAgent):\n",
    "    def smart_predict(self, messages: list[ChatAgentMessage]) -> ChatAgentResponse:\n",
    "        \"\"\"\n",
    "        Automatically adjust configuration based on query characteristics\n",
    "        \"\"\"\n",
    "        query = messages[0].get(\"content\", \"\")\n",
    "        \n",
    "        # Analyze query complexity\n",
    "        word_count = len(query.split())\n",
    "        has_technical_terms = any(term in query.lower() \n",
    "                                  for term in [\"algorithm\", \"architecture\", \"implementation\"])\n",
    "        \n",
    "        # Adjust configuration based on query\n",
    "        if word_count > 50 or has_technical_terms:\n",
    "            # Complex query: more detailed response\n",
    "            self.partial_update_config({\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_tokens\": 2000\n",
    "            })\n",
    "            print(\"ðŸ“Š Detected complex query - using detailed configuration\")\n",
    "        else:\n",
    "            # Simple query: concise response\n",
    "            self.partial_update_config({\n",
    "                \"temperature\": 0.4,\n",
    "                \"max_tokens\": 800\n",
    "            })\n",
    "            print(\"ðŸ’¬ Detected simple query - using concise configuration\")\n",
    "        \n",
    "        # Make prediction with adjusted config\n",
    "        return self.predict(messages)\n",
    "\n",
    "# Usage\n",
    "adaptive_agent = AdaptiveAgent(analytics_config, tools)\n",
    "\n",
    "# Simple query - automatically uses concise config\n",
    "response1 = adaptive_agent.smart_predict([\n",
    "    {\"role\": \"user\", \"content\": \"What is AI?\"}\n",
    "])\n",
    "\n",
    "# Complex query - automatically uses detailed config\n",
    "response2 = adaptive_agent.smart_predict([\n",
    "    {\"role\": \"user\", \"content\": \"Explain the transformer architecture and its implementation details in modern large language models\"}\n",
    "])\n",
    "```\n",
    "\n",
    "### Benefits of Dynamic Configuration Updates\n",
    "\n",
    "**1. Resource Efficiency**\n",
    "```python\n",
    "# Instead of creating multiple agent instances\n",
    "analytics_agent = DynamicDocsAgent(analytics_config, tools)\n",
    "storytelling_agent = DynamicDocsAgent(storytelling_config, tools)\n",
    "support_agent = DynamicDocsAgent(support_config, tools)\n",
    "\n",
    "# Use one agent with dynamic updates\n",
    "agent = ConfigurableDocsAgent(analytics_config, tools)\n",
    "agent.update_config(storytelling_config)  # Switch on demand\n",
    "agent.update_config(support_config)       # Switch again\n",
    "```\n",
    "\n",
    "**2. Easy A/B Testing**\n",
    "```python\n",
    "agent = ConfigurableDocsAgent(analytics_config, tools)\n",
    "\n",
    "# Test different temperature values\n",
    "for temp in [0.1, 0.3, 0.5, 0.7]:\n",
    "    agent.partial_update_config({\"temperature\": temp})\n",
    "    response = agent.predict([{\"role\": \"user\", \"content\": \"Test query\"}])\n",
    "    print(f\"Temperature {temp}: {len(response.messages[-1].content)} chars\")\n",
    "```\n",
    "\n",
    "**3. Real-time Optimization**\n",
    "```python\n",
    "agent = ConfigurableDocsAgent(analytics_config, tools)\n",
    "\n",
    "# Start with default settings\n",
    "response = agent.predict([{\"role\": \"user\", \"content\": \"Explain machine learning\"}])\n",
    "\n",
    "# If response is too short, increase max_tokens\n",
    "if len(response.messages[-1].content) < 500:\n",
    "    agent.partial_update_config({\"max_tokens\": 1500})\n",
    "    response = agent.predict([{\"role\": \"user\", \"content\": \"Explain machine learning\"}])\n",
    "```\n",
    "\n",
    "## How RunnableConfig Enables Dynamic Behavior\n",
    "\n",
    "The key insight is that **the same agent graph** produces different outputs based on the `RunnableConfig` it receives. Let's see how:\n",
    "\n",
    "### Temperature Impact\n",
    "\n",
    "```python\n",
    "# Low temperature (0.1) for analytics\n",
    "config_precise = RunnableConfig(configurable={\"temperature\": 0.1})\n",
    "# Output: \"Artificial Intelligence (AI) refers to systems designed to \n",
    "#          perform tasks that typically require human intelligence...\"\n",
    "\n",
    "# High temperature (0.8) for storytelling  \n",
    "config_creative = RunnableConfig(configurable={\"temperature\": 0.8})\n",
    "# Output: \"Imagine a world where machines dream, think, and learn just \n",
    "#          like we do! This magical realm is called Artificial Intelligence...\"\n",
    "```\n",
    "\n",
    "### Max Tokens Impact\n",
    "\n",
    "```python\n",
    "# Longer responses for detailed analysis\n",
    "config_detailed = RunnableConfig(configurable={\"max_tokens\": 2000})\n",
    "\n",
    "# Concise responses for quick support\n",
    "config_concise = RunnableConfig(configurable={\"max_tokens\": 500})\n",
    "```\n",
    "\n",
    "### Metadata for Business Logic\n",
    "\n",
    "```python\n",
    "config_with_metadata = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.5},\n",
    "    metadata={\n",
    "        \"use_case\": \"customer_support\",\n",
    "        \"priority\": \"high\",\n",
    "        \"language\": \"en\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Later in your code, you can access this metadata\n",
    "# to make decisions about post-processing, logging, etc.\n",
    "```\n",
    "\n",
    "## RunnableConfig and MLflow Tracing\n",
    "\n",
    "One of the most powerful aspects of `RunnableConfig` is how it integrates with MLflow tracing. Every configuration parameter becomes part of your trace, enabling powerful debugging and analysis.\n",
    "\n",
    "### Tracing Configuration in Action\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Enable tracing\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_experiment(\"Agent_Configuration_Demo\")\n",
    "\n",
    "# Execute with different configs\n",
    "with mlflow.start_run(run_name=\"analytics_execution\"):\n",
    "    config = RunnableConfig(\n",
    "        configurable={\"temperature\": 0.1, \"max_tokens\": 2000},\n",
    "        metadata={\"use_case\": \"analytics\"}\n",
    "    )\n",
    "    result = agent.invoke({\"messages\": [...]}, config=config)\n",
    "\n",
    "# The trace will show:\n",
    "# - All configuration parameters used\n",
    "# - How they affected execution\n",
    "# - Performance metrics per configuration\n",
    "```\n",
    "\n",
    "### What Gets Traced\n",
    "\n",
    "When you use `RunnableConfig`, MLflow automatically captures:\n",
    "\n",
    "```python\n",
    "# Trace attributes include:\n",
    "{\n",
    "    \"mlflow.chat.temperature\": 0.1,\n",
    "    \"mlflow.chat.max_tokens\": 2000,\n",
    "    \"use_case\": \"analytics\",\n",
    "    \"execution_time_ms\": 2450,\n",
    "    \"total_tokens\": 1235\n",
    "}\n",
    "```\n",
    "\n",
    "This allows you to:\n",
    "- Compare performance across different configurations\n",
    "- Identify which settings work best for each use case\n",
    "- Debug configuration-related issues\n",
    "- Optimize costs based on token usage patterns\n",
    "\n",
    "## Best Practices for Using RunnableConfig\n",
    "\n",
    "### 1. Separate Configuration from Logic\n",
    "\n",
    "```python\n",
    "# Good: Configuration is external\n",
    "config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "agent = DynamicDocsAgent(config, tools)\n",
    "\n",
    "# Bad: Hardcoded values\n",
    "def agent_function(state):\n",
    "    llm = ChatDatabricks(temperature=0.1)  # Hardcoded!\n",
    "```\n",
    "\n",
    "### 2. Use Meaningful Metadata\n",
    "\n",
    "```python\n",
    "# Good: Descriptive metadata\n",
    "config = RunnableConfig(\n",
    "    metadata={\n",
    "        \"use_case\": \"customer_analytics\",\n",
    "        \"user_segment\": \"enterprise\",\n",
    "        \"session_id\": \"session_123\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Bad: Unclear metadata\n",
    "config = RunnableConfig(\n",
    "    metadata={\"type\": \"a\", \"id\": \"123\"}\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Keep Configurations Organized\n",
    "\n",
    "```python\n",
    "# Create a configuration registry\n",
    "AGENT_CONFIGS = {\n",
    "    \"analytics\": analytics_config,\n",
    "    \"storytelling\": storytelling_config,\n",
    "    \"support\": support_config\n",
    "}\n",
    "\n",
    "def get_agent_for_use_case(use_case: str, tools: list):\n",
    "    config = AGENT_CONFIGS.get(use_case)\n",
    "    if not config:\n",
    "        raise ValueError(f\"Unknown use case: {use_case}\")\n",
    "    return DynamicDocsAgent(config, tools)\n",
    "```\n",
    "\n",
    "### 4. Choose the Right Update Method\n",
    "\n",
    "```python\n",
    "# Use update_config() for complete use case switches\n",
    "agent.update_config(storytelling_config)  # Completely different behavior\n",
    "\n",
    "# Use partial_update_config() for parameter tweaking\n",
    "agent.partial_update_config({\"temperature\": 0.3})  # Fine-tuning\n",
    "```\n",
    "\n",
    "### 5. Document Configuration Changes\n",
    "\n",
    "```python\n",
    "class DocumentedAgent(ConfigurableDocsAgent):\n",
    "    def __init__(self, base_config, tools):\n",
    "        super().__init__(base_config, tools)\n",
    "        self.config_history = [base_config.copy()]\n",
    "    \n",
    "    def partial_update_config(self, config_updates):\n",
    "        \"\"\"Update config with history tracking\"\"\"\n",
    "        super().partial_update_config(config_updates)\n",
    "        self.config_history.append(self.base_config.copy())\n",
    "        \n",
    "    def get_config_history(self):\n",
    "        \"\"\"View all configuration changes\"\"\"\n",
    "        for i, config in enumerate(self.config_history):\n",
    "            print(f\"Config {i}: temp={config.get('temperature')}, \"\n",
    "                  f\"max_tokens={config.get('max_tokens')}\")\n",
    "```\n",
    "\n",
    "## Common Pitfalls to Avoid\n",
    "\n",
    "### Pitfall 1: Forgetting to Pass Config\n",
    "\n",
    "```python\n",
    "# Wrong: Config not passed through\n",
    "def tool_calling_llm(state: ChatAgentState, config: RunnableConfig):\n",
    "    # Config is received but not used!\n",
    "    response = model_runnable.invoke(state)  # Missing config!\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Correct: Config passed through\n",
    "def tool_calling_llm(state: ChatAgentState, config: RunnableConfig):\n",
    "    response = model_runnable.invoke(state, config)  # Config passed!\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "### Pitfall 2: Mixing State and Config\n",
    "\n",
    "```python\n",
    "# Wrong: Putting configuration in state\n",
    "state = {\n",
    "    \"messages\": [...],\n",
    "    \"temperature\": 0.1  # Don't do this!\n",
    "}\n",
    "\n",
    "# Correct: Configuration in RunnableConfig\n",
    "config = RunnableConfig(configurable={\"temperature\": 0.1})\n",
    "state = {\"messages\": [...]}\n",
    "```\n",
    "\n",
    "### Pitfall 3: Ignoring Config in Manual Tracing\n",
    "\n",
    "```python\n",
    "# Wrong: Manual span without config context\n",
    "with mlflow.start_span(name=\"llm_call\") as span:\n",
    "    response = model.invoke(messages)  # No config!\n",
    "\n",
    "# Correct: Include config information\n",
    "with mlflow.start_span(name=\"llm_call\") as span:\n",
    "    span.set_attributes({\n",
    "        \"temperature\": config.configurable.get(\"temperature\"),\n",
    "        \"max_tokens\": config.configurable.get(\"max_tokens\")\n",
    "    })\n",
    "    response = model.invoke(messages, config=config)\n",
    "```\n",
    "\n",
    "### Pitfall 4: Updating Config Too Frequently\n",
    "\n",
    "```python\n",
    "# Wrong: Unnecessary rebuilds\n",
    "for message in messages:\n",
    "    agent.partial_update_config({\"temperature\": 0.1})  # Rebuilds agent every time!\n",
    "    agent.predict(message)\n",
    "\n",
    "# Correct: Update once, use multiple times\n",
    "agent.partial_update_config({\"temperature\": 0.1})\n",
    "for message in messages:\n",
    "    agent.predict(message)\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "`RunnableConfig` is the bridge between flexible configuration and production-ready AI agents. By understanding how it flows through your agent's execution chain and how to update it dynamically, you can:\n",
    "\n",
    "1. **Build once, configure many times**: Create a single agent that serves multiple use cases\n",
    "2. **Enable dynamic behavior**: Adjust agent behavior at runtime without code changes\n",
    "3. **Support MLflow observability**: Automatically track how configuration affects performance\n",
    "4. **Simplify deployment**: Deploy one agent with different configurations per environment\n",
    "5. **Optimize on the fly**: Use configuration updates for A/B testing and real-time optimization\n",
    "6. **Efficient resource usage**: Switch between use cases without creating multiple agent instances\n",
    "\n",
    "The key takeaway is that `RunnableConfig` separates *what* your agent does (the logic) from *how* it does it (the configuration), making your agents more maintainable, flexible, and observable. Combined with dynamic configuration updates, you have a powerful system that can adapt to changing requirements without code modifications.\n",
    "\n",
    "As you build production AI agents with LangChain and MLflow, embrace `RunnableConfig` and its dynamic update capabilities as your tools for creating adaptable, well-instrumented systems that can evolve with your needs.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "# Basic RunnableConfig structure\n",
    "config = RunnableConfig(\n",
    "    configurable={\n",
    "        # Model parameters\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 1000,\n",
    "    },\n",
    "    metadata={\n",
    "        # Business context\n",
    "        \"use_case\": \"analytics\",\n",
    "        \"user_id\": \"user_123\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Using config with your agent\n",
    "output = agent.invoke(request, config=config)\n",
    "\n",
    "# Config flows automatically through chains\n",
    "model_runnable = preprocessor | model\n",
    "response = model_runnable.invoke(state, config)  # Config propagates!\n",
    "\n",
    "# Complete configuration update\n",
    "agent.update_config(new_config)  # Replace entire config\n",
    "\n",
    "# Partial configuration update\n",
    "agent.partial_update_config({\n",
    "    \"temperature\": 0.3,\n",
    "    \"max_tokens\": 1200\n",
    "})  # Update specific parameters\n",
    "```\n",
    "\n",
    "Happy building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Explain RunnableConfig from Capstone Project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
