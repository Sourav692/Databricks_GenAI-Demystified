{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe2f96d1-dfe4-471d-b098-be3dc7c7e62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deep Dive: The `config` Parameter in `call_model`\n",
    "\n",
    "Let me explain the `config: RunnableConfig` parameter in detail, as it's a crucial but often overlooked component that controls how the agent executes.\n",
    "\n",
    "## What is RunnableConfig?\n",
    "\n",
    "```python\n",
    "def call_model(\n",
    "    state: ChatAgentState,\n",
    "    config: RunnableConfig,  # ‚Üê This parameter\n",
    "):\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "`RunnableConfig` is LangChain's way of passing **execution-time configuration** and **runtime context** to any runnable component. Think of it as a \"control panel\" for how your agent should behave during execution.\n",
    "\n",
    "---\n",
    "\n",
    "## RunnableConfig Structure\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    # Core configuration\n",
    "    configurable: Dict[str, Any] = {},\n",
    "    \n",
    "    # Observability and monitoring\n",
    "    callbacks: List[BaseCallbackHandler] = [],\n",
    "    metadata: Dict[str, Any] = {},\n",
    "    tags: List[str] = [],\n",
    "    \n",
    "    # Execution control\n",
    "    recursion_limit: int = 25,\n",
    "    max_concurrency: Optional[int] = None,\n",
    "    \n",
    "    # Threading and async\n",
    "    thread_id: Optional[str] = None,\n",
    "    checkpoint_id: Optional[str] = None,\n",
    ")\n",
    "```\n",
    "\n",
    "Let me break down each component:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Configurable Parameters\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.9,\n",
    "        \"custom_param\": \"value\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### What it does:\n",
    "- **Runtime overrides**: Change model behavior without modifying the base configuration\n",
    "- **Per-request customization**: Different users can have different settings\n",
    "- **A/B testing**: Test different configurations dynamically\n",
    "\n",
    "### Example usage in our Databricks agent:\n",
    "\n",
    "```python\n",
    "# Base agent configuration\n",
    "baseline_config = {\n",
    "    \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "\n",
    "# Runtime override for creative tasks\n",
    "creative_config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"temperature\": 0.8,  # More creative responses\n",
    "        \"max_tokens\": 2000   # Longer responses\n",
    "    }\n",
    ")\n",
    "\n",
    "# Runtime override for analytical tasks  \n",
    "analytical_config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"temperature\": 0.0,  # Deterministic responses\n",
    "        \"max_tokens\": 500    # Concise responses\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use different configs for different scenarios\n",
    "response = agent.invoke(messages, config=creative_config)\n",
    "```\n",
    "\n",
    "### How it works in practice:\n",
    "\n",
    "```python\n",
    "def _build_agent_from_config(self):\n",
    "    llm = ChatDatabricks(\n",
    "        endpoint=self.config.get(\"endpoint_name\"),\n",
    "        temperature=self.config.get(\"temperature\"),  # Can be overridden\n",
    "        max_tokens=self.config.get(\"max_tokens\"),    # Can be overridden\n",
    "    )\n",
    "    # ...\n",
    "```\n",
    "\n",
    "When `call_model` executes:\n",
    "```python\n",
    "# The configurable parameters merge with base config\n",
    "effective_temperature = config.configurable.get(\"temperature\") or base_config[\"temperature\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Callbacks for Observability\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    callbacks=[\n",
    "        StdOutCallbackHandler(),        # Print to console\n",
    "        MLflowCallbackHandler(),        # Log to MLflow\n",
    "        CustomMetricsHandler(),         # Custom monitoring\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### What callbacks enable:\n",
    "\n",
    "**Real-time monitoring:**\n",
    "```python\n",
    "class ConversationMonitor(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(f\"ü§ñ LLM called with {len(prompts)} prompts\")\n",
    "        \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(f\"‚úÖ LLM responded with {len(response.generations)} outputs\")\n",
    "        \n",
    "    def on_tool_start(self, serialized, input_str, **kwargs):\n",
    "        print(f\"üîß Tool {serialized['name']} called\")\n",
    "        \n",
    "    def on_tool_end(self, output, **kwargs):\n",
    "        print(f\"‚úÖ Tool completed: {output[:100]}...\")\n",
    "```\n",
    "\n",
    "**Performance tracking:**\n",
    "```python\n",
    "class PerformanceTracker(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.token_count = 0\n",
    "        \n",
    "    def on_llm_start(self, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        duration = time.time() - self.start_time\n",
    "        # Log metrics to your monitoring system\n",
    "        log_metric(\"llm_duration\", duration)\n",
    "        log_metric(\"token_count\", self.token_count)\n",
    "```\n",
    "\n",
    "**Usage in our agent:**\n",
    "```python\n",
    "# Monitor a specific conversation\n",
    "monitoring_config = RunnableConfig(\n",
    "    callbacks=[ConversationMonitor(), PerformanceTracker()]\n",
    ")\n",
    "\n",
    "# Every call_model execution will trigger these callbacks\n",
    "response = call_model(state, monitoring_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Metadata for Context\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    metadata={\n",
    "        \"user_id\": \"sourav_banerjee\",\n",
    "        \"session_id\": \"session_123\",\n",
    "        \"request_type\": \"documentation_query\",\n",
    "        \"priority\": \"high\",\n",
    "        \"source\": \"web_interface\",\n",
    "        \"experiment_group\": \"feature_test_a\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### How metadata is used:\n",
    "\n",
    "**Request tracking:**\n",
    "```python\n",
    "def call_model(state: ChatAgentState, config: RunnableConfig):\n",
    "    user_id = config.metadata.get(\"user_id\", \"anonymous\")\n",
    "    session_id = config.metadata.get(\"session_id\", \"unknown\")\n",
    "    \n",
    "    # Log request with context\n",
    "    logger.info(f\"Processing request for user {user_id} in session {session_id}\")\n",
    "    \n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "**Personalization:**\n",
    "```python\n",
    "def personalized_preprocessing(state, config):\n",
    "    user_id = config.metadata.get(\"user_id\")\n",
    "    user_preferences = get_user_preferences(user_id)\n",
    "    \n",
    "    # Modify system prompt based on user preferences\n",
    "    if user_preferences.get(\"technical_level\") == \"expert\":\n",
    "        system_prompt = \"Provide detailed technical responses...\"\n",
    "    else:\n",
    "        system_prompt = \"Provide beginner-friendly responses...\"\n",
    "    \n",
    "    return enhanced_state\n",
    "```\n",
    "\n",
    "**Analytics and A/B testing:**\n",
    "```python\n",
    "def analytics_callback(config):\n",
    "    experiment_group = config.metadata.get(\"experiment_group\")\n",
    "    \n",
    "    # Track metrics by experiment group\n",
    "    track_metric(f\"response_time_{experiment_group}\", duration)\n",
    "    track_metric(f\"tool_usage_{experiment_group}\", tool_count)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tags for Categorization\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    tags=[\n",
    "        \"production\",\n",
    "        \"databricks_docs\",\n",
    "        \"technical_support\",\n",
    "        \"priority_user\"\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Tag usage examples:\n",
    "\n",
    "**Filtering and monitoring:**\n",
    "```python\n",
    "class TaggedMetricsHandler(BaseCallbackHandler):\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        tags = kwargs.get(\"tags\", [])\n",
    "        \n",
    "        if \"priority_user\" in tags:\n",
    "            # Send to priority monitoring dashboard\n",
    "            send_to_priority_dashboard(response)\n",
    "            \n",
    "        if \"production\" in tags:\n",
    "            # Different SLA requirements\n",
    "            check_response_quality(response)\n",
    "```\n",
    "\n",
    "**Configuration routing:**\n",
    "```python\n",
    "def get_model_config(tags):\n",
    "    if \"high_accuracy\" in tags:\n",
    "        return {\"temperature\": 0.0}\n",
    "    elif \"creative\" in tags:\n",
    "        return {\"temperature\": 0.8}\n",
    "    else:\n",
    "        return {\"temperature\": 0.1}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Execution Control Parameters\n",
    "\n",
    "### Recursion Limit\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10  # Prevent infinite loops\n",
    ")\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "```python\n",
    "# Without recursion limit, this could loop forever:\n",
    "# Agent calls tool ‚Üí Tool calls another tool ‚Üí Tool calls agent ‚Üí Agent calls tool...\n",
    "\n",
    "# With recursion limit:\n",
    "# After 10 levels deep, execution stops with an error\n",
    "```\n",
    "\n",
    "**Custom recursion handling:**\n",
    "```python\n",
    "def safe_call_model(state, config):\n",
    "    try:\n",
    "        return model_runnable.invoke(state, config)\n",
    "    except RecursionError:\n",
    "        return {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": \"I apologize, this query is too complex. Could you break it down into smaller parts?\"\n",
    "        }\n",
    "```\n",
    "\n",
    "### Max Concurrency\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    max_concurrency=5  # Limit parallel operations\n",
    ")\n",
    "```\n",
    "\n",
    "**Use case:**\n",
    "```python\n",
    "# If agent calls multiple tools simultaneously:\n",
    "tools = [\"search_docs\", \"query_database\", \"call_api\", \"analyze_data\"]\n",
    "\n",
    "# Without limit: All 4 tools run in parallel (might overwhelm system)\n",
    "# With limit: Only 5 concurrent operations allowed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Threading and Checkpointing\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    thread_id=\"conversation_abc123\",\n",
    "    checkpoint_id=\"checkpoint_001\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Thread ID usage:\n",
    "```python\n",
    "# Maintains conversation context across multiple calls\n",
    "thread_id = f\"user_{user_id}_session_{session_id}\"\n",
    "\n",
    "config = RunnableConfig(\n",
    "    thread_id=thread_id,\n",
    "    metadata={\"persistent_context\": True}\n",
    ")\n",
    "\n",
    "# All calls with same thread_id share context\n",
    "```\n",
    "\n",
    "### Checkpointing:\n",
    "```python\n",
    "# Save conversation state at specific points\n",
    "checkpoint_config = RunnableConfig(\n",
    "    checkpoint_id=f\"before_complex_operation_{timestamp}\"\n",
    ")\n",
    "\n",
    "# If something goes wrong, can restore from checkpoint\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Config Examples\n",
    "\n",
    "### 1. Development vs Production\n",
    "\n",
    "```python\n",
    "# Development configuration\n",
    "dev_config = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.2},\n",
    "    callbacks=[StdOutCallbackHandler()],  # Verbose logging\n",
    "    metadata={\"environment\": \"development\"},\n",
    "    tags=[\"debug\", \"testing\"],\n",
    "    recursion_limit=50  # More lenient for debugging\n",
    ")\n",
    "\n",
    "# Production configuration  \n",
    "prod_config = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.01},\n",
    "    callbacks=[MLflowCallbackHandler(), AlertingHandler()],\n",
    "    metadata={\"environment\": \"production\"},\n",
    "    tags=[\"production\", \"monitored\"],\n",
    "    recursion_limit=10  # Strict limits\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. User-Specific Configuration\n",
    "\n",
    "```python\n",
    "def get_user_config(user_id, preferences):\n",
    "    return RunnableConfig(\n",
    "        configurable={\n",
    "            \"temperature\": preferences.get(\"creativity_level\", 0.1),\n",
    "            \"max_tokens\": preferences.get(\"response_length\", 1000)\n",
    "        },\n",
    "        metadata={\n",
    "            \"user_id\": user_id,\n",
    "            \"preferences\": preferences,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        tags=[\n",
    "            f\"user_{user_id}\",\n",
    "            preferences.get(\"user_tier\", \"standard\"),\n",
    "            preferences.get(\"language\", \"english\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Usage\n",
    "user_config = get_user_config(\"sourav_banerjee\", {\n",
    "    \"creativity_level\": 0.3,\n",
    "    \"response_length\": 1500,\n",
    "    \"user_tier\": \"premium\",\n",
    "    \"language\": \"english\"\n",
    "})\n",
    "\n",
    "response = agent.invoke(messages, config=user_config)\n",
    "```\n",
    "\n",
    "### 3. Experiment Configuration\n",
    "\n",
    "```python\n",
    "# A/B test different configurations\n",
    "def get_experiment_config(experiment_group):\n",
    "    base_config = {\n",
    "        \"callbacks\": [ExperimentTracker(experiment_group)],\n",
    "        \"metadata\": {\"experiment\": experiment_group},\n",
    "        \"tags\": [\"experiment\", experiment_group]\n",
    "    }\n",
    "    \n",
    "    if experiment_group == \"high_creativity\":\n",
    "        base_config[\"configurable\"] = {\"temperature\": 0.8}\n",
    "    elif experiment_group == \"high_precision\":\n",
    "        base_config[\"configurable\"] = {\"temperature\": 0.0}\n",
    "    else:  # control group\n",
    "        base_config[\"configurable\"] = {\"temperature\": 0.1}\n",
    "        \n",
    "    return RunnableConfig(**base_config)\n",
    "\n",
    "# Random assignment for A/B testing\n",
    "experiment_group = random.choice([\"high_creativity\", \"high_precision\", \"control\"])\n",
    "config = get_experiment_config(experiment_group)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How Config Flows Through the Agent\n",
    "\n",
    "Let's trace how config flows through our entire agent:\n",
    "\n",
    "### 1. Initial Call\n",
    "```python\n",
    "# User makes a request\n",
    "user_config = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.5},\n",
    "    metadata={\"user_id\": \"sourav\"},\n",
    "    callbacks=[MyCustomTracker()]\n",
    ")\n",
    "\n",
    "response = agent.invoke(messages, config=user_config)\n",
    "```\n",
    "\n",
    "### 2. Agent Invocation\n",
    "```python\n",
    "# Inside the compiled graph\n",
    "def agent_node(state, config):  # Config passed through\n",
    "    return call_model(state, config)\n",
    "```\n",
    "\n",
    "### 3. Call Model Execution  \n",
    "```python\n",
    "def call_model(state: ChatAgentState, config: RunnableConfig):\n",
    "    # Config is passed to the model pipeline\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "### 4. Model Pipeline\n",
    "```python\n",
    "# Preprocessor gets config\n",
    "processed_messages = preprocessor.invoke(state, config)\n",
    "\n",
    "# Model gets config (with all overrides)\n",
    "response = model.invoke(processed_messages, config)\n",
    "```\n",
    "\n",
    "### 5. Tool Execution\n",
    "```python\n",
    "# If tools are called, they also get the config\n",
    "def tools_node(state, config):  # Same config propagated\n",
    "    return tool_executor.invoke(state, config)\n",
    "```\n",
    "\n",
    "**Key insight**: The same `config` object flows through every component, ensuring consistent behavior and monitoring across the entire execution.\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices for Config Usage\n",
    "\n",
    "### 1. Environment-Specific Configs\n",
    "```python\n",
    "class ConfigManager:\n",
    "    @staticmethod\n",
    "    def get_config(environment=\"production\"):\n",
    "        if environment == \"production\":\n",
    "            return RunnableConfig(\n",
    "                configurable={\"temperature\": 0.01},\n",
    "                callbacks=[ProductionMonitor()],\n",
    "                recursion_limit=5\n",
    "            )\n",
    "        elif environment == \"staging\":\n",
    "            return RunnableConfig(\n",
    "                configurable={\"temperature\": 0.1},\n",
    "                callbacks=[StagingMonitor()],\n",
    "                recursion_limit=10\n",
    "            )\n",
    "        else:  # development\n",
    "            return RunnableConfig(\n",
    "                configurable={\"temperature\": 0.2},\n",
    "                callbacks=[DebugMonitor()],\n",
    "                recursion_limit=25\n",
    "            )\n",
    "```\n",
    "\n",
    "### 2. Request-Level Customization\n",
    "```python\n",
    "def create_request_config(user_request):\n",
    "    base_config = get_default_config()\n",
    "    \n",
    "    # Adjust based on request type\n",
    "    if \"creative\" in user_request.lower():\n",
    "        base_config.configurable[\"temperature\"] = 0.7\n",
    "    elif \"precise\" in user_request.lower():\n",
    "        base_config.configurable[\"temperature\"] = 0.0\n",
    "        \n",
    "    # Add request-specific metadata\n",
    "    base_config.metadata.update({\n",
    "        \"request_hash\": hash(user_request),\n",
    "        \"request_length\": len(user_request),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    return base_config\n",
    "```\n",
    "\n",
    "### 3. Error Handling with Config\n",
    "```python\n",
    "def robust_agent_call(messages, config=None):\n",
    "    if config is None:\n",
    "        config = get_default_config()\n",
    "        \n",
    "    try:\n",
    "        return agent.invoke(messages, config=config)\n",
    "    except RecursionError:\n",
    "        # Retry with lower recursion limit\n",
    "        safe_config = RunnableConfig(\n",
    "            **config.dict(),\n",
    "            recursion_limit=5\n",
    "        )\n",
    "        return agent.invoke(messages, config=safe_config)\n",
    "    except Exception as e:\n",
    "        # Log error with config context\n",
    "        logger.error(f\"Agent failed with config {config.dict()}: {e}\")\n",
    "        raise\n",
    "```\n",
    "\n",
    "The `config` parameter is what makes LangChain agents truly production-ready, providing the control, observability, and flexibility needed for real-world deployments. It's the bridge between your agent's core logic and the operational requirements of running AI systems at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b03be431-027a-42f5-86d9-0ac58880724e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deep Dive: The `config` Parameter in `call_model`\n",
    "\n",
    "Let me explain the `config: RunnableConfig` parameter in detail, as it's a crucial but often overlooked component that controls how the agent executes.\n",
    "\n",
    "## What is RunnableConfig?\n",
    "\n",
    "```python\n",
    "def call_model(\n",
    "    state: ChatAgentState,\n",
    "    config: RunnableConfig,  # ‚Üê This parameter\n",
    "):\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "`RunnableConfig` is LangChain's way of passing **execution-time configuration** and **runtime context** to any runnable component. Think of it as a \"control panel\" for how your agent should behave during execution.\n",
    "\n",
    "---\n",
    "\n",
    "## RunnableConfig Structure\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    # Core configuration\n",
    "    configurable: Dict[str, Any] = {},\n",
    "    \n",
    "    # Observability and monitoring\n",
    "    callbacks: List[BaseCallbackHandler] = [],\n",
    "    metadata: Dict[str, Any] = {},\n",
    "    tags: List[str] = [],\n",
    "    \n",
    "    # Execution control\n",
    "    recursion_limit: int = 25,\n",
    "    max_concurrency: Optional[int] = None,\n",
    "    \n",
    "    # Threading and async\n",
    "    thread_id: Optional[str] = None,\n",
    "    checkpoint_id: Optional[str] = None,\n",
    ")\n",
    "```\n",
    "\n",
    "Let me break down each component:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Configurable Parameters\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.9,\n",
    "        \"custom_param\": \"value\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### What it does:\n",
    "- **Runtime overrides**: Change model behavior without modifying the base configuration\n",
    "- **Per-request customization**: Different users can have different settings\n",
    "- **A/B testing**: Test different configurations dynamically\n",
    "\n",
    "### Example usage in our Databricks agent:\n",
    "\n",
    "```python\n",
    "# Base agent configuration\n",
    "baseline_config = {\n",
    "    \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "\n",
    "# Runtime override for creative tasks\n",
    "creative_config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"temperature\": 0.8,  # More creative responses\n",
    "        \"max_tokens\": 2000   # Longer responses\n",
    "    }\n",
    ")\n",
    "\n",
    "# Runtime override for analytical tasks  \n",
    "analytical_config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"temperature\": 0.0,  # Deterministic responses\n",
    "        \"max_tokens\": 500    # Concise responses\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use different configs for different scenarios\n",
    "response = agent.invoke(messages, config=creative_config)\n",
    "```\n",
    "\n",
    "### How it works in practice:\n",
    "\n",
    "```python\n",
    "def _build_agent_from_config(self):\n",
    "    llm = ChatDatabricks(\n",
    "        endpoint=self.config.get(\"endpoint_name\"),\n",
    "        temperature=self.config.get(\"temperature\"),  # Can be overridden\n",
    "        max_tokens=self.config.get(\"max_tokens\"),    # Can be overridden\n",
    "    )\n",
    "    # ...\n",
    "```\n",
    "\n",
    "When `call_model` executes:\n",
    "```python\n",
    "# The configurable parameters merge with base config\n",
    "effective_temperature = config.configurable.get(\"temperature\") or base_config[\"temperature\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Callbacks for Observability\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    callbacks=[\n",
    "        StdOutCallbackHandler(),        # Print to console\n",
    "        MLflowCallbackHandler(),        # Log to MLflow\n",
    "        CustomMetricsHandler(),         # Custom monitoring\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### What callbacks enable:\n",
    "\n",
    "**Real-time monitoring:**\n",
    "```python\n",
    "class ConversationMonitor(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(f\"ü§ñ LLM called with {len(prompts)} prompts\")\n",
    "        \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(f\"‚úÖ LLM responded with {len(response.generations)} outputs\")\n",
    "        \n",
    "    def on_tool_start(self, serialized, input_str, **kwargs):\n",
    "        print(f\"üîß Tool {serialized['name']} called\")\n",
    "        \n",
    "    def on_tool_end(self, output, **kwargs):\n",
    "        print(f\"‚úÖ Tool completed: {output[:100]}...\")\n",
    "```\n",
    "\n",
    "**Performance tracking:**\n",
    "```python\n",
    "class PerformanceTracker(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.token_count = 0\n",
    "        \n",
    "    def on_llm_start(self, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        duration = time.time() - self.start_time\n",
    "        # Log metrics to your monitoring system\n",
    "        log_metric(\"llm_duration\", duration)\n",
    "        log_metric(\"token_count\", self.token_count)\n",
    "```\n",
    "\n",
    "**Usage in our agent:**\n",
    "```python\n",
    "# Monitor a specific conversation\n",
    "monitoring_config = RunnableConfig(\n",
    "    callbacks=[ConversationMonitor(), PerformanceTracker()]\n",
    ")\n",
    "\n",
    "# Every call_model execution will trigger these callbacks\n",
    "response = call_model(state, monitoring_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Metadata for Context\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    metadata={\n",
    "        \"user_id\": \"sourav_banerjee\",\n",
    "        \"session_id\": \"session_123\",\n",
    "        \"request_type\": \"documentation_query\",\n",
    "        \"priority\": \"high\",\n",
    "        \"source\": \"web_interface\",\n",
    "        \"experiment_group\": \"feature_test_a\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### How metadata is used:\n",
    "\n",
    "**Request tracking:**\n",
    "```python\n",
    "def call_model(state: ChatAgentState, config: RunnableConfig):\n",
    "    user_id = config.metadata.get(\"user_id\", \"anonymous\")\n",
    "    session_id = config.metadata.get(\"session_id\", \"unknown\")\n",
    "    \n",
    "    # Log request with context\n",
    "    logger.info(f\"Processing request for user {user_id} in session {session_id}\")\n",
    "    \n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "**Personalization:**\n",
    "```python\n",
    "def personalized_preprocessing(state, config):\n",
    "    user_id = config.metadata.get(\"user_id\")\n",
    "    user_preferences = get_user_preferences(user_id)\n",
    "    \n",
    "    # Modify system prompt based on user preferences\n",
    "    if user_preferences.get(\"technical_level\") == \"expert\":\n",
    "        system_prompt = \"Provide detailed technical responses...\"\n",
    "    else:\n",
    "        system_prompt = \"Provide beginner-friendly responses...\"\n",
    "    \n",
    "    return enhanced_state\n",
    "```\n",
    "\n",
    "**Analytics and A/B testing:**\n",
    "```python\n",
    "def analytics_callback(config):\n",
    "    experiment_group = config.metadata.get(\"experiment_group\")\n",
    "    \n",
    "    # Track metrics by experiment group\n",
    "    track_metric(f\"response_time_{experiment_group}\", duration)\n",
    "    track_metric(f\"tool_usage_{experiment_group}\", tool_count)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tags for Categorization\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    tags=[\n",
    "        \"production\",\n",
    "        \"databricks_docs\",\n",
    "        \"technical_support\",\n",
    "        \"priority_user\"\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Tag usage examples:\n",
    "\n",
    "**Filtering and monitoring:**\n",
    "```python\n",
    "class TaggedMetricsHandler(BaseCallbackHandler):\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        tags = kwargs.get(\"tags\", [])\n",
    "        \n",
    "        if \"priority_user\" in tags:\n",
    "            # Send to priority monitoring dashboard\n",
    "            send_to_priority_dashboard(response)\n",
    "            \n",
    "        if \"production\" in tags:\n",
    "            # Different SLA requirements\n",
    "            check_response_quality(response)\n",
    "```\n",
    "\n",
    "**Configuration routing:**\n",
    "```python\n",
    "def get_model_config(tags):\n",
    "    if \"high_accuracy\" in tags:\n",
    "        return {\"temperature\": 0.0}\n",
    "    elif \"creative\" in tags:\n",
    "        return {\"temperature\": 0.8}\n",
    "    else:\n",
    "        return {\"temperature\": 0.1}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Execution Control Parameters\n",
    "\n",
    "### Recursion Limit\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10  # Prevent infinite loops\n",
    ")\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "```python\n",
    "# Without recursion limit, this could loop forever:\n",
    "# Agent calls tool ‚Üí Tool calls another tool ‚Üí Tool calls agent ‚Üí Agent calls tool...\n",
    "\n",
    "# With recursion limit:\n",
    "# After 10 levels deep, execution stops with an error\n",
    "```\n",
    "\n",
    "**Custom recursion handling:**\n",
    "```python\n",
    "def safe_call_model(state, config):\n",
    "    try:\n",
    "        return model_runnable.invoke(state, config)\n",
    "    except RecursionError:\n",
    "        return {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": \"I apologize, this query is too complex. Could you break it down into smaller parts?\"\n",
    "        }\n",
    "```\n",
    "\n",
    "### Max Concurrency\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    max_concurrency=5  # Limit parallel operations\n",
    ")\n",
    "```\n",
    "\n",
    "**Use case:**\n",
    "```python\n",
    "# If agent calls multiple tools simultaneously:\n",
    "tools = [\"search_docs\", \"query_database\", \"call_api\", \"analyze_data\"]\n",
    "\n",
    "# Without limit: All 4 tools run in parallel (might overwhelm system)\n",
    "# With limit: Only 5 concurrent operations allowed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Threading and Checkpointing\n",
    "\n",
    "```python\n",
    "config = RunnableConfig(\n",
    "    thread_id=\"conversation_abc123\",\n",
    "    checkpoint_id=\"checkpoint_001\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Thread ID usage:\n",
    "```python\n",
    "# Maintains conversation context across multiple calls\n",
    "thread_id = f\"user_{user_id}_session_{session_id}\"\n",
    "\n",
    "config = RunnableConfig(\n",
    "    thread_id=thread_id,\n",
    "    metadata={\"persistent_context\": True}\n",
    ")\n",
    "\n",
    "# All calls with same thread_id share context\n",
    "```\n",
    "\n",
    "### Checkpointing:\n",
    "```python\n",
    "# Save conversation state at specific points\n",
    "checkpoint_config = RunnableConfig(\n",
    "    checkpoint_id=f\"before_complex_operation_{timestamp}\"\n",
    ")\n",
    "\n",
    "# If something goes wrong, can restore from checkpoint\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Config Examples\n",
    "\n",
    "### 1. Development vs Production\n",
    "\n",
    "```python\n",
    "# Development configuration\n",
    "dev_config = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.2},\n",
    "    callbacks=[StdOutCallbackHandler()],  # Verbose logging\n",
    "    metadata={\"environment\": \"development\"},\n",
    "    tags=[\"debug\", \"testing\"],\n",
    "    recursion_limit=50  # More lenient for debugging\n",
    ")\n",
    "\n",
    "# Production configuration  \n",
    "prod_config = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.01},\n",
    "    callbacks=[MLflowCallbackHandler(), AlertingHandler()],\n",
    "    metadata={\"environment\": \"production\"},\n",
    "    tags=[\"production\", \"monitored\"],\n",
    "    recursion_limit=10  # Strict limits\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. User-Specific Configuration\n",
    "\n",
    "```python\n",
    "def get_user_config(user_id, preferences):\n",
    "    return RunnableConfig(\n",
    "        configurable={\n",
    "            \"temperature\": preferences.get(\"creativity_level\", 0.1),\n",
    "            \"max_tokens\": preferences.get(\"response_length\", 1000)\n",
    "        },\n",
    "        metadata={\n",
    "            \"user_id\": user_id,\n",
    "            \"preferences\": preferences,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        tags=[\n",
    "            f\"user_{user_id}\",\n",
    "            preferences.get(\"user_tier\", \"standard\"),\n",
    "            preferences.get(\"language\", \"english\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Usage\n",
    "user_config = get_user_config(\"sourav_banerjee\", {\n",
    "    \"creativity_level\": 0.3,\n",
    "    \"response_length\": 1500,\n",
    "    \"user_tier\": \"premium\",\n",
    "    \"language\": \"english\"\n",
    "})\n",
    "\n",
    "response = agent.invoke(messages, config=user_config)\n",
    "```\n",
    "\n",
    "### 3. Experiment Configuration\n",
    "\n",
    "```python\n",
    "# A/B test different configurations\n",
    "def get_experiment_config(experiment_group):\n",
    "    base_config = {\n",
    "        \"callbacks\": [ExperimentTracker(experiment_group)],\n",
    "        \"metadata\": {\"experiment\": experiment_group},\n",
    "        \"tags\": [\"experiment\", experiment_group]\n",
    "    }\n",
    "    \n",
    "    if experiment_group == \"high_creativity\":\n",
    "        base_config[\"configurable\"] = {\"temperature\": 0.8}\n",
    "    elif experiment_group == \"high_precision\":\n",
    "        base_config[\"configurable\"] = {\"temperature\": 0.0}\n",
    "    else:  # control group\n",
    "        base_config[\"configurable\"] = {\"temperature\": 0.1}\n",
    "        \n",
    "    return RunnableConfig(**base_config)\n",
    "\n",
    "# Random assignment for A/B testing\n",
    "experiment_group = random.choice([\"high_creativity\", \"high_precision\", \"control\"])\n",
    "config = get_experiment_config(experiment_group)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How Config Flows Through the Agent\n",
    "\n",
    "Let's trace how config flows through our entire agent:\n",
    "\n",
    "### 1. Initial Call\n",
    "```python\n",
    "# User makes a request\n",
    "user_config = RunnableConfig(\n",
    "    configurable={\"temperature\": 0.5},\n",
    "    metadata={\"user_id\": \"sourav\"},\n",
    "    callbacks=[MyCustomTracker()]\n",
    ")\n",
    "\n",
    "response = agent.invoke(messages, config=user_config)\n",
    "```\n",
    "\n",
    "### 2. Agent Invocation\n",
    "```python\n",
    "# Inside the compiled graph\n",
    "def agent_node(state, config):  # Config passed through\n",
    "    return call_model(state, config)\n",
    "```\n",
    "\n",
    "### 3. Call Model Execution  \n",
    "```python\n",
    "def call_model(state: ChatAgentState, config: RunnableConfig):\n",
    "    # Config is passed to the model pipeline\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "### 4. Model Pipeline\n",
    "```python\n",
    "# Preprocessor gets config\n",
    "processed_messages = preprocessor.invoke(state, config)\n",
    "\n",
    "# Model gets config (with all overrides)\n",
    "response = model.invoke(processed_messages, config)\n",
    "```\n",
    "\n",
    "### 5. Tool Execution\n",
    "```python\n",
    "# If tools are called, they also get the config\n",
    "def tools_node(state, config):  # Same config propagated\n",
    "    return tool_executor.invoke(state, config)\n",
    "```\n",
    "\n",
    "**Key insight**: The same `config` object flows through every component, ensuring consistent behavior and monitoring across the entire execution.\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices for Config Usage\n",
    "\n",
    "### 1. Environment-Specific Configs\n",
    "```python\n",
    "class ConfigManager:\n",
    "    @staticmethod\n",
    "    def get_config(environment=\"production\"):\n",
    "        if environment == \"production\":\n",
    "            return RunnableConfig(\n",
    "                configurable={\"temperature\": 0.01},\n",
    "                callbacks=[ProductionMonitor()],\n",
    "                recursion_limit=5\n",
    "            )\n",
    "        elif environment == \"staging\":\n",
    "            return RunnableConfig(\n",
    "                configurable={\"temperature\": 0.1},\n",
    "                callbacks=[StagingMonitor()],\n",
    "                recursion_limit=10\n",
    "            )\n",
    "        else:  # development\n",
    "            return RunnableConfig(\n",
    "                configurable={\"temperature\": 0.2},\n",
    "                callbacks=[DebugMonitor()],\n",
    "                recursion_limit=25\n",
    "            )\n",
    "```\n",
    "\n",
    "### 2. Request-Level Customization\n",
    "```python\n",
    "def create_request_config(user_request):\n",
    "    base_config = get_default_config()\n",
    "    \n",
    "    # Adjust based on request type\n",
    "    if \"creative\" in user_request.lower():\n",
    "        base_config.configurable[\"temperature\"] = 0.7\n",
    "    elif \"precise\" in user_request.lower():\n",
    "        base_config.configurable[\"temperature\"] = 0.0\n",
    "        \n",
    "    # Add request-specific metadata\n",
    "    base_config.metadata.update({\n",
    "        \"request_hash\": hash(user_request),\n",
    "        \"request_length\": len(user_request),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    return base_config\n",
    "```\n",
    "\n",
    "### 3. Error Handling with Config\n",
    "```python\n",
    "def robust_agent_call(messages, config=None):\n",
    "    if config is None:\n",
    "        config = get_default_config()\n",
    "        \n",
    "    try:\n",
    "        return agent.invoke(messages, config=config)\n",
    "    except RecursionError:\n",
    "        # Retry with lower recursion limit\n",
    "        safe_config = RunnableConfig(\n",
    "            **config.dict(),\n",
    "            recursion_limit=5\n",
    "        )\n",
    "        return agent.invoke(messages, config=safe_config)\n",
    "    except Exception as e:\n",
    "        # Log error with config context\n",
    "        logger.error(f\"Agent failed with config {config.dict()}: {e}\")\n",
    "        raise\n",
    "```\n",
    "\n",
    "The `config` parameter is what makes LangChain agents truly production-ready, providing the control, observability, and flexibility needed for real-world deployments. It's the bridge between your agent's core logic and the operational requirements of running AI systems at scale."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RunnableConfig Deep Dive",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
