{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ddb8a43-ea43-44a8-872d-e40f27710a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Supported Foundation Models on Mosaic AI Model Serving | Databricks on AWS\n",
    "\n",
    "Foundation models represent a transformative shift in artificial intelligence, offering large, pre-trained neural networks that can be adapted for various AI applications. Databricks provides comprehensive support for deploying and serving these models through their Mosaic AI Model Serving platform.\n",
    "\n",
    "## What Are Foundation Models?\n",
    "\n",
    "Foundation models are **large, pre-trained neural networks** trained on extensive and diverse datasets. These models learn general patterns in language, images, or other data types and can be fine-tuned for specific tasks with additional training.\n",
    "\n",
    "`Important: Your use of certain foundation models is subject to the model developer's license and acceptable use policy. Customers are responsible for ensuring compliance with applicable model licenses.`\n",
    "\n",
    "## Flexible Hosting Options for Foundation Models\n",
    "\n",
    "Databricks offers multiple ways to access and deploy foundation models based on your specific needs:\n",
    "\n",
    "### 1. **AI Functions Optimized Models**\n",
    "- Subset of Databricks-hosted models optimized for AI Functions\n",
    "- Apply AI to your data at scale\n",
    "- Run batch inference production workloads using supported functions\n",
    "\n",
    "### 2. **Pay-Per-Token**\n",
    "- **Ideal for:** Experimentation and quick exploration\n",
    "- No upfront infrastructure commitments\n",
    "- Query pre-configured endpoints directly in your workspace\n",
    "- Cost-effective for testing and development\n",
    "\n",
    "### 3. **Provisioned Throughput**\n",
    "- **Recommended for:** Production use cases requiring performance guarantees\n",
    "- Deploy fine-tuned foundation models\n",
    "- Optimized serving endpoints with guaranteed capacity\n",
    "- Better cost efficiency for high-volume workloads\n",
    "\n",
    "### 4. **External Models**\n",
    "- Access foundation models hosted outside Databricks\n",
    "- Supported providers: OpenAI, Anthropic, Cohere, and more\n",
    "- Centrally managed within Databricks for streamlined governance\n",
    "- Unified interface for multiple LLM providers\n",
    "\n",
    "## Databricks-Hosted Foundation Models\n",
    "\n",
    "### Available Model Families\n",
    "\n",
    "Databricks hosts state-of-the-art open foundation models through **Foundation Model APIs**. Key model families include:\n",
    "\n",
    "#### **Current Supported Models:**\n",
    "- **OpenAI GPT Series:**\n",
    "  - databricks-gpt-5-1\n",
    "  - databricks-gpt-5\n",
    "  - databricks-gpt-5-mini\n",
    "  - databricks-gpt-5-nano\n",
    "\n",
    "- **Google Gemini Series:**\n",
    "  - databricks-gemini-3-pro\n",
    "  - databricks-gemini-2-5-pro\n",
    "  - databricks-gemini-2-5-flash\n",
    "\n",
    "- **Anthropic Claude Series:**\n",
    "  - databricks-claude-sonnet-4-5\n",
    "  - databricks-claude-opus-4-1\n",
    "  - databricks-claude-sonnet-4\n",
    "  - databricks-claude-3.7-sonnet\n",
    "\n",
    "- **Meta Llama Series:**\n",
    "  - databricks-llama-4-maverick (Public Preview)\n",
    "  - databricks-meta-llama-3-3-70b-instruct\n",
    "  - databricks-meta-llama-3-1-405b-instruct\n",
    "  - databricks-meta-llama-3-1-8b-instruct\n",
    "\n",
    "- **Open Source Models:**\n",
    "  - databricks-gpt-oss-20b\n",
    "  - databricks-gpt-oss-120b\n",
    "  - databricks-gemma-3-12b\n",
    "  - databricks-qwen3-next-80b-a3b-instruct (Beta)\n",
    "\n",
    "- **Embedding Models:**\n",
    "  - databricks-gte-large-en (GTE v1.5 English)\n",
    "  - BGE v1.5 (English)\n",
    "\n",
    "#### **Important Model Updates:**\n",
    "\n",
    "**Meta Llama 4 Maverick:**\n",
    "- Available in Public Preview for provisioned throughput workloads\n",
    "- Represents the latest advancement in the Llama model family\n",
    "\n",
    "**Retirement Notices:**\n",
    "- **Meta-Llama-3.1-405B-Instruct:** \n",
    "  - Pay-per-token unavailable after February 15, 2026\n",
    "  - Provisioned throughput unavailable after May 15, 2026\n",
    "\n",
    "- **Models Retiring February 15, 2026:**\n",
    "  - DBRX family\n",
    "  - Llama 3 70B and 8B\n",
    "  - Llama 2 70B and 13B\n",
    "  - Mistral 8x7B / Mixtral 8x7B\n",
    "  - MPT 30B and 7B\n",
    "\n",
    "**Migration Guidance:** See the Retired Models documentation for recommended replacement models and migration strategies.\n",
    "\n",
    "### Regional Availability\n",
    "\n",
    "Foundation model support varies by AWS region. Here's a summary of key regions:\n",
    "\n",
    "#### **Full Support Regions:**\n",
    "- **us-east-1, us-east-2, us-west-2:** Complete model catalog with all features\n",
    "- **eu-central-1, eu-west-1, eu-west-2:** Full European support\n",
    "- **ap-northeast-1, ap-northeast-2, ap-southeast-1, ap-southeast-2:** Comprehensive Asia-Pacific coverage\n",
    "- **ca-central-1:** Complete Canadian region support\n",
    "\n",
    "#### **Limited/No Support:**\n",
    "- **us-west-1, us-gov-west-1, eu-west-3:** Not supported\n",
    "- **ap-south-1, sa-east-1:** Limited support with specific model restrictions\n",
    "\n",
    "**Note:** Some models require cross-geography routing to be enabled based on GPU availability.\n",
    "\n",
    "## External Model Providers\n",
    "\n",
    "Databricks supports integration with leading LLM providers through External Models:\n",
    "\n",
    "### **Supported Providers and Model Types:**\n",
    "\n",
    "#### **OpenAI**\n",
    "- **Completions:** gpt-3.5-turbo-instruct, babbage-002, davinci-002\n",
    "- **Chat:** o1, o1-mini, gpt-3.5-turbo, gpt-4, gpt-4-turbo, gpt-4o, gpt-4o-mini\n",
    "- **Embeddings:** text-embedding-ada-002, text-embedding-3-large, text-embedding-3-small\n",
    "- **Note:** Supports fine-tuned completion and chat models\n",
    "\n",
    "#### **Azure OpenAI**\n",
    "- **Completions:** text-davinci-003, gpt-35-turbo-instruct\n",
    "- **Chat:** o1, o1-mini, gpt-35-turbo variants, gpt-4 variants, gpt-4o variants\n",
    "- **Embeddings:** text-embedding series\n",
    "- **Note:** Supports fine-tuned models\n",
    "\n",
    "#### **Anthropic**\n",
    "- **Chat Models:** \n",
    "  - Latest: claude-3-5-sonnet-latest, claude-3-5-haiku-latest, claude-3-5-opus-latest\n",
    "  - Versioned: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022\n",
    "  - Legacy: claude-1, claude-2 series, claude-instant-1.2\n",
    "\n",
    "#### **Cohere**\n",
    "- **Chat:** command series, command-r variants\n",
    "- **Embeddings:** embed-english, embed-multilingual (v2.0 and v3.0)\n",
    "- **Note:** Supports fine-tuned models\n",
    "\n",
    "#### **Amazon Bedrock**\n",
    "- **Completions:** Anthropic Claude, Cohere Command, AI21 Labs J2 series\n",
    "- **Chat:** Claude 3.5 Sonnet, Claude 3 Opus/Sonnet/Haiku, Cohere Command-R, Amazon Nova series\n",
    "- **Embeddings:** Amazon Titan Embed, Cohere Embed\n",
    "\n",
    "#### **Google Cloud Vertex AI**\n",
    "- **Completions:** text-bison\n",
    "- **Chat:** chat-bison, gemini-pro, gemini-1.0/1.5/2.0 variants\n",
    "- **Embeddings:** text-embedding-004/005, textembedding-gecko\n",
    "\n",
    "#### **Mosaic AI Model Serving**\n",
    "- Can serve any Databricks-hosted endpoint for completions, chat, and embeddings\n",
    "\n",
    "## Creating Foundation Model Serving Endpoints\n",
    "\n",
    "To use foundation models in your AI applications, you must create a model serving endpoint:\n",
    "\n",
    "### **Endpoint Creation Methods:**\n",
    "\n",
    "1. **Foundation Model APIs Provisioned Throughput:**\n",
    "   - Use REST API for fine-tuned foundation model variants\n",
    "   - Best for production workloads with guaranteed throughput\n",
    "   - See provisioned throughput documentation for API details\n",
    "\n",
    "2. **External Models:**\n",
    "   - Create endpoints for externally hosted models\n",
    "   - Centralized governance and management\n",
    "   - Unified interface across providers\n",
    "\n",
    "3. **Unified API and UI:**\n",
    "   - Model Serving provides consistent experience\n",
    "   - Create and update endpoints through single interface\n",
    "   - Simplified management across model types\n",
    "\n",
    "## Querying Foundation Model Endpoints\n",
    "\n",
    "### **OpenAI-Compatible API:**\n",
    "- **Unified experience** across all foundation models\n",
    "- **SDK support** for major programming languages\n",
    "- **Simplified integration** for production applications\n",
    "- **Cross-cloud compatibility** for consistent behavior\n",
    "\n",
    "### **Key Benefits:**\n",
    "- Experiment with different models using the same code\n",
    "- Easy switching between providers\n",
    "- Streamlined production deployment\n",
    "- Consistent API regardless of underlying model\n",
    "\n",
    "## Best Practices and Considerations\n",
    "\n",
    "### **Choosing the Right Option:**\n",
    "\n",
    "**Use Pay-Per-Token when:**\n",
    "- Experimenting with different models\n",
    "- Low to moderate usage volumes\n",
    "- Testing proof-of-concepts\n",
    "- No performance guarantees needed\n",
    "\n",
    "**Use Provisioned Throughput when:**\n",
    "- Production workloads with high volume\n",
    "- Performance guarantees required\n",
    "- Predictable usage patterns\n",
    "- Cost optimization for sustained usage\n",
    "\n",
    "**Use External Models when:**\n",
    "- Need specific proprietary models (OpenAI, Anthropic)\n",
    "- Multi-provider strategy\n",
    "- Centralized governance requirements\n",
    "- Leveraging existing provider relationships\n",
    "\n",
    "### **Regional Deployment Strategy:**\n",
    "- Deploy in regions closest to your data and users\n",
    "- Consider data residency requirements\n",
    "- Enable cross-geography routing for GPU availability when needed\n",
    "- Plan for model retirement timelines\n",
    "\n",
    "### **Governance and Compliance:**\n",
    "- Review model developer licenses and acceptable use policies\n",
    "- Implement centralized governance through Databricks\n",
    "- Track usage and costs across model providers\n",
    "- Ensure compliance with applicable regulations\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Assess Your Requirements:**\n",
    "   - Determine use case (experimentation vs. production)\n",
    "   - Evaluate performance needs\n",
    "   - Consider budget constraints\n",
    "   - Review compliance requirements\n",
    "\n",
    "2. **Select Your Models:**\n",
    "   - Choose between Databricks-hosted and external models\n",
    "   - Verify regional availability\n",
    "   - Check for upcoming retirements\n",
    "   - Consider fine-tuning needs\n",
    "\n",
    "3. **Create Endpoints:**\n",
    "   - Use unified API or UI\n",
    "   - Configure appropriate serving type (pay-per-token or provisioned)\n",
    "   - Set up governance and access controls\n",
    "   - Test with sample queries\n",
    "\n",
    "4. **Integrate and Deploy:**\n",
    "   - Use OpenAI-compatible SDK\n",
    "   - Implement error handling and retries\n",
    "   - Monitor usage and performance\n",
    "   - Plan for scaling and optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Foundation Model APIs Documentation](https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis/)\n",
    "- [External Models Guide](https://docs.databricks.com/aws/en/generative-ai/external-models/)\n",
    "- [Model Serving Limits and Regions](https://docs.databricks.com/aws/en/machine-learning/model-serving/model-serving-limits)\n",
    "- [Retired Models Policy](https://docs.databricks.com/aws/en/machine-learning/retired-models-policy)\n",
    "\n",
    "*Â© Databricks 2025 | Last Updated: November 20, 2025*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Introduction of Foundational Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
