{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58ef52c7-30b6-4ef4-ae2f-4a18c0360506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I'll create a well-structured blog article based on the Databricks documentation about reasoning models.\n",
    "\n",
    "---\n",
    "\n",
    "# Understanding Reasoning Models in Databricks: A Complete Guide\n",
    "\n",
    "**Last Updated: November 20, 2025**\n",
    "\n",
    "Artificial intelligence has evolved beyond simple pattern matching. Today's foundation models can \"think through\" problems, breaking them down step-by-step before providing answers. This capability, known as reasoning, represents a significant leap forward in AI capabilities. In this comprehensive guide, we'll explore how Databricks implements reasoning models through their Foundation Model API.\n",
    "\n",
    "## What Are Reasoning Models?\n",
    "\n",
    "Reasoning models are advanced AI systems that can tackle complex tasks by engaging in a structured thought process before generating responses. Unlike traditional models that produce immediate outputs, reasoning models allocate computational resources to \"think\" about the problem, considering different approaches and refining their understanding before delivering a final answer.\n",
    "\n",
    "Some reasoning models even provide transparency by revealing their step-by-step thought process, allowing users to understand how they arrived at their conclusions.\n",
    "\n",
    "## Two Types of Reasoning Approaches\n",
    "\n",
    "Databricks categorizes reasoning models into two distinct types:\n",
    "\n",
    "### 1. Reasoning-Only Models\n",
    "\n",
    "These models always engage in internal reasoning for every response. Examples include:\n",
    "- **GPT-5 family**: databricks-gpt-5-1, databricks-gpt-5, databricks-gpt-5-mini, and databricks-gpt-5-nano\n",
    "- **GPT OSS models**: databricks-gpt-oss-120b and databricks-gpt-oss-20b\n",
    "\n",
    "These models use a `reasoning_effort` parameter that controls how deeply they think through problems. Higher reasoning effort can produce more thoughtful and accurate responses, though it may increase latency and token usage.\n",
    "\n",
    "### 2. Hybrid Reasoning Models\n",
    "\n",
    "Hybrid models offer flexibility by supporting both instant responses and deeper reasoning when needed. This category includes:\n",
    "- **Claude models**: databricks-claude-sonnet-4 and databricks-claude-sonnet-4-5\n",
    "- **Gemini 3 models**: databricks-gemini-3-pro\n",
    "- **Gemini 2.5 models**: databricks-gemini-2-5-pro and databricks-gemini-2-5-flash\n",
    "\n",
    "Hybrid models allow developers to choose when to invoke reasoning capabilities, making them ideal for applications that need to balance speed with complexity.\n",
    "\n",
    "## Key Parameters for Controlling Reasoning\n",
    "\n",
    "Different models use different parameters to control their reasoning behavior:\n",
    "\n",
    "### For Claude and Gemini 2.5 Models: `thinking.budget_tokens`\n",
    "\n",
    "This parameter controls how many tokens the model can allocate to internal thought processes. Higher budgets enable the model to tackle more complex tasks, though optimal results typically occur below 32K tokens. Importantly, `budget_tokens` must be less than `max_tokens`.\n",
    "\n",
    "### For GPT-5 and GPT OSS Models: `reasoning_effort`\n",
    "\n",
    "This parameter accepts different values depending on the model:\n",
    "- **GPT-5.1**: Set to \"none\" by default, but can be overridden\n",
    "- **GPT-5 models**: Set to \"minimal\" by default\n",
    "- **GPT OSS models**: Accepts \"low\", \"medium\" (default), or \"high\"\n",
    "\n",
    "### For Gemini 3 Models: `reasoning_effort`\n",
    "\n",
    "Accepts \"low\" (default), \"medium\", or \"high\" values to control the depth of reasoning.\n",
    "\n",
    "## Practical Implementation Examples\n",
    "\n",
    "Let's look at how to query these models using Python and the OpenAI client library:\n",
    "\n",
    "### Querying a Claude Model with Reasoning\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get('YOUR_DATABRICKS_TOKEN'),\n",
    "    base_url=os.environ.get('YOUR_DATABRICKS_BASE_URL')\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"databricks-claude-3-7-sonnet\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n",
    "    ],\n",
    "    max_tokens=20480,\n",
    "    extra_body={\n",
    "        \"thinking\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 10240\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract reasoning and answer\n",
    "msg = response.choices[0].message\n",
    "reasoning = msg.content[0][\"summary\"][0][\"text\"]\n",
    "answer = msg.content[1][\"text\"]\n",
    "\n",
    "print(\"Reasoning:\", reasoning)\n",
    "print(\"Answer:\", answer)\n",
    "```\n",
    "\n",
    "This example demonstrates how Claude models can expose their reasoning process, allowing you to see both the thought process and the final answer.\n",
    "\n",
    "## Understanding the Response Structure\n",
    "\n",
    "When a reasoning model responds, it includes multiple content blocks:\n",
    "\n",
    "1. **Reasoning block**: Contains the model's internal thought process (when exposed)\n",
    "2. **Text block**: Contains the final answer\n",
    "\n",
    "This structure allows applications to choose whether to display the reasoning process to users or only show the final answer.\n",
    "\n",
    "## Managing Multi-Turn Conversations\n",
    "\n",
    "For models like Claude that expose reasoning tokens, managing multi-turn conversations requires careful consideration. The reasoning blocks from only the most recent assistant turn are visible to the model and counted as input tokens.\n",
    "\n",
    "You have two options:\n",
    "\n",
    "**Option 1: Omit reasoning blocks** - If the model doesn't need to reference its previous thought process, you can exclude reasoning blocks to save tokens.\n",
    "\n",
    "**Option 2: Include full assistant messages** - If you're building experiences that surface intermediate reasoning or need the model to reflect on its previous thinking, include the complete assistant message with reasoning blocks.\n",
    "\n",
    "## How Reasoning Models Work Under the Hood\n",
    "\n",
    "Reasoning models introduce special reasoning tokens alongside standard input and output tokens. These tokens enable the model to:\n",
    "\n",
    "1. **Break down complex prompts** into manageable components\n",
    "2. **Consider multiple approaches** to solving the problem\n",
    "3. **Refine understanding** before committing to an answer\n",
    "4. **Generate a final response** based on this internal deliberation\n",
    "\n",
    "Different model families handle these reasoning tokens differently. Some expose them to users for transparency, while others keep them internal and only show the final output.\n",
    "\n",
    "## Best Practices for Using Reasoning Models\n",
    "\n",
    "1. **Match reasoning effort to task complexity**: Simple queries don't need high reasoning effort, while complex problems benefit from it\n",
    "2. **Monitor token usage**: Reasoning consumes additional tokens, so balance quality needs with cost considerations\n",
    "3. **Consider latency requirements**: Higher reasoning effort increases response time\n",
    "4. **Choose the right model type**: Use reasoning-only models for consistently complex tasks, hybrid models for varied workloads\n",
    "5. **Leverage transparency**: When models expose reasoning, use it to validate outputs and improve prompts\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Reasoning models represent a significant advancement in AI capabilities, offering enhanced problem-solving abilities for complex tasks. Databricks' Foundation Model API provides unified access to various reasoning models, each with unique characteristics and control parameters.\n",
    "\n",
    "By understanding the differences between reasoning-only and hybrid models, and learning to effectively use parameters like `reasoning_effort` and `budget_tokens`, developers can build more sophisticated AI applications that balance performance, cost, and quality.\n",
    "\n",
    "Whether you're building analytical tools, decision support systems, or complex reasoning applications, Databricks' reasoning models provide the flexibility and power needed to tackle challenging problems effectively.\n",
    "\n",
    "---\n",
    "\n",
    "**Related Resources:**\n",
    "- [Query Chat Models on Databricks](https://docs.databricks.com/aws/en/machine-learning/model-serving/query-chat-models)\n",
    "- [Query Embedding Models](https://docs.databricks.com/aws/en/machine-learning/model-serving/query-embedding-models)\n",
    "- [Query Vision Models](https://docs.databricks.com/aws/en/machine-learning/model-serving/query-vision-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2295ad7-c226-4ca8-a915-1633149b7601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All reasoning models are accessed through the chat completions endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f69e0c0-cd84-40c3-b3c1-30e3ee6db6cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Claude Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237a8a91-2eb1-449a-850d-7b1e785cb9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qq databricks-sdk[openai]>=0.35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c67569-e976-4d91-8473-0259238902b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7677ea72-a2df-434d-9c7d-69f1b748cc56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import httpx\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"\",\n",
    "  base_url=\"https://fe-vm-agentic-ai.cloud.databricks.com/serving-endpoints\"\n",
    "  )\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"databricks-claude-3-7-sonnet\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    max_tokens=20480,\n",
    "    extra_body={\n",
    "        \"thinking\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 10240\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "msg = response.choices[0].message\n",
    "reasoning = msg.content[0][\"summary\"][0][\"text\"]\n",
    "answer = msg.content[1][\"text\"]\n",
    "\n",
    "print(\"Reasoning:\", reasoning)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d257d6a3-a7ce-492a-bb2e-30910b7a0038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(msg.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53649f92-254a-4c41-bf24-32a7b345eade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import httpx\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"\",\n",
    "  base_url=\"https://fe-vm-agentic-ai.cloud.databricks.com/serving-endpoints\"\n",
    "  )\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"databricks-claude-3-7-sonnet\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    max_tokens=20480,\n",
    "    extra_body={\n",
    "        \"thinking\": {\n",
    "            \"type\": \"disabled\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "msg = response.choices[0].message\n",
    "# reasoning = msg.content[0][\"summary\"][0][\"text\"]\n",
    "# answer = msg.content[1][\"text\"]\n",
    "\n",
    "# print(\"Reasoning:\", reasoning)\n",
    "# print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f8289a-bf14-4426-b260-770582f7e99d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(msg.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a07ee0-2f0f-458d-879e-74b5371d7543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## GPT-5.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9e1cfd-3d0a-40b9-b029-8fc1ea6fda84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import httpx\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"\",\n",
    "  base_url=\"https://fe-vm-agentic-ai.cloud.databricks.com/serving-endpoints\"\n",
    "  )\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"databricks-gpt-5-1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    max_tokens=20480,\n",
    "    reasoning_effort= \"high\"\n",
    ")\n",
    "\n",
    "msg = response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a40efa-d182-4e9c-9636-33a9c2741477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(msg.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cab2590-3d7e-4fc1-8a82-be56c8ca0449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(msg.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83a490d8-18d3-410f-85b8-9abdb012603a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Quey Reasoning Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
