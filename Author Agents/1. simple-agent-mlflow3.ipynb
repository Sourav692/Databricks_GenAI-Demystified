{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a5c5b1-0fa7-499f-a4a9-f83219c4c435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Mosaic AI Agent Framework: Author, deploy, and trace a simple agent\n",
    "\n",
    "This notebook demonstrates how to build and manage a simple gen AI agent:\n",
    "- Author a simple gen AI agent with the MLflow 3 `ResponsesAgent` API.\n",
    "- Manually test the agent, and run batch evaluation using MLflow.\n",
    "- Log and deploy the agent with Mosaic AI Agent Framework.\n",
    "- Trace and monitor the agent in real time.\n",
    "\n",
    "You can use this pattern with any Agent Framework agent ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/author-agent)).\n",
    "\n",
    "MLflow 3 ([AWS](https://docs.databricks.com/aws/en/mlflow3/genai) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/) | [GCP](https://docs.databricks.com/gcp/en/mlflow3/genai) | [OSS](https://mlflow.org/docs/latest/genai)) provides observability features allowing you to:\n",
    "- Track quality and operational performance (latency, request volume, errors, etc.)\n",
    "- Run LLM-based evaluations on production traffic to detect drift or regressions using Agent Evaluation's LLM judges\n",
    "- Deep dive into individual requests to debug and improve agent responses.\n",
    "- Transform real-world logs into evaluation sets to drive continuous improvements\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Address `TODO`s in this notebook before clicking `Run all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f39e7d3-b1e6-4c0d-80e1-6af211ae2c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q backoff databricks-openai uv databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5dedd52-85ee-453a-a2a3-9a67733b2f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Define the agent in code\n",
    "Define the agent code in a single cell below. This lets you easily write the agent code to a local Python file `agent.py`, using the `%%writefile` magic command, for subsequent logging and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a03dc7d-c497-4a2c-8f7c-d2f8bc53dcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import json\n",
    "import warnings\n",
    "from typing import Any, Generator\n",
    "\n",
    "import backoff\n",
    "import mlflow\n",
    "import openai\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that provides brief, clear responses.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SimpleChatAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Simple chat agent that calls an LLM using the Databricks OpenAI client API.\n",
    "\n",
    "    You can replace this with your own agent.\n",
    "    The decorators @mlflow.trace tell MLflow Tracing to track calls to the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.workspace_client = WorkspaceClient()\n",
    "        self.client: OpenAI = self.workspace_client.serving_endpoints.get_open_ai_client()\n",
    "        self.llm_endpoint = LLM_ENDPOINT_NAME\n",
    "        self.SYSTEM_PROMPT = SYSTEM_PROMPT\n",
    "\n",
    "    @backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
    "    @mlflow.trace(span_type=SpanType.LLM)\n",
    "    def call_llm(self, messages: list[dict[str, Any]]) -> Generator[dict[str, Any], None, None]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"PydanticSerializationUnexpectedValue\")\n",
    "            for chunk in self.client.chat.completions.create(\n",
    "                model=self.llm_endpoint,\n",
    "                messages=self.prep_msgs_for_cc_llm(messages),\n",
    "                stream=True,\n",
    "            ):\n",
    "                yield chunk.to_dict()\n",
    "\n",
    "    # With autologging, you do not need @mlflow.trace here, but you can add it to override the span type.\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    # With autologging, you do not need @mlflow.trace here, but you can add it to override the span type.\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + [\n",
    "            i.model_dump() for i in request.input\n",
    "        ]\n",
    "        yield from self.output_to_responses_items_stream(chunks=self.call_llm(messages))\n",
    "\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "AGENT = SimpleChatAgent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100a078c-4458-49e1-960d-22b224ba51e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. \n",
    "\n",
    "Since you manually traced methods within `ResponsesAgent`, you can view the trace for each step the agent takes, with any LLM calls made via the OpenAI SDK automatically traced by autologging.\n",
    "\n",
    "Replace this placeholder input with an appropriate domain-specific example for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68c0173-aa16-4922-9536-42cd3a56a949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e62f8f3-cf63-4c4e-b2d5-ace5f52ef690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"What is 5+5?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc77443c-9886-48fb-88e8-435e85b77bff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for event in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"What is 5+5?\"}]}\n",
    "):\n",
    "    print(event, \"-----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a80113-c0ad-49c1-81bb-fe805ac79652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Log the agent as an MLflow model, and register it to Unity Catalog\n",
    "\n",
    "Log the agent as code from the `agent.py` file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "In the same logging call, we can register the model to Unity Catalog, which will be needed for deploying the agent in the next step.  Read the Databricks documentation to learn more about Models in Unity Catalog ([AWS](https://docs.databricks.com/aws/en/machine-learning/manage-model-lifecycle/) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/manage-model-lifecycle/) | [GCP](https://docs.databricks.com/gcp/en/machine-learning/manage-model-lifecycle/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93be434-3908-47ab-9436-96f98d8e68e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "# The model registry is already set to Databricks Unity Catalog by default,\n",
    "# but you can change the registry below as needed.\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: For a UC-registered model, define the catalog, schema, and model name:\n",
    "catalog = \"AgenticAI\"\n",
    "schema = \"week1\"\n",
    "model_name = \"simple-agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        # Change the model/agent name to be more descriptive for your use case:\n",
    "        name=\"agent\",\n",
    "        # Specify the model via the python file created above:\n",
    "        python_model=\"agent.py\",\n",
    "        # Pin all required dependencies to compatible versions to avoid environment build errors\n",
    "        extra_pip_requirements=[\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",  # Pin to current mlflow version\n",
    "        ],\n",
    "        resources=[DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)],\n",
    "        # This optional parameter lets you register the model at the same time as logging it:\n",
    "        registered_model_name=UC_MODEL_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93cd132c-43e3-4f0b-b988-0bbd33cccd87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pre-deployment agent validation\n",
    "Before deploying the agent, perform pre-deployment checks.\n",
    "\n",
    "* **Manual vibe checks** using the [mlflow.models.predict() API](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.predict). See the Databricks documentation ([AWS](https://docs.databricks.com/en/machine-learning/model-serving/model-serving-debug.html#validate-inputs) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/model-serving-debug#before-model-deployment-validation-checks) | [GCP](https://docs.databricks.com/gcp/en/machine-learning/model-serving/model-serving-debug)).\n",
    "* **Dataset evaluation checks** using the [mlflow.genai.evaluate() API](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.genai.html#mlflow.genai.evaluate).  See the Databricks documentation ([AWS](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/evaluate-app) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/evaluate-app) | [GCP](https://docs.databricks.com/gcp/en/mlflow3/genai/eval-monitor/evaluate-app))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c574f092-51d1-4fc8-88d6-7d793d7bc8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(logged_agent_info.registered_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4238aad-c08e-4470-99cd-eb1b3af30ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"Hello!\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b889280-27b4-4ba1-b251-aa2fbf9fd5b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Batch evaluation\n",
    "\n",
    "We next demonstrate how to use MLflow to evaluate the agent on a batch of traces.  See Databricks documentation ([AWS](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/) | [GCP](https://docs.databricks.com/gcp/en/mlflow3/genai/eval-monitor/)).\n",
    "* Collect traces to score.\n",
    "* Select scorers (LLM judges) to run.\n",
    "* Run evaluation to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afcf3398-290f-4e59-882b-914be9a6502c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traces = mlflow.search_traces(max_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a926e9-5cc5-4c3c-a9d3-e13b80461fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2779b90-9ab6-4dfb-93f5-3f6d068e81b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import (\n",
    "    RelevanceToQuery,\n",
    "    Safety,\n",
    "    Guidelines,\n",
    ")\n",
    "\n",
    "scorers = [\n",
    "  RelevanceToQuery(),  # Checks if email addresses the user's request\n",
    "  Safety(),  # Checks for harmful or inappropriate content\n",
    "  # Custom guideline below:\n",
    "  Guidelines(\n",
    "      name=\"concise_communication\",\n",
    "      guidelines=\"The response MUST be concise and to the point.\",\n",
    "  ),\n",
    "]\n",
    "\n",
    "# Run evaluation with the scorers selected above.\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=traces,\n",
    "    model_id=logged_agent_info.model_id,\n",
    "    scorers=scorers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5bb4ec3-69c9-46dd-b1e4-d2bfd3811dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent\n",
    "\n",
    "Deploy the agent using Agent Framework ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/author-agent)).  This will, by default, log the deployed agent's traces to the current experiment, as well as inference tables (if enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df93a1f-30d7-47aa-b126-bd580715311a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(UC_MODEL_NAME, model_version=logged_agent_info.registered_model_version, tags={\"endpointSource\": \"docs\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef4eba9f-a77f-4c07-b2d3-0f8caa2c06e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View **real time** traces from your endpoint\n",
    "\n",
    "By default, the deployed agent will log, in **real-time**, its traces to the MLflow Experiment attached to this notebook. If you wish to change the MLflow Experiment that contains your traces, call `mlflow.set_experiment(...)` before calling `agents.deploy(...)`.\n",
    "\n",
    "You can optionally enable production monitoring to copy traces from the MLflow experiment into a Delta table. ([AWS](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/production-monitoring) | [GCP](https://docs.databricks.com/gcp/en/mlflow3/genai/eval-monitor/production-monitoring)).  If you enable monitoring, then you can visit the MLflow Experiment's **Monitoring** tab to update the quality scorers run on your production traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a23d9f7-74cd-45bc-a805-c66ae937026e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\\nView traces from your endpoint in REAL TIME in the MLflow experiment here: https://{mlflow.utils.databricks_utils.get_browser_hostname()}/ml/experiments/{mlflow.get_experiment_by_name(mlflow.utils.databricks_utils.get_notebook_path()).experiment_id}/traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f615a74d-307b-4de8-b56c-814952424b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with your organization for feedback, or embed it in a production application.\n",
    "\n",
    "## Resources\n",
    "\n",
    "* Agent Framework documentation [AWS](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent) | [GCP](https://docs.databricks.com/gcp/en/generative-ai/agent-framework/author-agent)\n",
    "* MLflow 3 documentation [AWS](https://docs.databricks.com/aws/en/mlflow3/genai) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/) | [GCP](https://docs.databricks.com/gcp/en/mlflow3/genai) | [OSS](https://mlflow.org/docs/latest/genai)\n",
    "* MLflow Evaluation documentation [AWS](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/) | [GCP](https://docs.databricks.com/gcp/en/mlflow3/genai/eval-monitor/) | [OSS](https://mlflow.org/docs/latest/genai/eval-monitor/)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6074041342590311,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1. simple-agent-mlflow3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
