{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d0205df-8d30-4ca0-973c-98e5fedf303d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "https://docs.databricks.com/aws/en/notebooks/source/machine-learning/large-language-models/llm-benchmarking.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe87557-a0b7-4e9e-b334-f871a65e42d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Large language model endpoints benchmarking script\n",
    "\n",
    "To use this notebook, update the Databricks serving `endpoint_name` and number of `input_tokens` and `output tokens` in the next cell. At the end of the notebook a latency versus throughput graph is calculated and the benchmark is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e60fb74-21e8-40ef-9b01-11a39ec4e616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update this with the name of the endpoint to benchmark\n",
    "endpoint_name = '<YOUR-ENDPOINT-NAME>'\n",
    "# Number of input and outut tokens to benchmark\n",
    "input_tokens = 2048\n",
    "output_tokens = 256\n",
    "# Number of queries per thread, higher gives more accurate results\n",
    "num_queries_per_thread = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c815473-9a28-48e1-a8ea-7e18fcc6ea4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d3d9e8-98ab-4a7d-a733-666e3658a00b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "import requests\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib\n",
    "import math\n",
    "\n",
    "# Set up the endpoint UTL and headers so that you can query the server.\n",
    "API_ROOT = \"<YOUR-WORKSPACE-URL>\"\n",
    "API_TOKEN = \"<YOUR-API-TOKEN>\"\n",
    "\n",
    "\n",
    "headers = {'Authorization': f'Bearer {API_TOKEN}', 'Content-Type': 'application/json'}\n",
    "endpoint_url = f'{API_ROOT}/serving-endpoints/{endpoint_name}/invocations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0883c0c-bf0a-4b6e-9548-1c313f1a6d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following `get_request` function sets the request for each query. The number of tokens in the prompt must match the number of tokens the model sees. The prompt also must contain a single token from the tokenizer corresponding to the model being benchmarked. The example in this notebook works for Llama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e37648-8b84-4070-9c1c-fae9e6cc2d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_request(in_tokens, out_tokens):\n",
    "  # Edit the code so that the input of input/output tokens is as expected. This might depend on the tokenizer the model is using.\n",
    "  return {'prompt': '<|begin_of_text|>'*(in_tokens-1) , 'temperature': 0.0, 'max_tokens': out_tokens, 'ignore_eos': True}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab62a0e-25f3-4e11-96da-d7feb71a7d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, you can validate the number of input tokens. However, you might need to manually edit this as it depends on the tokenizer used by the model. The following example: \n",
    "\n",
    "- Runs 10 queries.\n",
    "- Validates the number of input tokens matches the number of tokens the model can see.\n",
    "- Warms up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebafd9b6-b165-4fb0-a537-b42dd3809eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sends an inital set of warm up requests and validates that you are sending the correct number of input tokens.\n",
    "def warm_up_and_validate(in_tokens=2048, out_tokens=256, warm_up_requests=10):\n",
    "  input_data = get_request(in_tokens, out_tokens)\n",
    "  input_json = json.dumps(input_data)\n",
    "  req = requests.Request('POST', endpoint_url, headers=headers, data=input_json)\n",
    "  prepped = req.prepare()\n",
    "  session = requests.Session()\n",
    "  for _ in range(warm_up_requests):\n",
    "    resp = session.send(prepped)\n",
    "    result = json.loads(resp.text)\n",
    "    assert(result['usage']['completion_tokens'] == out_tokens)\n",
    "    assert(result['usage']['prompt_tokens'] == in_tokens), f\"Model received {result['usage']['prompt_tokens']} input tokens, expected {in_tokens}. Please adjust the input prompt in cell 4.\"\n",
    "\n",
    "warm_up_and_validate(input_tokens, output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b8a6e98-d701-465f-92fb-fee36b8627c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Benchmarking library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f423a7e-d642-4495-9bec-1eaf904139ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latencies = []\n",
    "\n",
    "# This is a single worker, which processes the given number of requests, one after the other.\n",
    "async def worker(index, num_requests, in_tokens=2048, out_tokens=256):\n",
    "  input_data = get_request(in_tokens, out_tokens)\n",
    "  # Sleep some time to offset the the threads.\n",
    "  await asyncio.sleep(0.1*index)\n",
    "  \n",
    "  for i in range(num_requests):\n",
    "    request_start_time = time.time()\n",
    "    \n",
    "    success = False \n",
    "    while not success:\n",
    "      timeout = aiohttp.ClientTimeout(total=3 * 3600)\n",
    "      async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "        async with session.post(endpoint_url, headers=headers, json=input_data) as response:\n",
    "          success = response.ok\n",
    "          chunks = []\n",
    "          async for chunk, _ in response.content.iter_chunks():\n",
    "            chunks.append(chunk)\n",
    "    latency = time.time() - request_start_time\n",
    "    result = json.loads(b''.join(chunks))\n",
    "    latencies.append((result['usage']['prompt_tokens'], \n",
    "                      result['usage']['completion_tokens'], latency))\n",
    "\n",
    "\n",
    "# This code runs parallel_requests' parallel sets of queries with num_requests_per_worker queries per worker.\n",
    "async def single_benchmark(num_requests_per_worker, num_workers, in_tokens=2048, out_tokens=256):\n",
    "  tasks = []\n",
    "  for i in range(num_workers):\n",
    "    task = asyncio.create_task(worker(i, num_requests_per_worker, in_tokens, out_tokens))\n",
    "    tasks.append(task)\n",
    "  await asyncio.gather(*tasks)\n",
    "\n",
    "# This runs the benchmark with 1, n//2 and n output tokens to enable deriving time to first token (from 1 output token)\n",
    "# and the time per token by looking at the difference in latency between 64 and 128 output tokens.\n",
    "async def benchmark(parallel_queries=1, in_tokens=2048, out_tokens=256, num_tries=5):\n",
    "  # store statistics about the number of input/outpu and the latency for each setup.\n",
    "  avg_num_input_tokens = [0, 0, 0]\n",
    "  avg_num_output_tokens = [0, 0, 0]\n",
    "  median_latency = [0, 0, 0]\n",
    "  print(f\"Parallel queries {parallel_queries}\")\n",
    "  for i, out_tokens in enumerate([1, out_tokens//2, out_tokens]):\n",
    "    # Clear the latencies array so that you get fresh statistics.\n",
    "    latencies.clear()\n",
    "    await single_benchmark(num_tries, parallel_queries, in_tokens, out_tokens)\n",
    "    # Compute the median latency and the mean number of tokens.\n",
    "    avg_num_input_tokens[i] = statistics.mean([inp for inp, _, _ in latencies])\n",
    "    avg_num_output_tokens[i] = statistics.mean([outp for _, outp, _ in latencies])\n",
    "    median_latency[i] = statistics.median([latency for _, _, latency in latencies])\n",
    "    tokens_per_sec = (avg_num_input_tokens[i]+avg_num_output_tokens[i])*parallel_queries/median_latency[i]\n",
    "    print(f'Output tokens {avg_num_output_tokens[i]}, median latency (s): {round(median_latency[i], 2)}, tokens per second {round(tokens_per_sec, 1)}')\n",
    "  \n",
    "  # Use the difference in the time between out_tokens//2 and out_tokens to find the time per output token\n",
    "  # these are stored in median_latency[1] and median_latency[2] respectively\n",
    "  # The time to generate just 1 token to get the time to first token is stored in median_latency[0]\n",
    "  output_token_time = (median_latency[2] - median_latency[1])*1000/(avg_num_output_tokens[2]-avg_num_output_tokens[1])\n",
    "  print(f'Time to first token (s): {round(median_latency[0],2)}, Time per output token (ms) {round(output_token_time,2)}')\n",
    "  data.append([median_latency[2],\n",
    "               (avg_num_input_tokens[2]+avg_num_output_tokens[2])*parallel_queries/median_latency[2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af90a159-4d80-4157-a9a8-10943900d3e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run the benchmark with differing parallel queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46476524-668e-400e-94f3-1d2fbd4d72bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This runs until the throughput of the model is no longer increasing by 10%.\n",
    "data = []\n",
    "for parallel_queries in [1, 2, 4, 8]:\n",
    "  print(f\"Input tokens {input_tokens}\")\n",
    "  await benchmark(parallel_queries, input_tokens, output_tokens, num_queries_per_thread)\n",
    "  # Break if the throughput doesn't increase by more than 10%\n",
    "  if len(data) > 1 and (data[-1][1] - data[-2][1])/data[-2][1] < 0.1:\n",
    "    break\n",
    "\n",
    "# Plot the latency vs throughput curve\n",
    "matplotlib.pyplot.xlabel(\"Latency (s)\")\n",
    "matplotlib.pyplot.ylabel(\"Throughput (tok/s)\")\n",
    "line = matplotlib.pyplot.plot([x[0] for x in data], [x[1] for x in data], marker='o')\n",
    "matplotlib.pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3. Model endpoints benchmarking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
