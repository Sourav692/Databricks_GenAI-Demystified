{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1fdbf43-86f9-4617-abcd-6aff3a12f58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Building a Databricks Mosaic AI Agent: A Complete Walkthrough\n",
    "\n",
    "As a Solution Architect at Databricks, I'm excited to walk you through this comprehensive notebook that demonstrates how to build an intelligent AI agent using Databricks' Mosaic AI platform. This notebook showcases the power of combining retrieval-augmented generation (RAG) with tool-calling capabilities to create a sophisticated question-answering system for Databricks documentation.\n",
    "\n",
    "## Overview: What We're Building\n",
    "\n",
    "This notebook creates a **DocsAgent** - an intelligent assistant that can answer questions about Databricks by retrieving relevant documentation chunks and leveraging Unity Catalog functions. The agent uses TF-IDF-based retrieval combined with a large language model to provide accurate, contextual responses.\n",
    "\n",
    "Let's dive deep into each component:\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 1: Essential Imports and Dependencies\n",
    "\n",
    "```python\n",
    "from typing import Any, Optional, Sequence, Union\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.models import ModelConfig\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "```\n",
    "\n",
    "### Purpose and Breakdown:\n",
    "\n",
    "**Type Hints & Core Libraries:**\n",
    "- `typing` imports provide type safety for better code documentation\n",
    "- `mlflow` handles model lifecycle management and experiment tracking\n",
    "- `pandas` manages data manipulation for our document corpus\n",
    "\n",
    "**Databricks-Specific Integrations:**\n",
    "- `ChatDatabricks`: Interface to Databricks-hosted language models\n",
    "- `DatabricksFunctionClient` & `UCFunctionToolkit`: Enable integration with Unity Catalog functions\n",
    "- These allow our agent to call registered functions from Unity Catalog as tools\n",
    "\n",
    "**LangChain Framework Components:**\n",
    "- `langchain_core`: Provides abstractions for language models and runnables\n",
    "- `langgraph`: Enables building complex agent workflows with state management\n",
    "- These create the orchestration layer for our agent's decision-making process\n",
    "\n",
    "**ML/NLP Libraries:**\n",
    "- `TfidfVectorizer`: Creates vector representations of documents for similarity search\n",
    "- This forms the backbone of our retrieval system\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 2: Data Loading and Preprocessing\n",
    "\n",
    "```python\n",
    "databricks_docs_url = \"https://raw.githubusercontent.com/databricks/genai-cookbook/refs/heads/main/quick_start_demo/chunked_databricks_docs_filtered.jsonl\"\n",
    "parsed_docs_df = pd.read_json(databricks_docs_url, lines=True)\n",
    "\n",
    "documents = parsed_docs_df\n",
    "doc_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = doc_vectorizer.fit_transform(documents[\"content\"])\n",
    "```\n",
    "\n",
    "### What's Happening Here:\n",
    "\n",
    "**Document Corpus Loading:**\n",
    "- Downloads pre-processed Databricks documentation chunks in JSONL format\n",
    "- Each line contains a document chunk with content, metadata, and source URI\n",
    "- The chunks are optimally sized for retrieval and LLM context windows\n",
    "\n",
    "**TF-IDF Vectorization Process:**\n",
    "1. **TfidfVectorizer Configuration:**\n",
    "   - `stop_words=\"english\"`: Removes common English words that don't carry semantic meaning\n",
    "   - Creates a vocabulary from all unique words in the document corpus\n",
    "\n",
    "2. **Matrix Creation:**\n",
    "   - `fit_transform()` converts each document's content into a numerical vector\n",
    "   - Each dimension represents the TF-IDF score for a specific term\n",
    "   - This creates a sparse matrix where documents can be compared mathematically\n",
    "\n",
    "**Why TF-IDF?**\n",
    "- **Term Frequency (TF):** Measures how often a word appears in a document\n",
    "- **Inverse Document Frequency (IDF):** Reduces weight of common words across the corpus\n",
    "- Results in vectors that highlight distinctive terms for each document\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 3: Document Retrieval Tool\n",
    "\n",
    "```python\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5) -> list[dict[str, Any]]:\n",
    "    \"\"\"gets relevant documents for the query\"\"\"\n",
    "    query_tfidf = doc_vectorizer.transform([query])\n",
    "    similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
    "    ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    result = []\n",
    "    for idx, score in ranked_docs[:top_n]:\n",
    "        row = documents.iloc[idx]\n",
    "        content = row[\"content\"]\n",
    "        doc_entry = {\n",
    "            \"page_content\": content,\n",
    "            \"metadata\": {\n",
    "                \"doc_uri\": row[\"doc_uri\"],\n",
    "                \"score\": score,\n",
    "            },\n",
    "        }\n",
    "        result.append(doc_entry)\n",
    "    return result\n",
    "```\n",
    "\n",
    "### Detailed Function Analysis:\n",
    "\n",
    "**Decorators:**\n",
    "- `@tool`: Marks this function as a LangChain tool that can be called by the agent\n",
    "- `@mlflow.trace()`: Enables MLflow tracking for observability and debugging\n",
    "- `SpanType.RETRIEVER`: Categorizes this as a retrieval operation in MLflow traces\n",
    "\n",
    "**Retrieval Algorithm:**\n",
    "1. **Query Vectorization:**\n",
    "   ```python\n",
    "   query_tfidf = doc_vectorizer.transform([query])\n",
    "   ```\n",
    "   - Converts the user's query into the same TF-IDF vector space as documents\n",
    "   - Uses the fitted vocabulary from the document corpus\n",
    "\n",
    "2. **Similarity Computation:**\n",
    "   ```python\n",
    "   similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
    "   ```\n",
    "   - Performs matrix multiplication between document matrix and query vector\n",
    "   - Results in cosine similarity scores for each document\n",
    "   - Higher scores indicate better semantic matches\n",
    "\n",
    "3. **Ranking and Selection:**\n",
    "   ```python\n",
    "   ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "   ```\n",
    "   - Sorts documents by similarity score in descending order\n",
    "   - `enumerate()` preserves original document indices\n",
    "   - Returns top N most relevant documents\n",
    "\n",
    "**Output Structure:**\n",
    "Each returned document contains:\n",
    "- `page_content`: The actual text content\n",
    "- `metadata`: Source URI and similarity score for provenance tracking\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 4: Agent Workflow Architecture\n",
    "\n",
    "```python\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    def routing_logic(state: ChatAgentState):\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = {\"role\": \"system\", \"content\": agent_prompt}\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        routing_logic,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    return workflow.compile()\n",
    "```\n",
    "\n",
    "### Agent Architecture Breakdown:\n",
    "\n",
    "**Tool Binding:**\n",
    "```python\n",
    "model = model.bind_tools(tools)\n",
    "```\n",
    "- Informs the LLM about available tools and their schemas\n",
    "- Enables the model to generate structured tool calls in its responses\n",
    "\n",
    "**Routing Logic Function:**\n",
    "```python\n",
    "def routing_logic(state: ChatAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.get(\"tool_calls\"):\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "```\n",
    "- Examines the last message in the conversation state\n",
    "- Decides whether to continue to tool execution or end the conversation\n",
    "- Forms the decision-making core of the agent workflow\n",
    "\n",
    "**Message Preprocessing:**\n",
    "```python\n",
    "if agent_prompt:\n",
    "    system_message = {\"role\": \"system\", \"content\": agent_prompt}\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [system_message] + state[\"messages\"]\n",
    "    )\n",
    "```\n",
    "- Prepends system prompt to every conversation if provided\n",
    "- Ensures consistent agent behavior and role definition\n",
    "- Uses LangChain's `RunnableLambda` for functional composition\n",
    "\n",
    "**Workflow Graph Construction:**\n",
    "1. **Node Creation:**\n",
    "   - `\"agent\"`: Handles LLM inference and decision-making\n",
    "   - `\"tools\"`: Executes tool calls and returns results\n",
    "\n",
    "2. **Edge Definitions:**\n",
    "   - Entry point starts with the agent\n",
    "   - Conditional edges from agent based on routing logic\n",
    "   - Deterministic edge from tools back to agent\n",
    "   - Creates a loop for multi-step reasoning\n",
    "\n",
    "**Graph Compilation:**\n",
    "- `workflow.compile()` creates an executable state machine\n",
    "- Optimizes the workflow for efficient execution\n",
    "- Returns a `CompiledGraph` ready for invocation\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 5: DocsAgent Class Implementation\n",
    "\n",
    "```python\n",
    "class DocsAgent(ChatAgent):\n",
    "    def __init__(self, config, tools):\n",
    "        self.config = ModelConfig(development_config=config)\n",
    "        self.tools = tools\n",
    "        self.agent = self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        llm = ChatDatabricks(\n",
    "            endpoint=self.config.get(\"endpoint_name\"),\n",
    "            temperature=self.config.get(\"temperature\"),\n",
    "            max_tokens=self.config.get(\"max_tokens\"),\n",
    "        )\n",
    "        agent = create_tool_calling_agent(\n",
    "            llm,\n",
    "            tools=self.tools,\n",
    "            agent_prompt=self.config.get(\"system_prompt\"),\n",
    "        )\n",
    "        return agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        output = self.agent.invoke(request)\n",
    "        return ChatAgentResponse(**output)\n",
    "```\n",
    "\n",
    "### Class Architecture Analysis:\n",
    "\n",
    "**Inheritance Structure:**\n",
    "- Extends `mlflow.pyfunc.ChatAgent` for MLflow integration\n",
    "- Provides standardized interface for model serving\n",
    "- Enables deployment to Databricks Model Serving endpoints\n",
    "\n",
    "**Initialization Process:**\n",
    "1. **Configuration Management:**\n",
    "   ```python\n",
    "   self.config = ModelConfig(development_config=config)\n",
    "   ```\n",
    "   - Wraps configuration in MLflow's ModelConfig\n",
    "   - Handles environment-specific settings (development vs. production)\n",
    "   - When deployed, `development_config` is replaced with serving config\n",
    "\n",
    "2. **Agent Construction:**\n",
    "   ```python\n",
    "   self.agent = self._build_agent_from_config()\n",
    "   ```\n",
    "   - Delegates agent building to separate method for clarity\n",
    "   - Maintains separation of concerns between configuration and construction\n",
    "\n",
    "**LLM Configuration:**\n",
    "```python\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=self.config.get(\"endpoint_name\"),\n",
    "    temperature=self.config.get(\"temperature\"),\n",
    "    max_tokens=self.config.get(\"max_tokens\"),\n",
    ")\n",
    "```\n",
    "- **endpoint_name**: Specifies which Databricks-hosted model to use\n",
    "- **temperature**: Controls response randomness (lower = more deterministic)\n",
    "- **max_tokens**: Limits response length to prevent excessive token usage\n",
    "\n",
    "**Prediction Interface:**\n",
    "```python\n",
    "def predict(self, messages: list[ChatAgentMessage], ...):\n",
    "    request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "    output = self.agent.invoke(request)\n",
    "    return ChatAgentResponse(**output)\n",
    "```\n",
    "\n",
    "**Message Processing Flow:**\n",
    "1. **Format Conversion:** Uses built-in helper to convert framework-specific messages to dictionaries\n",
    "2. **Agent Invocation:** Triggers the compiled workflow graph\n",
    "3. **Response Formatting:** Ensures output matches expected ChatAgentResponse schema\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 6: Configuration and Initialization\n",
    "\n",
    "```python\n",
    "# TODO fill in your catalog and schema name\n",
    "catalog = \"\"\n",
    "schema = \"\"\n",
    "\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "baseline_config = {\n",
    "    \"endpoint_name\": LLM_ENDPOINT,\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "tools = [find_relevant_documents]\n",
    "uc_client = DatabricksFunctionClient()\n",
    "set_uc_function_client(uc_client)\n",
    "uc_toolkit = UCFunctionToolkit(function_names=[f\"{catalog}.{schema}.*\"])\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "AGENT = DocsAgent(baseline_config, tools)\n",
    "mlflow.models.set_model(AGENT)\n",
    "```\n",
    "\n",
    "### Configuration Deep Dive:\n",
    "\n",
    "**Unity Catalog Integration:**\n",
    "```python\n",
    "catalog = \"\"  # Your Unity Catalog name\n",
    "schema = \"\"   # Your schema name\n",
    "```\n",
    "- These placeholders need to be filled with actual Unity Catalog coordinates\n",
    "- Enables access to registered functions in your data governance layer\n",
    "\n",
    "**Model Endpoint Selection:**\n",
    "```python\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "```\n",
    "- Uses Meta's Llama 3.3 70B model hosted on Databricks\n",
    "- This is a instruction-tuned model optimized for following directions\n",
    "- 70B parameters provide strong reasoning capabilities for complex queries\n",
    "\n",
    "**Agent Configuration Parameters:**\n",
    "1. **temperature: 0.01**\n",
    "   - Near-deterministic responses for consistent behavior\n",
    "   - Minimizes hallucination in factual question-answering scenarios\n",
    "\n",
    "2. **max_tokens: 1000**\n",
    "   - Reasonable limit for most documentation queries\n",
    "   - Prevents excessive token usage while allowing comprehensive answers\n",
    "\n",
    "3. **system_prompt**\n",
    "   - Defines agent's role and scope (Databricks-focused)\n",
    "   - Sets expectations for tool usage and clarifying questions\n",
    "   - Establishes boundaries for relevant vs. irrelevant queries\n",
    "\n",
    "**Tool Assembly Process:**\n",
    "1. **Base Tools:** Starts with our document retrieval function\n",
    "2. **Unity Catalog Integration:**\n",
    "   ```python\n",
    "   uc_client = DatabricksFunctionClient()\n",
    "   set_uc_function_client(uc_client)\n",
    "   uc_toolkit = UCFunctionToolkit(function_names=[f\"{catalog}.{schema}.*\"])\n",
    "   ```\n",
    "   - Creates client for Unity Catalog function calls\n",
    "   - Sets global client for the toolkit\n",
    "   - Discovers all functions matching the pattern `catalog.schema.*`\n",
    "   - Automatically converts UC functions into LangChain tools\n",
    "\n",
    "3. **Tool Combination:** Extends the base tool list with UC functions\n",
    "\n",
    "**Model Registration:**\n",
    "```python\n",
    "AGENT = DocsAgent(baseline_config, tools)\n",
    "mlflow.models.set_model(AGENT)\n",
    "```\n",
    "- Instantiates the complete agent with configuration and tools\n",
    "- Registers with MLflow for serving and lifecycle management\n",
    "- Makes the agent available for deployment to Model Serving endpoints\n",
    "\n",
    "---\n",
    "\n",
    "## How It All Works Together\n",
    "\n",
    "This notebook creates a sophisticated RAG-enhanced agent through several integrated components:\n",
    "\n",
    "1. **Document Processing Pipeline:** TF-IDF vectorization enables fast semantic search across Databricks documentation\n",
    "\n",
    "2. **Tool-Calling Architecture:** The agent can dynamically decide when to retrieve documents or call Unity Catalog functions based on user queries\n",
    "\n",
    "3. **Workflow Orchestration:** LangGraph manages the complex decision-making process, allowing for multi-step reasoning and tool chaining\n",
    "\n",
    "4. **Production Readiness:** MLflow integration ensures the agent can be deployed, monitored, and managed in production environments\n",
    "\n",
    "5. **Extensibility:** The Unity Catalog integration allows easy addition of custom business functions as agent capabilities\n",
    "\n",
    "## Key Benefits and Use Cases\n",
    "\n",
    "**For Data Engineers:**\n",
    "- Quick access to Databricks documentation during development\n",
    "- Integration with custom data processing functions via Unity Catalog\n",
    "\n",
    "**For Data Scientists:**\n",
    "- Interactive assistance with ML workflows and best practices\n",
    "- Access to both documentation and computational tools in one interface\n",
    "\n",
    "**For Solution Architects:**\n",
    "- Comprehensive platform knowledge at fingertips\n",
    "- Ability to validate architectural decisions against documentation\n",
    "\n",
    "This notebook demonstrates the power of combining retrieval-augmented generation with tool-calling capabilities, creating an intelligent assistant that can both access knowledge and perform actions - a crucial pattern for building practical AI applications in the enterprise.\n",
    "\n",
    "The architecture shown here can be easily extended to support additional data sources, more sophisticated retrieval methods (like vector databases), or domain-specific tool integrations, making it a robust foundation for enterprise AI agent development.\n",
    "\n",
    "[Source: Databricks Documentation](https://docs.databricks.com/aws/en/notebooks/source/generative-ai/mosaic-ai-agent-demo.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1081fe40-a48d-4531-ba5d-9d4cab4df7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step-by-Step Breakdown: `create_tool_calling_agent` Function\n",
    "\n",
    "Let me walk you through the `create_tool_calling_agent` function step by step, explaining each line and concept in detail.\n",
    "\n",
    "## Function Overview\n",
    "\n",
    "```python\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "```\n",
    "\n",
    "**What this function does**: Creates an AI agent that can decide when to use tools vs. when to respond directly to users.\n",
    "\n",
    "**Think of it like**: Building a smart assistant that knows when to look things up vs. when to answer from memory.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Bind Tools to the Model\n",
    "\n",
    "```python\n",
    "model = model.bind_tools(tools)\n",
    "```\n",
    "\n",
    "### What's happening here?\n",
    "\n",
    "**Before binding:**\n",
    "```python\n",
    "# Model only knows how to chat\n",
    "model = ChatDatabricks(endpoint=\"llama-3-70b\")\n",
    "# Model can only generate text responses\n",
    "```\n",
    "\n",
    "**After binding:**\n",
    "```python\n",
    "# Model now knows about available tools\n",
    "model = model.bind_tools([find_relevant_documents, unity_catalog_functions])\n",
    "# Model can now generate both text AND tool calls\n",
    "```\n",
    "\n",
    "### Detailed explanation:\n",
    "\n",
    "1. **Tool Schema Registration**: The model learns about each tool's:\n",
    "   - Name (e.g., \"find_relevant_documents\")\n",
    "   - Description (e.g., \"gets relevant documents for the query\")\n",
    "   - Parameters (e.g., query: string, top_n: integer)\n",
    "\n",
    "2. **Response Format Enhancement**: The model can now generate responses like:\n",
    "   ```python\n",
    "   # Option 1: Regular text response\n",
    "   {\"role\": \"assistant\", \"content\": \"Delta tables are...\"}\n",
    "   \n",
    "   # Option 2: Tool calling response\n",
    "   {\n",
    "       \"role\": \"assistant\", \n",
    "       \"content\": \"Let me search for that information\",\n",
    "       \"tool_calls\": [\n",
    "           {\n",
    "               \"function\": {\n",
    "                   \"name\": \"find_relevant_documents\",\n",
    "                   \"arguments\": '{\"query\": \"delta tables\", \"top_n\": 5}'\n",
    "               }\n",
    "           }\n",
    "       ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "**Real-world analogy**: Like giving a librarian (model) a catalog of all available resources (tools) they can use to help visitors.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Define Routing Logic\n",
    "\n",
    "```python\n",
    "def routing_logic(state: ChatAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.get(\"tool_calls\"):\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "```\n",
    "\n",
    "### What's the purpose?\n",
    "\n",
    "This function decides what happens after the model generates a response. It's like a traffic controller for the conversation flow.\n",
    "\n",
    "### Step-by-step breakdown:\n",
    "\n",
    "1. **Get the latest message**: \n",
    "   ```python\n",
    "   last_message = state[\"messages\"][-1]\n",
    "   ```\n",
    "   - Looks at the most recent message in the conversation\n",
    "   - This will be the response the model just generated\n",
    "\n",
    "2. **Check for tool calls**:\n",
    "   ```python\n",
    "   if last_message.get(\"tool_calls\"):\n",
    "   ```\n",
    "   - Examines if the model requested to use any tools\n",
    "   - `get(\"tool_calls\")` safely checks if this field exists\n",
    "\n",
    "3. **Make routing decision**:\n",
    "   ```python\n",
    "   return \"continue\"  # Go execute tools\n",
    "   # OR\n",
    "   return \"end\"       # Finish the conversation\n",
    "   ```\n",
    "\n",
    "### Visual flow:\n",
    "```\n",
    "Model Response → Routing Logic → Decision\n",
    "     ↓                ↓             ↓\n",
    "\"Let me search...\" → Has tool_calls? → \"continue\" → Execute tools\n",
    "     ↓                ↓             ↓\n",
    "\"Here's the answer\" → No tool_calls? → \"end\" → Finish\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Set Up Message Preprocessing\n",
    "\n",
    "```python\n",
    "if agent_prompt:\n",
    "    system_message = {\"role\": \"system\", \"content\": agent_prompt}\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [system_message] + state[\"messages\"]\n",
    "    )\n",
    "else:\n",
    "    preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "```\n",
    "\n",
    "### Why do we need preprocessing?\n",
    "\n",
    "Every time we call the model, we want to remind it of its role and instructions.\n",
    "\n",
    "### Detailed walkthrough:\n",
    "\n",
    "**Case 1: With system prompt**\n",
    "```python\n",
    "# Input state\n",
    "state = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"How do I create a Delta table?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# System message creation\n",
    "system_message = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"You are a helpful assistant that answers questions about Databricks...\"\n",
    "}\n",
    "\n",
    "# Preprocessor function\n",
    "preprocessor = RunnableLambda(\n",
    "    lambda state: [system_message] + state[\"messages\"]\n",
    ")\n",
    "\n",
    "# Result after preprocessing\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant...\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I create a Delta table?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "**Case 2: Without system prompt**\n",
    "```python\n",
    "# Just passes messages through unchanged\n",
    "preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "```\n",
    "\n",
    "### Why use RunnableLambda?\n",
    "\n",
    "```python\n",
    "RunnableLambda(lambda state: [system_message] + state[\"messages\"])\n",
    "```\n",
    "\n",
    "- **RunnableLambda**: Wraps a simple function to work with LangChain's pipeline system\n",
    "- **Lambda function**: Anonymous function that takes state and returns processed messages\n",
    "- **Integration**: Allows this preprocessing to be part of a larger pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Create the Model Pipeline\n",
    "\n",
    "```python\n",
    "model_runnable = preprocessor | model\n",
    "```\n",
    "\n",
    "### What's the `|` operator?\n",
    "\n",
    "This creates a **pipeline** where output of one step becomes input of the next:\n",
    "\n",
    "```python\n",
    "# Pipeline flow:\n",
    "state → preprocessor → processed_messages → model → response\n",
    "\n",
    "# Equivalent to:\n",
    "processed_messages = preprocessor.invoke(state)\n",
    "response = model.invoke(processed_messages)\n",
    "```\n",
    "\n",
    "### Visual representation:\n",
    "```\n",
    "Input State\n",
    "    ↓\n",
    "[Preprocessor] → Adds system prompt\n",
    "    ↓\n",
    "Processed Messages\n",
    "    ↓\n",
    "[Model] → Generates response (with possible tool calls)\n",
    "    ↓\n",
    "Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Define the Model Calling Function\n",
    "\n",
    "```python\n",
    "def call_model(\n",
    "    state: ChatAgentState,\n",
    "    config: RunnableConfig,\n",
    "):\n",
    "    response = model_runnable.invoke(state, config)\n",
    "    return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "### What does this function do?\n",
    "\n",
    "1. **Takes the current state**: All conversation history\n",
    "2. **Runs the pipeline**: Preprocessing + Model inference\n",
    "3. **Returns formatted response**: In the format the workflow expects\n",
    "\n",
    "### Step-by-step execution:\n",
    "```python\n",
    "# Step 1: Receive state\n",
    "state = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is Spark?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Step 2: Pipeline execution\n",
    "# preprocessor adds system prompt\n",
    "# model generates response\n",
    "response = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I'll search for information about Spark\",\n",
    "    \"tool_calls\": [...]\n",
    "}\n",
    "\n",
    "# Step 3: Return in expected format\n",
    "return {\"messages\": [response]}\n",
    "```\n",
    "\n",
    "### Why return `{\"messages\": [response]}`?\n",
    "\n",
    "This format tells the workflow system: \"Add this new message to the conversation history.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Build the Workflow Graph\n",
    "\n",
    "```python\n",
    "workflow = StateGraph(ChatAgentState)\n",
    "```\n",
    "\n",
    "### What's StateGraph?\n",
    "\n",
    "Think of it as building a flowchart for your AI agent:\n",
    "\n",
    "```\n",
    "[Start] → [Agent] → [Decision] → [Tools] → [Agent] → [End]\n",
    "                       ↓\n",
    "                    [End]\n",
    "```\n",
    "\n",
    "**StateGraph**: A framework for creating state machines where each node can process and modify the conversation state.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Add Nodes to the Graph\n",
    "\n",
    "```python\n",
    "workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "```\n",
    "\n",
    "### What are nodes?\n",
    "\n",
    "Nodes are the **processing stations** in your workflow:\n",
    "\n",
    "**Agent Node**: \n",
    "```python\n",
    "workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "```\n",
    "- **Name**: \"agent\"\n",
    "- **Function**: `call_model` (wrapped in RunnableLambda)\n",
    "- **Purpose**: Generate responses and decide on tool usage\n",
    "\n",
    "**Tools Node**:\n",
    "```python\n",
    "workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "```\n",
    "- **Name**: \"tools\"  \n",
    "- **Function**: `ChatAgentToolNode` (built-in LangGraph component)\n",
    "- **Purpose**: Execute the tools that the agent requested\n",
    "\n",
    "### Visual representation:\n",
    "```\n",
    "┌─────────────┐       ┌─────────────┐\n",
    "│    Agent    │       │    Tools    │\n",
    "│             │       │             │\n",
    "│ - call_model│       │ - execute   │\n",
    "│ - decide    │       │   tool_calls│\n",
    "│ - respond   │       │ - return    │\n",
    "│             │       │   results   │\n",
    "└─────────────┘       └─────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Set Entry Point\n",
    "\n",
    "```python\n",
    "workflow.set_entry_point(\"agent\")\n",
    "```\n",
    "\n",
    "### What does this mean?\n",
    "\n",
    "When a conversation starts, it always begins at the \"agent\" node.\n",
    "\n",
    "```\n",
    "User Question → [Agent Node] → Decision...\n",
    "```\n",
    "\n",
    "**Why start with agent?**\n",
    "- The agent needs to understand the question first\n",
    "- Then it decides whether to use tools or respond directly\n",
    "\n",
    "---\n",
    "\n",
    "## Step 9: Add Conditional Edges\n",
    "\n",
    "```python\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    routing_logic,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "```\n",
    "\n",
    "### Breaking this down:\n",
    "\n",
    "1. **Source node**: `\"agent\"` - Where the decision is made\n",
    "2. **Decision function**: `routing_logic` - How to decide\n",
    "3. **Possible outcomes**: Dictionary mapping decisions to destinations\n",
    "\n",
    "### Visual flow:\n",
    "```python\n",
    "# After agent generates response:\n",
    "if routing_logic(state) == \"continue\":\n",
    "    # Go to tools node\n",
    "    next_node = \"tools\"\n",
    "elif routing_logic(state) == \"end\":\n",
    "    # End the conversation  \n",
    "    next_node = END\n",
    "```\n",
    "\n",
    "### Complete conditional flow:\n",
    "```\n",
    "[Agent] generates response\n",
    "    ↓\n",
    "[Routing Logic] checks response\n",
    "    ↓\n",
    "Has tool_calls?\n",
    "    ↓         ↓\n",
    "   Yes       No\n",
    "    ↓         ↓\n",
    "[Tools]    [END]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 10: Add Direct Edge\n",
    "\n",
    "```python\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "```\n",
    "\n",
    "### What's this for?\n",
    "\n",
    "After tools execute, we **always** go back to the agent to process the results.\n",
    "\n",
    "```python\n",
    "# Tools execute and return results\n",
    "tool_results = execute_tools(tool_calls)\n",
    "\n",
    "# These results get added to conversation\n",
    "state[\"messages\"].append(tool_results)\n",
    "\n",
    "# Flow goes back to agent to synthesize the information\n",
    "# Agent can now provide a final answer OR call more tools\n",
    "```\n",
    "\n",
    "### Complete workflow loop:\n",
    "```\n",
    "[Agent] → decides to use tools → [Tools] → execute → [Agent] → synthesize results\n",
    "   ↓                                                      ↓\n",
    "decides no tools needed                            provide final answer\n",
    "   ↓                                                      ↓\n",
    " [END]                                                  [END]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 11: Compile the Workflow\n",
    "\n",
    "```python\n",
    "return workflow.compile()\n",
    "```\n",
    "\n",
    "### What does compile do?\n",
    "\n",
    "1. **Validates the graph**: Ensures all connections make sense\n",
    "2. **Optimizes execution**: Creates efficient execution plan  \n",
    "3. **Returns executable**: Creates a `CompiledGraph` ready to run\n",
    "\n",
    "### Before vs After compilation:\n",
    "\n",
    "**Before (workflow definition):**\n",
    "```python\n",
    "workflow = StateGraph(ChatAgentState)\n",
    "# Just a blueprint/definition\n",
    "```\n",
    "\n",
    "**After (compiled graph):**\n",
    "```python\n",
    "compiled_agent = workflow.compile()\n",
    "# Executable agent ready for conversations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Flow Example\n",
    "\n",
    "Let's trace through a complete conversation:\n",
    "\n",
    "### User asks: \"How do I create a Delta table?\"\n",
    "\n",
    "**Step 1**: Start at agent node\n",
    "```python\n",
    "state = {\"messages\": [{\"role\": \"user\", \"content\": \"How do I create a Delta table?\"}]}\n",
    "```\n",
    "\n",
    "**Step 2**: `call_model` executes\n",
    "```python\n",
    "# Preprocessor adds system prompt\n",
    "# Model sees: [system_prompt, user_question]\n",
    "# Model decides to search for information\n",
    "response = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I'll search for Delta table information\",\n",
    "    \"tool_calls\": [{\"function\": {\"name\": \"find_relevant_documents\", ...}}]\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 3**: `routing_logic` checks response\n",
    "```python\n",
    "# Sees tool_calls in response\n",
    "# Returns \"continue\"\n",
    "```\n",
    "\n",
    "**Step 4**: Flow goes to tools node\n",
    "```python\n",
    "# Executes find_relevant_documents\n",
    "# Gets documentation about Delta tables\n",
    "tool_result = {\"role\": \"tool\", \"content\": \"Delta table documentation...\"}\n",
    "```\n",
    "\n",
    "**Step 5**: Flow returns to agent node  \n",
    "```python\n",
    "# Agent now sees: [user_question, agent_response, tool_result]\n",
    "# Model synthesizes information\n",
    "final_response = {\n",
    "    \"role\": \"assistant\", \n",
    "    \"content\": \"Based on the documentation, here's how to create a Delta table: ...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 6**: `routing_logic` checks final response\n",
    "```python\n",
    "# No tool_calls this time\n",
    "# Returns \"end\"\n",
    "```\n",
    "\n",
    "**Step 7**: Conversation ends with complete answer\n",
    "\n",
    "---\n",
    "\n",
    "## Key Benefits of This Architecture\n",
    "\n",
    "### 1. **Flexible Decision Making**\n",
    "The agent can choose when to use tools vs. respond directly:\n",
    "```python\n",
    "# Simple questions → Direct response\n",
    "\"What is Databricks?\" → Direct answer from training\n",
    "\n",
    "# Complex questions → Tool usage  \n",
    "\"How do I optimize my specific ETL pipeline?\" → Search documentation + call UC functions\n",
    "```\n",
    "\n",
    "### 2. **Multi-Step Reasoning**\n",
    "The agent can chain multiple tool calls:\n",
    "```python\n",
    "# Step 1: Search documentation\n",
    "# Step 2: Analyze user's context  \n",
    "# Step 3: Call Unity Catalog function for specific data\n",
    "# Step 4: Provide tailored recommendation\n",
    "```\n",
    "\n",
    "### 3. **Error Recovery**\n",
    "If tools fail, the agent can try alternatives:\n",
    "```python\n",
    "# Tool call fails → Agent tries different approach\n",
    "# No relevant docs found → Agent asks clarifying questions\n",
    "```\n",
    "\n",
    "### 4. **Observability**\n",
    "Every step is traceable through MLflow:\n",
    "```python\n",
    "# Track: What decisions were made? Which tools were called? How long did it take?\n",
    "```\n",
    "\n",
    "This architecture creates a sophisticated AI agent that can reason about when and how to use tools, making it much more capable than a simple question-answering system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b87b14-a43f-45f9-9110-32f74a63d463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create, evaluate, and deploy an AI agent\n",
    "\n",
    "This notebook walks you through building, evaluating, and deploying an AI agent that combines retrieval and tool usage. You'll work with a pre-chunked subset of Databricks documentation as your dataset.\n",
    "\n",
    "## Supporting documentation\n",
    "For a comprehensive understanding of this notebook's contents, including the rationale behind the code and the challenges it addresses, see the accompanying Databricks documentation page. ([AWS](https://docs.databricks.com/aws/en/generative-ai/tutorials/agent-framework-notebook) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/tutorials/agent-framework-notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0867e3-a091-46fe-9cfa-4d0a492e9730",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow langchain langgraph==0.3.4 databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86978017-ed05-4eda-afb6-cb60c044bdd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create an agent and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f3d6c6-5ac3-410b-8592-628ef04be409",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Connect to an LLM"
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a50f9756-3e5f-48bc-bfd8-10e14d696d3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Dataset"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "databricks_docs_url = \"https://raw.githubusercontent.com/databricks/genai-cookbook/refs/heads/main/quick_start_demo/chunked_databricks_docs_filtered.jsonl\"\n",
    "parsed_docs_df = pd.read_json(databricks_docs_url, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7addf225-3435-4b5d-9fc0-efb209401f3d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Keyword Extraction Unity Catalog Tool"
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "\n",
    "uc_client = DatabricksFunctionClient()\n",
    "set_uc_function_client(uc_client)\n",
    "\n",
    "\n",
    "def tfidf_keywords(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts keywords from the provided text using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        text (string): Input text.\n",
    "    Returns:\n",
    "        list[str]: List of extracted keywords in ascending order of importance.\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    def extract_keywords(text, top_n=5):\n",
    "        \"\"\"Extracts top keywords from input text using trained TF-IDF vectorizer\"\"\"\n",
    "        keyword_vectorizer = TfidfVectorizer(\n",
    "            stop_words=\"english\"\n",
    "        )  # New vectorizer for query\n",
    "        query_tfidf = keyword_vectorizer.fit_transform([text])  # Fit on query only\n",
    "        scores = query_tfidf.toarray()[0]\n",
    "        indices = scores.argsort()[-top_n:][::-1]  # Get top N keywords\n",
    "        return [\n",
    "            keyword_vectorizer.get_feature_names_out()[i]\n",
    "            for i in indices\n",
    "            if scores[i] > 0\n",
    "        ]\n",
    "\n",
    "    return extract_keywords(text)\n",
    "\n",
    "# TODO fill in your catalog and schema name\n",
    "catalog = \"agentic_ai\"\n",
    "schema = \"databricks\"\n",
    "\n",
    "assert (catalog and schema)\n",
    "\n",
    "# Create the function within the Unity Catalog catalog and schema specified\n",
    "function_info = uc_client.create_python_function(\n",
    "    func=tfidf_keywords,\n",
    "    catalog=catalog,\n",
    "    schema=schema,\n",
    "    replace=True,  # Set to True to overwrite if the function already exists\n",
    ")\n",
    "\n",
    "uc_tool_names = [f\"{catalog}.{schema}.tfidf_keywords\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e205ba1-e219-4dc0-8d95-a09f22d86b03",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inspect and test the tool"
    }
   },
   "outputs": [],
   "source": [
    "print(uc_toolkit.tools[0])\n",
    "uc_toolkit.tools[0].invoke({\"text\": \"The quick brown fox jumped over the lazy brown dog.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b96fc9-fbcd-4441-8120-ff3319ac9590",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a client-side keyword search retriever"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import mlflow\n",
    "from langchain_core.tools import tool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = parsed_docs_df\n",
    "doc_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = doc_vectorizer.fit_transform(documents[\"content\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5) -> list[dict[str, Any]]:\n",
    "    \"\"\"gets relevant documents for the query\"\"\"\n",
    "    query_tfidf = doc_vectorizer.transform([query])\n",
    "    similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
    "    ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    result = []\n",
    "    for idx, score in ranked_docs[:top_n]:\n",
    "        row = documents.iloc[idx]\n",
    "        content = row[\"content\"]\n",
    "        doc_entry = {\n",
    "            \"page_content\": content,\n",
    "            \"metadata\": {\n",
    "                \"doc_uri\": row[\"doc_uri\"],\n",
    "                \"score\": score,\n",
    "            },\n",
    "        }\n",
    "        result.append(doc_entry)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027cdf3b-d83b-41a4-ba45-f1de0bfab3a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define create agent factory function"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence, Union\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    def routing_logic(state: ChatAgentState):\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = {\"role\": \"system\", \"content\": agent_prompt}\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        routing_logic,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e99bc0-444b-407e-8a22-82a533788a4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test the agent"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools=[*uc_toolkit.tools, find_relevant_documents])\n",
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\":\"How can I create a Delta Live Tables pipeline that processes CDC events using apply_changes() in Python, including handling out-of-order data?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6809396-beb4-452c-b6ca-cee168a3e48d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wrapping it in a ChatAgent"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "\n",
    "class DocsAgent(ChatAgent):\n",
    "  def __init__(self, agent):\n",
    "    self.agent = agent\n",
    "\n",
    "  def predict(\n",
    "      self,\n",
    "      messages: list[ChatAgentMessage],\n",
    "      context: Optional[ChatContext] = None,\n",
    "      custom_inputs: Optional[dict[str, Any]] = None,\n",
    "  ) -> ChatAgentResponse:\n",
    "      # ChatAgent has a built-in helper method to help convert framework-specific messages, like langchain BaseMessage to a python dictionary\n",
    "      request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "      output = agent.invoke(request)\n",
    "      # Here 'output' is already a ChatAgentResponse, but to make the ChatAgent signature explicit for this demonstration we are returning a new instance\n",
    "      return ChatAgentResponse(**output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc12c150-f8a3-4646-bda7-bfdeb739b1a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test ChatAgent"
    }
   },
   "outputs": [],
   "source": [
    "AGENT = DocsAgent(agent=agent)\n",
    "AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"How can I create a Delta Live Tables pipeline that processes CDC events using apply_changes() in Python, including handling out-of-order data??\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c22727-1be7-4384-b54e-cd64c3956f40",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using Model Config"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import ModelConfig\n",
    "\n",
    "baseline_config = {\n",
    "   \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "   \"temperature\": 0.01,\n",
    "   \"max_tokens\": 1000,\n",
    "   \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "   \"tool_list\": [f\"{catalog}.{schema}.*\"],\n",
    "}\n",
    "\n",
    "\n",
    "class DocsAgent(ChatAgent):\n",
    "    def __init__(self):\n",
    "        self.config = ModelConfig(development_config=baseline_config)\n",
    "        self.agent = self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        temperature = self.config.get(\"temperature\")\n",
    "        max_tokens = self.config.get(\"max_tokens\")\n",
    "        system_prompt = self.config.get(\"system_prompt\")\n",
    "        llm_endpoint_name = self.config.get(\"endpoint_name\")\n",
    "        tool_list = self.config.get(\"tool_list\")\n",
    "\n",
    "        llm = ChatDatabricks(endpoint=llm_endpoint_name, temperature=temperature, max_tokens=max_tokens)\n",
    "        toolkit = UCFunctionToolkit(function_names=tool_list)\n",
    "        agent = create_tool_calling_agent(llm, tools=[*toolkit.tools, find_relevant_documents], agent_prompt=system_prompt)\n",
    "\n",
    "        return agent\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # ChatAgent has a built-in helper method to help convert framework-specific messages, like langchain BaseMessage to a python dictionary\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        output = self.agent.invoke(request)\n",
    "        # Here 'output' is already a ChatAgentResponse, but to make the ChatAgent signature explicit for this demonstration we are returning a new instance\n",
    "        return ChatAgentResponse(**output)\n",
    "\n",
    "agent = DocsAgent()\n",
    "agent.predict({\"messages\": [{\"role\": \"user\", \"content\": \"What is DLT\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dcc7af8-d0d9-4c6e-96de-9e1f23b358be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = agent.predict({\"messages\": [{\"role\": \"user\", \"content\": \"How can I create a Delta Live Tables pipeline that processes CDC events using apply_changes() in Python, including handling out-of-order data?\"}]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d704f7-c9d9-46de-aaa3-427c8bd39132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7ad280-aa8a-4df9-8ffd-9d1be284d269",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write file for models from code"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile getting_started_agent.py\n",
    "from typing import Any, Optional, Sequence, Union\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.models import ModelConfig\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "databricks_docs_url = \"https://raw.githubusercontent.com/databricks/genai-cookbook/refs/heads/main/quick_start_demo/chunked_databricks_docs_filtered.jsonl\"\n",
    "parsed_docs_df = pd.read_json(databricks_docs_url, lines=True)\n",
    "\n",
    "documents = parsed_docs_df\n",
    "doc_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = doc_vectorizer.fit_transform(documents[\"content\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5) -> list[dict[str, Any]]:\n",
    "    \"\"\"gets relevant documents for the query\"\"\"\n",
    "    query_tfidf = doc_vectorizer.transform([query])\n",
    "    similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
    "    ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    result = []\n",
    "    for idx, score in ranked_docs[:top_n]:\n",
    "        row = documents.iloc[idx]\n",
    "        content = row[\"content\"]\n",
    "        doc_entry = {\n",
    "            \"page_content\": content,\n",
    "            \"metadata\": {\n",
    "                \"doc_uri\": row[\"doc_uri\"],\n",
    "                \"score\": score,\n",
    "            },\n",
    "        }\n",
    "        result.append(doc_entry)\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    def routing_logic(state: ChatAgentState):\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = {\"role\": \"system\", \"content\": agent_prompt}\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        routing_logic,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class DocsAgent(ChatAgent):\n",
    "    def __init__(self, config, tools):\n",
    "        # Load config\n",
    "        # When this agent is deployed to Model Serving, the configuration loaded here is replaced with the config passed to mlflow.pyfunc.log_model(model_config=...)\n",
    "        self.config = ModelConfig(development_config=config)\n",
    "        self.tools = tools\n",
    "        self.agent = self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        llm = ChatDatabricks(\n",
    "            endpoint=self.config.get(\"endpoint_name\"),\n",
    "            temperature=self.config.get(\"temperature\"),\n",
    "            max_tokens=self.config.get(\"max_tokens\"),\n",
    "        )\n",
    "        agent = create_tool_calling_agent(\n",
    "            llm,\n",
    "            tools=self.tools,\n",
    "            agent_prompt=self.config.get(\"system_prompt\"),\n",
    "        )\n",
    "        return agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # ChatAgent has a built-in helper method to help convert framework-specific messages, like langchain BaseMessage to a python dictionary\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        output = self.agent.invoke(request)\n",
    "        # Here 'output' is already a ChatAgentResponse, but to make the ChatAgent signature explicit for this demonstration we are returning a new instance\n",
    "        return ChatAgentResponse(**output)\n",
    "    \n",
    "\n",
    "# TODO fill in your catalog and schema name\n",
    "catalog = \"agentic_ai\"\n",
    "schema = \"databricks\"\n",
    "\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "baseline_config = {\n",
    "    \"endpoint_name\": LLM_ENDPOINT,\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "tools = [find_relevant_documents]\n",
    "uc_client = DatabricksFunctionClient()\n",
    "set_uc_function_client(uc_client)\n",
    "uc_toolkit = UCFunctionToolkit(function_names=[f\"{catalog}.{schema}.*\"])\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "\n",
    "AGENT = DocsAgent(baseline_config, tools)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316d2039-6159-40a7-894d-10980a38969e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ad75a4-4dae-4f4e-9740-d710bb1fe515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from getting_started_agent import AGENT\n",
    "\n",
    "AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"How can I create a Delta Live Tables pipeline that processes CDC events using apply_changes() in Python, including handling out-of-order data?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527aee2c-9ea6-4433-8e73-5bae791a60eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Enable tracing at the start of your notebook/script\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Alternative: Set tracing environment variable\n",
    "import os\n",
    "os.environ[\"MLFLOW_ENABLE_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1e4e97-9a1d-49b9-8075-d602ce7348c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from getting_started_agent import LLM_ENDPOINT, baseline_config, tools\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT)]\n",
    "for tool in tools:\n",
    "    if isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"getting_started_agent.py\",\n",
    "        artifact_path=\"agent\",\n",
    "        model_config=baseline_config,\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"langchain\",\n",
    "            \"langgraph==0.3.4\",\n",
    "            \"databricks-langchain\",\n",
    "            \"unitycatalog-langchain[databricks]\",\n",
    "            \"pydantic\",\n",
    "        ],\n",
    "        input_example={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"How can I create a Delta Live Tables pipeline that processes CDC events using apply_changes() in Python, including handling out-of-order data?\"}]\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b7c577-6280-4fb5-b3e2-edc17d8be1ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from databricks.agents.evals import generate_evals_df\n",
    "\n",
    "agent_description = \"\"\"\n",
    "The agent is a RAG chatbot that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\"\"\"\n",
    "question_guidelines = \"\"\"\n",
    "# User personas\n",
    "- A developer who is new to the Databricks platform\n",
    "- An experienced, highly technical Data Scientist or Data Engineer\n",
    "\n",
    "\n",
    "# Example questions\n",
    "- what API lets me parallelize operations over rows of a delta table?\n",
    "- Which cluster settings will give me the best performance when using Spark?\n",
    "\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "databricks_docs_url = \"https://raw.githubusercontent.com/databricks/genai-cookbook/refs/heads/main/quick_start_demo/chunked_databricks_docs_filtered.jsonl\"\n",
    "parsed_docs_df = pd.read_json(databricks_docs_url, lines=True)\n",
    "\n",
    "\n",
    "num_evals = 25\n",
    "evals = generate_evals_df(\n",
    "    docs=parsed_docs_df[\n",
    "        :500\n",
    "    ],  # Pass your docs. They should be in a Pandas or Spark DataFrame with columns `content STRING` and `doc_uri STRING`.\n",
    "    num_evals=num_evals,  # How many synthetic evaluations to generate\n",
    "    agent_description=agent_description,\n",
    "    question_guidelines=question_guidelines,\n",
    ")\n",
    "display(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5c11ec-9e37-4bff-b080-cb9b56bc9bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import metric\n",
    "from getting_started_agent import catalog, schema\n",
    "@metric\n",
    "def uses_keywords_and_retriever(request, trace):\n",
    "  retriever_spans = trace.search_spans(span_type='RETRIEVER')\n",
    "  keyword_tool_spans = trace.search_spans(name=f\"{catalog}__{schema}__tfidf_keywords\")\n",
    "  return len(keyword_tool_spans) > 0 and len(retriever_spans) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dc9235-7706-41c0-be3e-e0c10c85e320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"my_agent\"):\n",
    "  eval_results = mlflow.evaluate(\n",
    "      data=evals,  # Your evaluation set\n",
    "      model=model_info.model_uri,  # Logged agent from above\n",
    "      model_type=\"databricks-agent\",  # activate Mosaic AI Agent Evaluation,\n",
    "      extra_metrics=[uses_keywords_and_retriever]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7456a2-0246-42bf-814e-0b3df35dcb65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks import agents\n",
    "\n",
    "# Connect to the Unity catalog model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "\n",
    "# TODO: define the catalog and schema for your UC model\n",
    "catalog = \"agentic_ai\"\n",
    "schema = \"databricks\"\n",
    "assert (catalog and schema)\n",
    "UC_MODEL_NAME = f\"agentic_ai.databricks.getting_started_agent\"\n",
    "\n",
    "\n",
    "# Register to Unity catalog\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=model_info.model_uri, name=UC_MODEL_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b67d95e6-928c-404b-a2aa-44b087c5b0ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deploy to enable the review app and create an API endpoint\n",
    "deployment_info = agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, deploy_feedback_model=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "5. Create, evaluate, and deploy an AI agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
