{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4828974-a1c1-4322-8131-fc6950f6068b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qq langchain\n",
    "!pip install -qq langchain_core\n",
    "!pip install -qq langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2e349a-8e46-4c7d-93bc-18dc5955018f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea2380b-1f16-413e-b70e-f397fa82c8f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following to use the full abilities of langchain autologgin\n",
    "# %pip install `langchain_community>=0.0.16`\n",
    "# These two libraries enable autologging to log text analysis related artifacts\n",
    "# %pip install textstat spacy\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = dbutils.secrets.get(scope=\"sourav_secret_scope\", key=\"OPENAI_API_KEY\")\n",
    "\n",
    "# Enable mlflow langchain autologging\n",
    "# Note: We only support auto-logging models that do not contain retrievers\n",
    "mlflow.langchain.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=True,\n",
    "    log_models=True,\n",
    "    registered_model_name=\"lc_model\",\n",
    "    log_traces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25852f6-80c1-4e35-8990-9b3b21b9fdb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt_with_history_str = \"\"\"\n",
    "Here is a history between you and a human: {chat_history}\n",
    "\n",
    "Now, please answer this question: {question}\n",
    "\"\"\"\n",
    "prompt_with_history = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"], template=prompt_with_history_str\n",
    ")\n",
    "\n",
    "\n",
    "def extract_question(input):\n",
    "    return input[-1][\"content\"]\n",
    "\n",
    "\n",
    "def extract_history(input):\n",
    "    return input[:-1]\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# Build a chain with LCEL\n",
    "chain_with_history = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | prompt_with_history\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"Who owns MLflow?\"}]}\n",
    "\n",
    "print(chain_with_history.invoke(inputs))\n",
    "# sample output:\n",
    "# \"1. Databricks\\n2. Microsoft\\n3. Google\\n4. Amazon\\n\\nEnter your answer: 1\\n\\n\n",
    "# Correct! MLflow is an open source project developed by Databricks. ...\n",
    "\n",
    "# We automatically log the model and trace related artifacts\n",
    "# A model with name `lc_model` is registered, we can load it back as a PyFunc model\n",
    "model_name = \"lc_model\"\n",
    "model_version = 1\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "print(loaded_model.predict(inputs))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2. chain_autolog",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
